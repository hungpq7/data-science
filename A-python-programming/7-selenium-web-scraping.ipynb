{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "624cab33",
   "metadata": {},
   "source": [
    "First image:\n",
    "\n",
    "<img src='../_image/rest_api_response.png' style='height:300px'>\n",
    "\n",
    "Some text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eff594",
   "metadata": {},
   "source": [
    "Second image:\n",
    "\n",
    "<center><img src='../_image/rest_api_response.png' style='height:300px; margin:20px auto 20px;'></center>\n",
    "\n",
    "Some text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1708e979",
   "metadata": {},
   "source": [
    "Third image\n",
    "\n",
    "<center><img src='../_image/rest_api_response.png' style='height:300px'></center>\n",
    "\n",
    "Hi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c4bdb9-d3ef-484d-9fb0-dab93f4b3228",
   "metadata": {},
   "source": [
    "# Selenium: Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf9279be-92db-4530-b800-ea3bc721873e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-22T06:19:48.670281Z",
     "iopub.status.busy": "2023-01-22T06:19:48.669649Z",
     "iopub.status.idle": "2023-01-22T06:19:50.158831Z",
     "shell.execute_reply": "2023-01-22T06:19:50.158100Z",
     "shell.execute_reply.started": "2023-01-22T06:19:48.670173Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from urllib.parse import urljoin\n",
    "import requests\n",
    "\n",
    "import bs4\n",
    "import pandas as pd\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951c3b81-5ab9-4de2-b65b-42cda5bf8ce4",
   "metadata": {},
   "source": [
    "## 1. Overview\n",
    "There are basically two ways to scrape data from websites, one is via HTML responses (server-side) and the other is via JSON responses (client-side).\n",
    "- Server-side: We extract *unstructured* data from user interface, which is friendly for human eyes but not for computers. Data is availale most of the time except for cases when website owners protect their data intentionally. This crawling approach requires some basic knowledge of HTML.\n",
    "- Client-side: We try to crawl *structured* REST API responses, which is only available in specific website that use this protocol. The advantage is that the returned data is in JSON format, so they can be easily extracted and processed. Crawling data this way is much easier, so we are going to start with it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a10951-8692-4a66-9629-432a26dd0717",
   "metadata": {},
   "source": [
    "### 1.1. Requests\n",
    "Instead of accessing websites using a browser such as Google Chrome, we can use the [Requests] library to download the raw content of that page and interact with it. Most of the time, we are going to use the `get()` function followed by the `text` or `content` attributes.\n",
    "\n",
    "[Requests]: https://github.com/psf/requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e94cf571-cdf4-4e7d-98b9-da7fed7ded19",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c0e7a567-3ba1-4a12-8d98-5989285a5bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://books.toscrape.com/index.html'\n",
    "response = requests.get(url)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "24b731f3-bcac-4f7d-9fec-3fda13426bca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html>\\n<!--[if lt IE 7]>      <html lang=\"en-us\" class=\"no-js lt-ie9 lt-ie8 lt-ie7\"> <![endif]-->\\n<!--[if IE 7]>         <html lang=\"en-us\" class=\"no-js lt-ie9 lt-ie8\"> <![endif]-->\\n<!--[if IE 8]>         <html lang=\"en-us\" class=\"no-js lt-ie9\"> <![endif]-->\\n<!--[if gt IE 8]><!--> <html lang=\"en-us\" class=\"no-js\"> <!--<![endif]-->\\n    <head>\\n        <title>\\n    All products | Books to Scrape - Sandbox\\n</title>\\n\\n        <meta http-equiv=\"content-type\" content=\"text/html; charset=UTF-8\" />\\n        <meta name=\"created\" content=\"24th Jun 2016 09:29\" />\\n        <meta name=\"description\" content=\"\" />\\n        <meta name=\"viewport\" content=\"width=device-width\" />\\n        <meta name=\"robots\" content=\"NOARCHIVE,NOCACHE\" />\\n\\n        <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->\\n        <!--[if lt IE 9]>\\n        <script src=\"//html5shim.googlecode.com/svn/trunk/html5.js\"></script>\\n        <![endif]-->\\n\\n        \\n            <link rel=\"shortcut icon\" href=\"static/oscar/favicon.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "66b0b76e-8bed-4d93-9f53-9ab797a08f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Date': 'Fri, 06 Jan 2023 02:51:21 GMT', 'Content-Type': 'text/html', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Last-Modified': 'Thu, 26 May 2022 21:15:15 GMT', 'ETag': 'W/\"628fede3-c85e\"', 'Strict-Transport-Security': 'max-age=0; includeSubDomains; preload', 'Content-Encoding': 'br'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.headers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b8d570-0f88-48cd-8820-c5ba66897f3d",
   "metadata": {},
   "source": [
    "### 1.2. APIs crawling\n",
    "Not all URLs point to an HTML page, for example, the URL https://api.github.com/repos/dmlc/xgboost points to a raw document in JSON format. Such an URL is called a REST API endpoint and can be easily converted into Python dictionaries using the  `json()` method. But APIs in practice are not always that simple, they can go with concepts such as headers and payloads. So, learning real-world API structures and how to find them will be our goal in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6b64e288-57a1-41c9-b892-56c3af9b8141",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T16:52:54.348200Z",
     "iopub.status.busy": "2023-01-08T16:52:54.347875Z",
     "iopub.status.idle": "2023-01-08T16:52:54.351352Z",
     "shell.execute_reply": "2023-01-08T16:52:54.350219Z",
     "shell.execute_reply.started": "2023-01-08T16:52:54.348178Z"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27557af2-65e1-4bee-8324-94cf78820399",
   "metadata": {},
   "source": [
    "#### Documented APIs\n",
    "Many organizations officially support REST APIs for data accessing, such as [GitHub], [Facebook], [Twitter], [Reddit] and [Clash Royale]. To start using APIs provided this way, developers usually need to register an account and generate an API key, but some of them don't require any authenication. Either way, the instructions for requesting data can be found in their documentation sites.\n",
    "\n",
    "Now let's hand on an example by requesting GitHub's [list-repository-languages] endpoint to show the size (in bytes) of code written in each language.\n",
    "- The main component of a request is the URL which follows a pre-defined syntax. In this case, the URL has two placeholders for `OWNER` and `REPO` they can be handled nicely using Python formatted strings. We can use this URL to access data of any public repository.\n",
    "- For private repositories, we must provide an authenication key with appropriae permissions. These additional information are called the headers, you can think of them as metadata of the API call.\n",
    "- We might notice that there are different requesting methods available such as GET, POST, PUT and DELETE that serve different purposes. As we only want to collect data, we only need to care about GET, and sometimes, POST.\n",
    "\n",
    "[GitHub]: https://docs.github.com/en/rest\n",
    "[Facebook]: https://developers.facebook.com/docs/groups-api/reference\n",
    "[Twitter]: https://developer.twitter.com/en/docs/twitter-api\n",
    "[Reddit]: https://www.reddit.com/dev/api/\n",
    "[Clash Royale]: https://developer.clashroyale.com/\n",
    "[list-repository-languages]: https://docs.github.com/en/rest/repos/repos#list-repository-languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a048a2b-6c21-4efd-87ef-1993cc948a00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-06T16:36:53.095704Z",
     "iopub.status.busy": "2023-01-06T16:36:53.095191Z",
     "iopub.status.idle": "2023-01-06T16:36:55.091535Z",
     "shell.execute_reply": "2023-01-06T16:36:55.090835Z",
     "shell.execute_reply.started": "2023-01-06T16:36:53.095666Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C++': 2215388,\n",
       " 'Python': 1203192,\n",
       " 'Cuda': 863316,\n",
       " 'Scala': 470919,\n",
       " 'R': 343950,\n",
       " 'Java': 206895,\n",
       " 'CMake': 52369,\n",
       " 'Shell': 45902,\n",
       " 'C': 22503,\n",
       " 'Makefile': 8179,\n",
       " 'PowerShell': 4308,\n",
       " 'CSS': 3812,\n",
       " 'Dockerfile': 2364,\n",
       " 'M4': 2131,\n",
       " 'Batchfile': 1383,\n",
       " 'Groovy': 1251,\n",
       " 'TeX': 913}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OWNER = 'dmlc'\n",
    "REPO = 'xgboost'\n",
    "\n",
    "url = f'https://api.github.com/repos/{OWNER}/{REPO}/languages'\n",
    "response = requests.get(url)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50e04a33-10b6-4023-9839-41af3d0a9674",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-06T16:40:28.328603Z",
     "iopub.status.busy": "2023-01-06T16:40:28.328241Z",
     "iopub.status.idle": "2023-01-06T16:40:29.234550Z",
     "shell.execute_reply": "2023-01-06T16:40:29.233662Z",
     "shell.execute_reply.started": "2023-01-06T16:40:28.328576Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Jupyter Notebook': 8166118, 'Perl': 1432, 'Shell': 1286}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OWNER = 'hungpq7'\n",
    "REPO = 'courses'\n",
    "\n",
    "url = f'https://api.github.com/repos/{OWNER}/{REPO}/languages'\n",
    "headers = {\n",
    "    'Accept': 'application/vnd.github+json',\n",
    "    'Authorization': 'Bearer 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',\n",
    "    'X-GitHub-Api-Version': '2022-11-28',\n",
    "}\n",
    "response = requests.get(url, headers=headers)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c67c6ca-c8c6-4b20-a66b-ec796d95acd9",
   "metadata": {},
   "source": [
    "&#9800;&nbsp;<b>Note</b><br>\n",
    "We can make API calls with command line too, using the [cURL](https://en.wikipedia.org/wiki/CURL) command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32631a72-295d-4a59-97dd-0148a1991bd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-06T16:40:43.139803Z",
     "iopub.status.busy": "2023-01-06T16:40:43.139439Z",
     "iopub.status.idle": "2023-01-06T16:40:45.202304Z",
     "shell.execute_reply": "2023-01-06T16:40:45.199904Z",
     "shell.execute_reply.started": "2023-01-06T16:40:43.139777Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Jupyter Notebook\": 8166118,\n",
      "  \"Perl\": 1432,\n",
      "  \"Shell\": 1286\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl https://api.github.com/repos/hungpq7/courses/languages \\\n",
    "    -H \"Accept: application/vnd.github+json\" \\\n",
    "    -H \"Authorization: Bearer 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\" \\\n",
    "    -H \"X-GitHub-Api-Version: 2022-11-28\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b52411-88cb-4f9a-b304-8c41cd5480d3",
   "metadata": {},
   "source": [
    "#### Hidden APIs\n",
    "The increasing popularity of JavaScript-based frameworks such as React, Angular and Vue encourages websites to be rendered *client-side*. This means, more websites use REST APIs to send and receive data to fill HTML templates, then render the page on user's computers. Of course, such APIs are not documented, so benefiting them requires some tricks including finding them and understanding their structures. In this section, we are going to inspect a page's network activities to locate scrapable APIs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4452a368-48d8-47fa-8742-e20cd3fc6b96",
   "metadata": {},
   "source": [
    "&#9800;&nbsp;<b>Usecase</b><br>\n",
    "\n",
    "We will be crawling all articles in the home page of https://techcrunch.com/ with the following steps:\n",
    "- Go to the target page and open the browser's developer tool. The shortcut in Google Chrome is `F12` or `Ctrl + Shift + I`. However, the tool will not record activities before it was opened, so we need to press `Ctrl + R` to reload the target page.\n",
    "- Navigate to the Network tab to show all requests the page has made and filter Fetch/XHR requests. This filter leaves only requests that fetch JSON data, separating them from other types of response that we don't need such as image, media and CSS. These buttons are yellow circled in the image below.\n",
    "\n",
    ":::{image} ../image/rest_api_response.png\n",
    ":height: 300px\n",
    ":align: center\n",
    ":::\n",
    "\n",
    "- At this point, one of the displaying requests returns the data we are looking for. We will need to explore a bit to determine that API, start with names. In the example of TechCrunch, the API \"magazine\" sounds promising. Indeed, when we click this API and preview its response, we see a list of items storing articles in the website.\n",
    "- Now we have found the API, let's learn how to use it. Switching to the Header tab reveals to us the URL and the request method of this API. Other APIs may require headers as well as payload, but this one is not the case.\n",
    "\n",
    ":::{image} ../image/rest_api_url.png\n",
    ":height: 250px\n",
    ":align: center\n",
    ":::\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab4dc691-8cf7-455e-a774-6dc250de081a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-22T06:20:10.952045Z",
     "iopub.status.busy": "2023-01-22T06:20:10.951582Z",
     "iopub.status.idle": "2023-01-22T06:20:12.418666Z",
     "shell.execute_reply": "2023-01-22T06:20:12.417949Z",
     "shell.execute_reply.started": "2023-01-22T06:20:10.952016Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://techcrunch.com/wp-json/tc/v1/magazine?page=1&_embed=true&cachePrevention=0'\n",
    "response = requests.get(url)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7079801a-86af-4dc3-9dfa-7c77ef98c077",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-22T06:20:16.720366Z",
     "iopub.status.busy": "2023-01-22T06:20:16.719912Z",
     "iopub.status.idle": "2023-01-22T06:20:16.742561Z",
     "shell.execute_reply": "2023-01-22T06:20:16.741663Z",
     "shell.execute_reply.started": "2023-01-22T06:20:16.720332Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2473566</td>\n",
       "      <td>social</td>\n",
       "      <td>Taylor Hatmaker</td>\n",
       "      <td>Microsoft is sunsetting social VR pioneer Alts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2472783</td>\n",
       "      <td>security</td>\n",
       "      <td>Zack Whittaker</td>\n",
       "      <td>A hack at ODIN Intelligence exposes a huge tro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2472976</td>\n",
       "      <td>startups</td>\n",
       "      <td>Kyle Wiggers</td>\n",
       "      <td>Alphabet makes cuts, Twitter bans third-party ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2472885</td>\n",
       "      <td>startups</td>\n",
       "      <td>Natasha Mascarenhas</td>\n",
       "      <td>Tech forgot its umbrella</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2472793</td>\n",
       "      <td>apps</td>\n",
       "      <td>Sarah Perez</td>\n",
       "      <td>This Week in Apps: Twitter kills third-party a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  category               author  \\\n",
       "0  2473566    social      Taylor Hatmaker   \n",
       "1  2472783  security       Zack Whittaker   \n",
       "2  2472976  startups         Kyle Wiggers   \n",
       "3  2472885  startups  Natasha Mascarenhas   \n",
       "4  2472793      apps          Sarah Perez   \n",
       "\n",
       "                                               title  \n",
       "0  Microsoft is sunsetting social VR pioneer Alts...  \n",
       "1  A hack at ODIN Intelligence exposes a huge tro...  \n",
       "2  Alphabet makes cuts, Twitter bans third-party ...  \n",
       "3                           Tech forgot its umbrella  \n",
       "4  This Week in Apps: Twitter kills third-party a...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for item in response.json():\n",
    "    sample = {\n",
    "        'id': item['id'],\n",
    "        'category': item['primary_category']['slug'],\n",
    "        'author': item['parselyMeta']['parsely-author'][0],\n",
    "        'title': item['parselyMeta']['parsely-title'],\n",
    "    }\n",
    "    data.append(sample)\n",
    "\n",
    "pd.DataFrame.from_dict(data).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289f9f70-5f9f-4696-98b7-781856376000",
   "metadata": {},
   "source": [
    "::::{admonition} Case study\n",
    ":class: seealso\n",
    "\n",
    "Sometimes, websites block connections from non-browser clients. For example, when inspecting the website https://tiki.vn/nha-sach-tiki/c8322, I found an API named \"listing\" which contains all products shown in the page. With the naked URL, we can access its response using Chrome but will get blocked using Requests. This can be easily bypassed by we overwriting the *user agent* header as follows.\n",
    "\n",
    ":::{image} ../image/rest_api_payload.png\n",
    ":height: 300px\n",
    ":align: center\n",
    ":::\n",
    "\n",
    "Now, if switch to the Payload tab, we can observe that the parameters here match exactly the components in the URL. With this insight, we can rewrite the request with URL and payload separatedly, which is far more readable.\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c3e04044-eea7-4a02-9b98-42ae4970dd22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T16:17:18.019284Z",
     "iopub.status.busy": "2023-01-08T16:17:18.018794Z",
     "iopub.status.idle": "2023-01-08T16:17:18.024806Z",
     "shell.execute_reply": "2023-01-08T16:17:18.023630Z",
     "shell.execute_reply.started": "2023-01-08T16:17:18.019243Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'User-Agent': 'python-requests/2.25.1', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.utils.default_headers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5a43ef5b-b131-4bd7-bd0d-ec7e1c2a0420",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T17:14:43.591562Z",
     "iopub.status.busy": "2023-01-08T17:14:43.591051Z",
     "iopub.status.idle": "2023-01-08T17:14:44.590359Z",
     "shell.execute_reply": "2023-01-08T17:14:44.588917Z",
     "shell.execute_reply.started": "2023-01-08T17:14:43.591503Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://tiki.vn/api/personalish/v1/blocks/listings?limit=40&category=8322&page=1&urlKey=nha-sach-tiki'\n",
    "headers = {'user-agent': 'Mozilla/5.0 Chrome/108.0.0.0 Safari/537.36'}\n",
    "response = requests.get(url, headers=headers)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a5dae78d-548a-401a-a9cb-0094d7b41a00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T17:33:06.856117Z",
     "iopub.status.busy": "2023-01-08T17:33:06.855519Z",
     "iopub.status.idle": "2023-01-08T17:33:07.575583Z",
     "shell.execute_reply": "2023-01-08T17:33:07.574970Z",
     "shell.execute_reply.started": "2023-01-08T17:33:06.856074Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://tiki.vn/api/personalish/v1/blocks/listings'\n",
    "headers = {'user-agent': 'Mozilla/5.0 Chrome/108.0.0.0 Safari/537.36'}\n",
    "payload = {\n",
    "    'limit': 40,\n",
    "    'category': 8322,\n",
    "    'page': 1,\n",
    "    'urlKey': 'nha-sach-tiki',\n",
    "}\n",
    "response = requests.get(url, headers=headers, params=payload)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1a548b06-bc20-4c58-80a9-1bad2b687c08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-08T17:15:49.068586Z",
     "iopub.status.busy": "2023-01-08T17:15:49.068208Z",
     "iopub.status.idle": "2023-01-08T17:15:49.075554Z",
     "shell.execute_reply": "2023-01-08T17:15:49.074100Z",
     "shell.execute_reply.started": "2023-01-08T17:15:49.068561Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cây Cam Ngọt Của Tôi\n",
      "Hành Tinh Của Một Kẻ Nghĩ Nhiều\n",
      "Không Phải Sói Nhưng Cũng Đừng Là Cừu -Tặng kèm bookmark 2 mặt\n",
      "Thao Túng Tâm Lý\n",
      "Thiên Tài Bên Trái, Kẻ Điên Bên Phải (Tái Bản)\n"
     ]
    }
   ],
   "source": [
    "for product in response.json()['data'][:5]:\n",
    "    name = product['name']\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d79f863-31ef-4993-943e-20523f24d3ec",
   "metadata": {},
   "source": [
    "## 2. HTML parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afe542a-3b64-4bd6-b994-8a01f7a0bced",
   "metadata": {},
   "source": [
    "### 2.1. HTML concepts\n",
    "[HTML] is a language for creating web pages. The easiest way to think about HTML, is a language with the same purpose with Markdown, with less readability but more expressivity. A HTML document is constructed by *elements*, organized in a hierarchical structure. For example, here are the components of an element:\n",
    "- The *tag* `<span>` usually pairs with a closing one `</span>`. Some tags can stands alone such as `<br>`. We also refer to tag as element name.\n",
    "- The text between two tags `computer` is the *content* of that element.\n",
    "- An element can have a number of *attributes* such as `class`, `style` and their corresponding *values* placed after the equal sign `=`.\n",
    "\n",
    "[HTML]: https://en.wikipedia.org/wiki/HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e42b9649-a6bc-498b-acfb-897144db64b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class='breadcrumb content' style='color:indianred'>computer</span>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<span class='breadcrumb content' style='color:indianred'>computer</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a36ef00-d471-45c6-b718-e3a1d122e0c4",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "\n",
    "Some global attributes occur everywhere in HTML documents. Keeping an eye on them will help you a lot in scraping data:\n",
    "- The attribute `id` is the identifier of an element, must be unique across the document. Useful in locating a specific element.\n",
    "- The attribute `class` makes reference to custom CSS styles. Useful in matching a list of items with the same style. An element can have multiple classes, for example *breadcrumb* and *content*.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6756ee-2072-4e95-9c6d-d0bee76665d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2. Parsing\n",
    "There is a powerful library, [Beautiful Soup], that helps us navigating HTML documents and finding the desired elements. All we need to do is passing the HTML document to it (to get an object called the *soup*), then uses its methods and attributes to extract what we want. We are going to demonstrate the main features of Beautiful Soup using a small document.\n",
    "\n",
    "[Beautiful Soup]: https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e04af98-e097-434c-b0e0-ca4bbadcef36",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d4c535ff-4ee1-47d4-a97d-dbc76655f46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = \"\"\"\n",
    "<html>\n",
    "    <head>\n",
    "        <title>The Dormouse's story</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "            <a href=\"https://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "            <a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "            <a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "            and they lived at the bottom of a well.\n",
    "        </p>\n",
    "        <time class=\"story\">2000-01-01 06:00:00</time>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cc846e28-34d0-4f0f-9d05-4e6e0e85e556",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs4.BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b763a776-2a04-4390-84c1-ed6a66f3a3a2",
   "metadata": {},
   "source": [
    "#### Tree navigation\n",
    "We can easily navigate a HTML document as BS has registered child tags and attributes to the *soup*. This syntax is simple, but the downside is that it cannot handle multiple children (only the first child is returned)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bff14403-ace1-41f9-9e63-942a493bc944",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = soup.body.p.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d29de813-1854-4fdc-b860-0ef4ddef277c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Elsie'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3382988a-efcb-4e63-8693-ed7d0efed730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://example.com/elsie'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348fb10c-3680-417b-906b-1cfbf669b2c0",
   "metadata": {},
   "source": [
    "#### Element searching\n",
    "BS supports a more reliable way for finding exactly the element we want, via the [`find_all()`] method. This function searchs for HTML tags and attributes:\n",
    "- The [first argument] is the tag you want to find. It can be a string or list of strings, a regex pattern or a function.\n",
    "- Other arguments are named after HTML attributes. But if some of them make conflicts to Python built-in names such as `id`, `class` and `custom-attribute`, we can pass them as a dictionary to the `attrs` argument.\n",
    "\n",
    "The ideal case in searching is when you know the ID of an element, thanks to its uniqueness. In this case, we can safely use the `find()` method instead, which returns only the first result. Otherwise, combinations of tags and attributes will help you finding the element you want very quickly.\n",
    "\n",
    "[`find_all()`]: https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-all\n",
    "[first argument]: https://www.crummy.com/software/BeautifulSoup/bs4/doc/#kinds-of-filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6fec2459-5657-443b-84ed-c3a17132ba36",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9b8b8d2f-528f-42fb-9b08-7f78dcad5201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(id='link2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6424c531-320c-47f8-b3c2-dcac103c1919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<title>The Dormouse's story</title>,\n",
       " <time class=\"story\">2000-01-01 06:00:00</time>]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(re.compile('^t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a68bbf85-c8b9-4024-a070-47ff6be0eae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"https://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('a', class_='sister')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "36753a4c-04eb-4e65-94b4-d4b314eb0694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"https://example.com/elsie\" id=\"link1\">Elsie</a>]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attrs = {\n",
    "    'class': 'sister',\n",
    "    'href': re.compile('https\\S+')\n",
    "}\n",
    "\n",
    "soup.find_all('a', attrs=attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b83545-df7e-4ed0-af7b-b55c90f08fcc",
   "metadata": {},
   "source": [
    ":::{admonition} Case study\n",
    ":class: seealso\n",
    "\n",
    "Let's use Beautiful Soup to crawl all fiction books in the page https://books.toscrape.com/catalogue/category/books/fiction_10/index.html. This website does not use REST APIs, so the only way is parsing its HTML souce. Our crawler contains two phases, (1) gathering book URLs and (2) actually crawling book information.\n",
    "\n",
    "*Phase 1*\n",
    "- First, examine the URL structure to find that we can replace \"index\" with \"page-n\" to access pages. The index of page will be set incrementally, as we will get an 404 error message when it exceeds the maximum number.\n",
    "- In the first page, use the *inspect* tool of Chrome to find book containers (each contains image, title, ratings and price). We observe that each container has 4 classes `col-xs-6`, `col-sm-4`, `col-md-3` and `col-lg-3`. You can re-check this information by searching and counting the number of elements that use all 4 classes. There are 20 of them, which matches the number of books the page shows.\n",
    "- All the information in the containers also appear in book pages. So, the only data we get here is book URLs. Note that URLs here are relative, we can easily convert in into full path using the function [`urljoin()`].\n",
    "\n",
    "*Phase 2*\n",
    "- Access each URL collected in the first phase. Locate the content container.\n",
    "- Extract the important fields and add them to a Pandas dataframe. There is no new technique in this phase.\n",
    "\n",
    ":::\n",
    "\n",
    "[`urljoin()`]: https://docs.python.org/3/library/urllib.parse.html#urllib.parse.urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56f8f69f-3e34-49ba-94fd-ce2caacd923f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T15:08:58.465235Z",
     "iopub.status.busy": "2023-01-09T15:08:58.463817Z",
     "iopub.status.idle": "2023-01-09T15:08:59.990623Z",
     "shell.execute_reply": "2023-01-09T15:08:59.989699Z",
     "shell.execute_reply.started": "2023-01-09T15:08:58.465196Z"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import bs4\n",
    "import pandas as pd\n",
    "import time\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f9ee252-5000-4915-8dbf-4ee391c0dbc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-22T06:20:29.552371Z",
     "iopub.status.busy": "2023-01-22T06:20:29.551637Z",
     "iopub.status.idle": "2023-01-22T06:20:29.558186Z",
     "shell.execute_reply": "2023-01-22T06:20:29.557258Z",
     "shell.execute_reply.started": "2023-01-22T06:20:29.552321Z"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def crawl_book_url(urlBase) -> list:\n",
    "    listUrlBook = []\n",
    "    nPage = 1\n",
    "    while True:\n",
    "        # access crawl page and check if it loads successfully (status 200)\n",
    "        urlPage = urlBase.replace('index', f'page-{nPage}')\n",
    "        response = requests.get(urlPage)\n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "        \n",
    "        # create soup\n",
    "        html = response.text\n",
    "        soup = bs4.BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # get book containers\n",
    "        listContainer = soup.find_all('li', class_='col-xs-6 col-sm-4 col-md-3 col-lg-3')\n",
    "        \n",
    "        # get book urls\n",
    "        for container in listContainer:\n",
    "            href = container.article.h3.a['href']\n",
    "            urlBook = urljoin(urlBase, href)\n",
    "            listUrlBook.append(urlBook)\n",
    "        \n",
    "        # advance to the next page\n",
    "        nPage += 1\n",
    "    \n",
    "    return listUrlBook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "570f0d96-6ffd-4b0a-8962-a718452522b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-22T06:34:13.502969Z",
     "iopub.status.busy": "2023-01-22T06:34:13.502619Z",
     "iopub.status.idle": "2023-01-22T06:34:13.509214Z",
     "shell.execute_reply": "2023-01-22T06:34:13.508128Z",
     "shell.execute_reply.started": "2023-01-22T06:34:13.502944Z"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def crawl_book_info(listUrlBook:list):\n",
    "    data = []\n",
    "    for urlBook in listUrlBook:\n",
    "        response = requests.get(urlBook)\n",
    "        html = response.content\n",
    "        soup = bs4.BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        container = soup.find('article', class_='product_page')\n",
    "        title = container.find('div', class_='product_main').h1.text\n",
    "        description = container.find('p', class_=False).text\n",
    "        \n",
    "        table = container.find('table', class_='table-striped').prettify()\n",
    "        table = pd.read_html(table)\n",
    "        table = pd.concat(table)\n",
    "        table = table.set_index(0)[1]\n",
    "        \n",
    "        upc = table['UPC']\n",
    "        price = table['Price (excl. tax)']\n",
    "        price = float(re.findall('\\d+\\.\\d+', price)[0])\n",
    "        tax = table['Tax']\n",
    "        tax = float(re.findall('\\d+\\.\\d+', tax)[0])\n",
    "        \n",
    "        sample = {\n",
    "            'upc': upc,\n",
    "            'title': title,\n",
    "            'price': price,\n",
    "            'tax': tax,\n",
    "        }\n",
    "        data.append(sample)\n",
    "        \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f134f91a-1196-4060-a1ea-273236152f4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-22T06:20:32.066652Z",
     "iopub.status.busy": "2023-01-22T06:20:32.066268Z",
     "iopub.status.idle": "2023-01-22T06:20:39.632750Z",
     "shell.execute_reply": "2023-01-22T06:20:39.631374Z",
     "shell.execute_reply.started": "2023-01-22T06:20:32.066623Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urlBase = 'https://books.toscrape.com/catalogue/category/books/fiction_10/index.html'\n",
    "listUrlBook = crawl_book_url(urlBase)\n",
    "len(listUrlBook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15cb55fd-c27f-4c6c-9f18-0c0d74ab8233",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-22T06:34:15.170127Z",
     "iopub.status.busy": "2023-01-22T06:34:15.169600Z",
     "iopub.status.idle": "2023-01-22T06:34:22.754863Z",
     "shell.execute_reply": "2023-01-22T06:34:22.754137Z",
     "shell.execute_reply.started": "2023-01-22T06:34:15.170094Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>upc</th>\n",
       "      <th>title</th>\n",
       "      <th>price</th>\n",
       "      <th>tax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6957f44c3847a760</td>\n",
       "      <td>Soumission</td>\n",
       "      <td>50.10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b12b89017878a60d</td>\n",
       "      <td>Private Paris (Private #10)</td>\n",
       "      <td>47.61</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8d455c7539795d2a</td>\n",
       "      <td>We Love You, Charlie Freeman</td>\n",
       "      <td>50.27</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>709822d0b5bcb7f4</td>\n",
       "      <td>Thirst</td>\n",
       "      <td>17.27</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d01ac97e2b8947c2</td>\n",
       "      <td>The Murder That Never Was (Forensic Instincts #5)</td>\n",
       "      <td>54.11</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                upc                                              title  price  \\\n",
       "0  6957f44c3847a760                                         Soumission  50.10   \n",
       "1  b12b89017878a60d                        Private Paris (Private #10)  47.61   \n",
       "2  8d455c7539795d2a                       We Love You, Charlie Freeman  50.27   \n",
       "3  709822d0b5bcb7f4                                             Thirst  17.27   \n",
       "4  d01ac97e2b8947c2  The Murder That Never Was (Forensic Instincts #5)  54.11   \n",
       "\n",
       "   tax  \n",
       "0  0.0  \n",
       "1  0.0  \n",
       "2  0.0  \n",
       "3  0.0  \n",
       "4  0.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = crawl_book_info(listUrlBook[:5])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b862c62-fd56-45bd-bca2-9bb98a32fc6c",
   "metadata": {},
   "source": [
    "## 3. Web driver\n",
    "In many scenarios, a website may require users to perform some actions to reveal data, which Beautiful Soup cannot handle. In such cases, we need a *web driver* that can emulate user interation with browsers. [Selenium] is a library born to serve this purpose; it is designed for automation test but can also be used for web scraping.\n",
    "\n",
    "[Selenium]: https://github.com/SeleniumHQ/selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d99d9c2-97fa-4936-8c34-9920611c1ea0",
   "metadata": {},
   "source": [
    "### 3.1. Driver initialization\n",
    "In order to use Selenium, we must first set up a web driver (Chrome is recommended). This is done by downloading [Chrome Driver] manually and giving the path to Selenium. But thanks to [Webdriver Manager], drivers for different browsers will be automatically detected and downloaded for us. After intializing, Selenium will open a driver window for us, so we can monitor how actions being performed. We can configure the web driver indifferent ways:\n",
    "- Add some [options], such as *headless* and *start-maximized*\n",
    "- Use a custom [page load strategy]\n",
    "- Quit the driver to free up memory\n",
    "\n",
    "[Chrome Driver]: https://chromedriver.chromium.org/home\n",
    "[Webdriver Manager]: https://github.com/SergeyPirogov/webdriver_manager\n",
    "[options]: https://www.selenium.dev/documentation/webdriver/drivers/options/\n",
    "[page load strategy]: https://www.selenium.dev/documentation/webdriver/drivers/options/#pageloadstrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fd3c70d-466d-4ce1-91a9-1e0f0c0c0bb0",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de7f7837-f55c-4f77-b7a1-3bd9dacc7230",
   "metadata": {},
   "outputs": [],
   "source": [
    "service = Service(ChromeDriverManager().install())\n",
    "options = Options()\n",
    "# options.add_argument('--headless')\n",
    "# options.add_argument('--window-size=1920,1080')\n",
    "options.add_argument('start-maximized')\n",
    "\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "time.sleep(5)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b130e1ca-d8c6-4d6a-a2c1-564397e5c7e1",
   "metadata": {},
   "source": [
    "### 3.2. Element locating\n",
    "In Selenium, we find an element using the [`find_elements()`] method with the first argument being a [`By`] locator. Besides basic element locating strategies using tags and attributes, Selenium provides two convinient locators, XPath and CSS selector. You can simply use them by right-click an element and copy its XPath/selector, but they worth being learned carefully.\n",
    "\n",
    "[`find_elements()`]: https://www.selenium.dev/documentation/webdriver/elements/finders\n",
    "[`By`]: https://selenium-python.readthedocs.io/locating-elements.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a183d5-aa3c-4ff8-9db8-e4887b1be232",
   "metadata": {},
   "source": [
    "#### XPath\n",
    "The general syntax of XPath is `/element/element/...`, starts from root. You can leave the first element blank `//element/element/...` to turn the absolute path into relative. The basic syntaxes for searching elements are `tag[@attr='value']` and `tag[ordinal]`. For example:\n",
    "- `*[@id='promotion']` matches any tag that has `id='promotion'`\n",
    "- `div[@class='breadcrumb']` matches elements that have `<div class='breadcrumb'>`\n",
    "- `span[7]` matches any `<span>` element that is the seventh child of its parent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9da4ec-32bf-40da-a390-2492f10ed35b",
   "metadata": {},
   "source": [
    "#### CSS selector\n",
    "The general syntax of CSS selector is `element > element > ...`, being relative by nature. The basic syntaxes for searching elements are `tag.class`, `tag#id`, `tag[attr=value]`, `tag:func(args)`. For example:\n",
    "- `#promotion` matches any tag that has `id='promotion'`\n",
    "- `div.breadcrumb` matches elements that have `<div class='breadcrumb'>`\n",
    "- `span[role=alert]` matches elements that have `<span role='alert'>`\n",
    "- `span:nth-child(7)` matches any `<span>` element that is the seventh child of its parent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafd183c-ff94-48ae-a1cc-c163aeff8032",
   "metadata": {},
   "source": [
    "#### Implementation\n",
    "In this section, we attemp to crawl first 20 books in the fiction category. We try traditional way first, using tags and classes, only to know that Selenium does not support this style very well. Next, we try to use XPath and CSS selector by copying those of a single book from Chrome and then tweaking them to match all 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b2c20a8-873a-4171-a318-9f0297133840",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T01:23:42.678360Z",
     "iopub.status.busy": "2023-01-11T01:23:42.678013Z",
     "iopub.status.idle": "2023-01-11T01:23:42.817499Z",
     "shell.execute_reply": "2023-01-11T01:23:42.816921Z",
     "shell.execute_reply.started": "2023-01-11T01:23:42.678302Z"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3012a2bd-1cd0-40b5-b57a-1ffb33f0a067",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T01:24:19.048866Z",
     "iopub.status.busy": "2023-01-11T01:24:19.048265Z",
     "iopub.status.idle": "2023-01-11T01:24:23.421310Z",
     "shell.execute_reply": "2023-01-11T01:24:23.420093Z",
     "shell.execute_reply.started": "2023-01-11T01:24:19.048831Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Downloading:  97%|█████████▋| 8.34M/8.61M [00:02<00:00, 5.49MB/s]"
     ]
    }
   ],
   "source": [
    "service = Service(ChromeDriverManager().install())\n",
    "options = Options()\n",
    "options.add_argument('start-maximized')\n",
    "driver = webdriver.Chrome(service=service, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5d85851-8c2a-4ae5-b783-957051625064",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T01:24:49.708159Z",
     "iopub.status.busy": "2023-01-11T01:24:49.707646Z",
     "iopub.status.idle": "2023-01-11T01:24:50.059012Z",
     "shell.execute_reply": "2023-01-11T01:24:50.058142Z",
     "shell.execute_reply.started": "2023-01-11T01:24:49.708124Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = 'https://books.toscrape.com/catalogue/category/books/fiction_10/index.html'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecdcb1a9-ed27-4c69-9f74-2559049707ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T01:26:18.629655Z",
     "iopub.status.busy": "2023-01-11T01:26:18.629236Z",
     "iopub.status.idle": "2023-01-11T01:26:18.655181Z",
     "shell.execute_reply": "2023-01-11T01:26:18.653836Z",
     "shell.execute_reply.started": "2023-01-11T01:26:18.629628Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listBook = driver.find_elements(By.CLASS_NAME, 'col-xs-6')\n",
    "len(listBook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7a0ae38-ee9b-4cdd-b930-b806758dd186",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-10T18:13:12.814211Z",
     "iopub.status.busy": "2023-01-10T18:13:12.813775Z",
     "iopub.status.idle": "2023-01-10T18:13:12.833818Z",
     "shell.execute_reply": "2023-01-10T18:13:12.832361Z",
     "shell.execute_reply.started": "2023-01-10T18:13:12.814170Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xpath = '//*[@id=\"default\"]/div/div/div/div/section/div[2]/ol/li'\n",
    "listBook = driver.find_elements(By.XPATH, xpath)\n",
    "len(listBook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba86ec04-01e8-4d32-b302-617a66c02d94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-10T18:15:28.313563Z",
     "iopub.status.busy": "2023-01-10T18:15:28.313091Z",
     "iopub.status.idle": "2023-01-10T18:15:28.328609Z",
     "shell.execute_reply": "2023-01-10T18:15:28.327322Z",
     "shell.execute_reply.started": "2023-01-10T18:15:28.313528Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = '#default > div > div > div > div > section > div:nth-child(2) > ol > li'\n",
    "listBook = driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "len(listBook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b00a19c2-1fae-467f-9264-67eb883095cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-11T01:26:24.742409Z",
     "iopub.status.busy": "2023-01-11T01:26:24.742037Z",
     "iopub.status.idle": "2023-01-11T01:26:24.857891Z",
     "shell.execute_reply": "2023-01-11T01:26:24.857296Z",
     "shell.execute_reply.started": "2023-01-11T01:26:24.742383Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fb2abb-9ce2-4b46-a221-7539d5b646e0",
   "metadata": {},
   "source": [
    "### 3.3. Actions\n",
    "In this section we use Selenium to perform basic [actions] on a website: clicking, typing keys and hovering.\n",
    "\n",
    "[actions]: https://selenium-python.readthedocs.io/navigating.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ce3061d-73f2-4c9b-aa97-678353d99738",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46c555e5-60db-4345-8b2b-4ea887b01548",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "service = Service(ChromeDriverManager().install())\n",
    "options = Options()\n",
    "options.add_argument('start-maximized')\n",
    "driver = webdriver.Chrome(service=service, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc5e35a3-298f-4652-b09e-b385a3fbc71e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = 'https://www.tensorflow.org/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9a8b78-9bf5-4644-af6b-ae1736001154",
   "metadata": {},
   "source": [
    "#### Send keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb027d9a-49b3-408d-a043-bed52ce2c621",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/html/body/section/devsite-header/div/div[1]/div/div/div[2]/devsite-search/form/div[1]/div/input'\n",
    "element = driver.find_element(By.XPATH, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d76b90a2-9255-4475-a1dd-9d5ce549b32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "element.send_keys('lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "53d83db1-0096-4487-a591-9b893698fff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "element.send_keys(Keys.CONTROL, 'A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "790459a2-8f38-4374-8b17-4cb45fa78a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "element.send_keys(Keys.CONTROL, 'C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9baebaf9-a8a5-44c7-8950-1481abf9eac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "element.send_keys(Keys.DOWN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7a3c14dc-1ef4-4254-93aa-e8d316a7b036",
   "metadata": {},
   "outputs": [],
   "source": [
    "element.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e99ffe8-3787-44e3-92cc-40662b75a1a8",
   "metadata": {},
   "source": [
    "#### Hover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c42fb0cf-7c9f-4475-977a-4cc453cd895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chains = ActionChains(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "842298b1-cf17-4ebd-93ac-c86dbea1ea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/html/body/section/devsite-header/div/div[1]/div/div/div[2]/div[1]/devsite-tabs/nav/tab[6]/a[1]'\n",
    "element = driver.find_element(By.XPATH, path)\n",
    "chains.move_to_element(element).perform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6e2b9d5a-a6ea-4f64-8986-0629a341cf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/html/body/section/devsite-header/div/div[1]/div/div/div[2]/div[1]/devsite-tabs/nav/tab[5]/a[1]'\n",
    "element = driver.find_element(By.XPATH, path)\n",
    "chains.move_to_element(element).perform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eceacd67-ecff-4b10-8ab5-077c34f4a116",
   "metadata": {},
   "outputs": [],
   "source": [
    "element.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2fa304-c6d9-4480-b727-4a9c529911cd",
   "metadata": {},
   "source": [
    "## Resources\n",
    "- *gregreda - [Web Scraping 201: finding the API](http://www.gregreda.com/2015/02/15/web-scraping-finding-the-api/)*\n",
    "- *jovian - [Introduction to Web Scraping and REST APIs](https://jovian.ai/aakashns/python-web-scraping-and-rest-api)*\n",
    "- *blog.devgenius - [Scrape Data without Selenium by Exposing Hidden APIs](https://blog.devgenius.io/scrape-data-without-selenium-by-exposing-hidden-apis-946b23850d47)*\n",
    "- *medium - [Web Crawling Made Easy with Scrapy and REST API](https://medium.com/@geneng/web-crawling-made-easy-with-scrapy-and-rest-api-ed993e84abd3)*\n",
    "- *w3schools - [XPath syntax](https://www.w3schools.com/xml/xpath_syntax.asp)*\n",
    "- *w3schools - [CSS selector reference](https://www.w3schools.com/cssref/css_selectors.php)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2cdcee-9d27-4465-8120-e52cef5b119a",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cd06d4-22b1-4e42-99d3-25fd128c316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47361a32-7697-49d9-800a-548082d37efc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install webdriver-manager"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

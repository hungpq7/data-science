{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Miscellaneous techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Mapping\n",
    "Mapping in PySpark requires using the <code style='font-size:13px;'>udf()</code> function, which allows a Python function to work on PySpark dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark; findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.enableHiveSupport().getOrCreate()\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>manufacturer</th><th>model</th><th>type</th><th>min_price</th><th>price</th><th>max_price</th><th>mpg_city</th><th>mpg_highway</th><th>airbags</th><th>drive_train</th><th>cylinders</th><th>engine_size</th><th>horsepower</th><th>rpm</th><th>rev_per_mile</th><th>man_trans_avail</th><th>fuel_tank_capacity</th><th>passengers</th><th>length</th><th>wheelbase</th><th>width</th><th>turn_circle</th><th>rear_seat_room</th><th>luggage_room</th><th>weight</th><th>origin</th><th>make</th></tr>\n",
       "<tr><td>Chevrolet</td><td>Cavalier</td><td>Compact</td><td>8.5</td><td>13.4</td><td>18.3</td><td>25</td><td>36</td><td>None</td><td>Front</td><td>4.0</td><td>2.2</td><td>110</td><td>5200</td><td>2380</td><td>Yes</td><td>15.2</td><td>5</td><td>182</td><td>101</td><td>66</td><td>38</td><td>25.0</td><td>13.0</td><td>2490</td><td>USA</td><td>Chevrolet Cavalier</td></tr>\n",
       "<tr><td>Chevrolet</td><td>Corsica</td><td>Compact</td><td>11.4</td><td>11.4</td><td>11.4</td><td>25</td><td>34</td><td>Driver only</td><td>Front</td><td>4.0</td><td>2.2</td><td>110</td><td>5200</td><td>2665</td><td>Yes</td><td>15.6</td><td>5</td><td>184</td><td>103</td><td>68</td><td>39</td><td>26.0</td><td>14.0</td><td>2785</td><td>USA</td><td>Chevrolet Corsica</td></tr>\n",
       "<tr><td>Chevrolet</td><td>Camaro</td><td>Sporty</td><td>13.4</td><td>15.1</td><td>16.8</td><td>19</td><td>28</td><td>Driver &amp; Passenger</td><td>Rear</td><td>6.0</td><td>3.4</td><td>160</td><td>4600</td><td>1805</td><td>Yes</td><td>15.5</td><td>4</td><td>193</td><td>101</td><td>74</td><td>43</td><td>25.0</td><td>13.0</td><td>3240</td><td>USA</td><td>Chevrolet Camaro</td></tr>\n",
       "<tr><td>Chevrolet</td><td>Lumina</td><td>Midsize</td><td>13.4</td><td>15.9</td><td>18.4</td><td>21</td><td>29</td><td>None</td><td>Front</td><td>4.0</td><td>2.2</td><td>110</td><td>5200</td><td>2595</td><td>No</td><td>16.5</td><td>6</td><td>198</td><td>108</td><td>71</td><td>40</td><td>28.5</td><td>16.0</td><td>3195</td><td>USA</td><td>Chevrolet Lumina</td></tr>\n",
       "<tr><td>Chevrolet</td><td>Lumina_APV</td><td>Van</td><td>14.7</td><td>16.3</td><td>18.0</td><td>18</td><td>23</td><td>None</td><td>Front</td><td>6.0</td><td>3.8</td><td>170</td><td>4800</td><td>1690</td><td>No</td><td>20.0</td><td>7</td><td>178</td><td>110</td><td>74</td><td>44</td><td>30.5</td><td>null</td><td>3715</td><td>USA</td><td>Chevrolet Lumina_APV</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------------+----------+-------+---------+-----+---------+--------+-----------+------------------+-----------+---------+-----------+----------+----+------------+---------------+------------------+----------+------+---------+-----+-----------+--------------+------------+------+------+--------------------+\n",
       "|manufacturer|     model|   type|min_price|price|max_price|mpg_city|mpg_highway|           airbags|drive_train|cylinders|engine_size|horsepower| rpm|rev_per_mile|man_trans_avail|fuel_tank_capacity|passengers|length|wheelbase|width|turn_circle|rear_seat_room|luggage_room|weight|origin|                make|\n",
       "+------------+----------+-------+---------+-----+---------+--------+-----------+------------------+-----------+---------+-----------+----------+----+------------+---------------+------------------+----------+------+---------+-----+-----------+--------------+------------+------+------+--------------------+\n",
       "|   Chevrolet|  Cavalier|Compact|      8.5| 13.4|     18.3|      25|         36|              None|      Front|      4.0|        2.2|       110|5200|        2380|            Yes|              15.2|         5|   182|      101|   66|         38|          25.0|        13.0|  2490|   USA|  Chevrolet Cavalier|\n",
       "|   Chevrolet|   Corsica|Compact|     11.4| 11.4|     11.4|      25|         34|       Driver only|      Front|      4.0|        2.2|       110|5200|        2665|            Yes|              15.6|         5|   184|      103|   68|         39|          26.0|        14.0|  2785|   USA|   Chevrolet Corsica|\n",
       "|   Chevrolet|    Camaro| Sporty|     13.4| 15.1|     16.8|      19|         28|Driver & Passenger|       Rear|      6.0|        3.4|       160|4600|        1805|            Yes|              15.5|         4|   193|      101|   74|         43|          25.0|        13.0|  3240|   USA|    Chevrolet Camaro|\n",
       "|   Chevrolet|    Lumina|Midsize|     13.4| 15.9|     18.4|      21|         29|              None|      Front|      4.0|        2.2|       110|5200|        2595|             No|              16.5|         6|   198|      108|   71|         40|          28.5|        16.0|  3195|   USA|    Chevrolet Lumina|\n",
       "|   Chevrolet|Lumina_APV|    Van|     14.7| 16.3|     18.0|      18|         23|              None|      Front|      6.0|        3.8|       170|4800|        1690|             No|              20.0|         7|   178|      110|   74|         44|          30.5|        null|  3715|   USA|Chevrolet Lumina_APV|\n",
       "+------------+----------+-------+---------+-----+---------+--------+-----------+------------------+-----------+---------+-----------+----------+----+------------+---------------+------------------+----------+------+---------+-----+-----------+--------------+------------+------+------+--------------------+"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfCar = pd.read_csv('data/cars.csv')\n",
    "dfCar = spark.createDataFrame(dfCar.astype(str)).replace('nan', None)\n",
    "dfCar = dfCar.withColumn('price', F.col('price').cast('float'))\n",
    "dfCar.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11.600000381469727, 15.899999618530273, 18.799999237060547]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfCar.approxQuantile('price', [0.25, 0.5, 0.75], relativeError=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tmp_getPriceLevel(price):\n",
    "    if price < 11.6:\n",
    "        return 'very low'\n",
    "    elif price < 15.9:\n",
    "        return 'low'\n",
    "    elif price < 18.8:\n",
    "        return 'high'\n",
    "    else:\n",
    "        return 'very high'\n",
    "\n",
    "getPriceLevel = F.udf(lambda price: tmp_getPriceLevel(price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>price</th><th>price_level</th></tr>\n",
       "<tr><td>13.4</td><td>low</td></tr>\n",
       "<tr><td>11.4</td><td>very low</td></tr>\n",
       "<tr><td>15.1</td><td>low</td></tr>\n",
       "<tr><td>15.9</td><td>low</td></tr>\n",
       "<tr><td>16.3</td><td>high</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----+-----------+\n",
       "|price|price_level|\n",
       "+-----+-----------+\n",
       "| 13.4|        low|\n",
       "| 11.4|   very low|\n",
       "| 15.1|        low|\n",
       "| 15.9|        low|\n",
       "| 16.3|       high|\n",
       "+-----+-----------+"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfCar.select('price', getPriceLevel('price').alias('price_level')).limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Window functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import findspark; findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.enableHiveSupport().getOrCreate()\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>employee</th><th>department</th><th>salary</th></tr>\n",
       "<tr><td>James</td><td>Sales</td><td>3000</td></tr>\n",
       "<tr><td>Harry</td><td>Sales</td><td>3500</td></tr>\n",
       "<tr><td>Ash</td><td>Sales</td><td>3000</td></tr>\n",
       "<tr><td>Michael</td><td>Sales</td><td>4600</td></tr>\n",
       "<tr><td>Robert</td><td>Sales</td><td>4100</td></tr>\n",
       "<tr><td>Maria</td><td>Finance</td><td>3000</td></tr>\n",
       "<tr><td>Wayne</td><td>Sales</td><td>3000</td></tr>\n",
       "<tr><td>Scott</td><td>Finance</td><td>3300</td></tr>\n",
       "<tr><td>Jen</td><td>Finance</td><td>3900</td></tr>\n",
       "<tr><td>Jeff</td><td>Marketing</td><td>3000</td></tr>\n",
       "<tr><td>Kumar</td><td>Marketing</td><td>2000</td></tr>\n",
       "<tr><td>Saif</td><td>Sales</td><td>4100</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+----------+------+\n",
       "|employee|department|salary|\n",
       "+--------+----------+------+\n",
       "|   James|     Sales|  3000|\n",
       "|   Harry|     Sales|  3500|\n",
       "|     Ash|     Sales|  3000|\n",
       "| Michael|     Sales|  4600|\n",
       "|  Robert|     Sales|  4100|\n",
       "|   Maria|   Finance|  3000|\n",
       "|   Wayne|     Sales|  3000|\n",
       "|   Scott|   Finance|  3300|\n",
       "|     Jen|   Finance|  3900|\n",
       "|    Jeff| Marketing|  3000|\n",
       "|   Kumar| Marketing|  2000|\n",
       "|    Saif|     Sales|  4100|\n",
       "+--------+----------+------+"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = (\n",
    "    ('James', 'Sales', 3000),\n",
    "    ('Harry', 'Sales', 3500),\n",
    "    ('Ash', 'Sales', 3000),\n",
    "    ('Michael', 'Sales', 4600),\n",
    "    ('Robert', 'Sales', 4100),\n",
    "    ('Maria', 'Finance', 3000),\n",
    "    ('Wayne', 'Sales', 3000),\n",
    "    ('Scott', 'Finance', 3300),\n",
    "    ('Jen', 'Finance', 3900),\n",
    "    ('Jeff', 'Marketing', 3000),\n",
    "    ('Kumar', 'Marketing', 2000),\n",
    "    ('Saif', 'Sales', 4100))\n",
    " \n",
    "schema = ['employee', 'department', 'salary']\n",
    "\n",
    "dfSalary = spark.createDataFrame(data, schema)\n",
    "dfSalary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to activate window functions, firstly initialize the window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "window = Window.partitionBy('department').orderBy('salary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>employee</th><th>department</th><th>salary</th><th>row_number</th><th>rank</th><th>dense_rank</th><th>percent_rank</th><th>cume_dist</th><th>ntile_3</th></tr>\n",
       "<tr><td>James</td><td>Sales</td><td>3000</td><td>1</td><td>1</td><td>1</td><td>0.0</td><td>0.43</td><td>1</td></tr>\n",
       "<tr><td>Ash</td><td>Sales</td><td>3000</td><td>2</td><td>1</td><td>1</td><td>0.0</td><td>0.43</td><td>1</td></tr>\n",
       "<tr><td>Wayne</td><td>Sales</td><td>3000</td><td>3</td><td>1</td><td>1</td><td>0.0</td><td>0.43</td><td>1</td></tr>\n",
       "<tr><td>Harry</td><td>Sales</td><td>3500</td><td>4</td><td>4</td><td>2</td><td>0.5</td><td>0.57</td><td>2</td></tr>\n",
       "<tr><td>Robert</td><td>Sales</td><td>4100</td><td>5</td><td>5</td><td>3</td><td>0.67</td><td>0.86</td><td>2</td></tr>\n",
       "<tr><td>Saif</td><td>Sales</td><td>4100</td><td>6</td><td>5</td><td>3</td><td>0.67</td><td>0.86</td><td>3</td></tr>\n",
       "<tr><td>Michael</td><td>Sales</td><td>4600</td><td>7</td><td>7</td><td>4</td><td>1.0</td><td>1.0</td><td>3</td></tr>\n",
       "<tr><td>Maria</td><td>Finance</td><td>3000</td><td>1</td><td>1</td><td>1</td><td>0.0</td><td>0.33</td><td>1</td></tr>\n",
       "<tr><td>Scott</td><td>Finance</td><td>3300</td><td>2</td><td>2</td><td>2</td><td>0.5</td><td>0.67</td><td>2</td></tr>\n",
       "<tr><td>Jen</td><td>Finance</td><td>3900</td><td>3</td><td>3</td><td>3</td><td>1.0</td><td>1.0</td><td>3</td></tr>\n",
       "<tr><td>Kumar</td><td>Marketing</td><td>2000</td><td>1</td><td>1</td><td>1</td><td>0.0</td><td>0.5</td><td>1</td></tr>\n",
       "<tr><td>Jeff</td><td>Marketing</td><td>3000</td><td>2</td><td>2</td><td>2</td><td>1.0</td><td>1.0</td><td>2</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+----------+------+----------+----+----------+------------+---------+-------+\n",
       "|employee|department|salary|row_number|rank|dense_rank|percent_rank|cume_dist|ntile_3|\n",
       "+--------+----------+------+----------+----+----------+------------+---------+-------+\n",
       "|   James|     Sales|  3000|         1|   1|         1|         0.0|     0.43|      1|\n",
       "|     Ash|     Sales|  3000|         2|   1|         1|         0.0|     0.43|      1|\n",
       "|   Wayne|     Sales|  3000|         3|   1|         1|         0.0|     0.43|      1|\n",
       "|   Harry|     Sales|  3500|         4|   4|         2|         0.5|     0.57|      2|\n",
       "|  Robert|     Sales|  4100|         5|   5|         3|        0.67|     0.86|      2|\n",
       "|    Saif|     Sales|  4100|         6|   5|         3|        0.67|     0.86|      3|\n",
       "| Michael|     Sales|  4600|         7|   7|         4|         1.0|      1.0|      3|\n",
       "|   Maria|   Finance|  3000|         1|   1|         1|         0.0|     0.33|      1|\n",
       "|   Scott|   Finance|  3300|         2|   2|         2|         0.5|     0.67|      2|\n",
       "|     Jen|   Finance|  3900|         3|   3|         3|         1.0|      1.0|      3|\n",
       "|   Kumar| Marketing|  2000|         1|   1|         1|         0.0|      0.5|      1|\n",
       "|    Jeff| Marketing|  3000|         2|   2|         2|         1.0|      1.0|      2|\n",
       "+--------+----------+------+----------+----+----------+------------+---------+-------+"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfSalary\\\n",
    "    .withColumn('row_number', F.row_number().over(window))\\\n",
    "    .withColumn('rank', F.rank().over(window))\\\n",
    "    .withColumn('dense_rank', F.dense_rank().over(window))\\\n",
    "    .withColumn('percent_rank', F.round(F.percent_rank().over(window), 2))\\\n",
    "    .withColumn('cume_dist', F.round(F.cume_dist().over(window), 2))\\\n",
    "    .withColumn('ntile_3', F.ntile(3).over(window))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>employee</th><th>department</th><th>salary</th><th>lag_1</th><th>lead_2</th></tr>\n",
       "<tr><td>James</td><td>Sales</td><td>3000</td><td>null</td><td>3000</td></tr>\n",
       "<tr><td>Ash</td><td>Sales</td><td>3000</td><td>3000</td><td>3500</td></tr>\n",
       "<tr><td>Wayne</td><td>Sales</td><td>3000</td><td>3000</td><td>4100</td></tr>\n",
       "<tr><td>Harry</td><td>Sales</td><td>3500</td><td>3000</td><td>4100</td></tr>\n",
       "<tr><td>Robert</td><td>Sales</td><td>4100</td><td>3500</td><td>4600</td></tr>\n",
       "<tr><td>Saif</td><td>Sales</td><td>4100</td><td>4100</td><td>null</td></tr>\n",
       "<tr><td>Michael</td><td>Sales</td><td>4600</td><td>4100</td><td>null</td></tr>\n",
       "<tr><td>Maria</td><td>Finance</td><td>3000</td><td>null</td><td>3900</td></tr>\n",
       "<tr><td>Scott</td><td>Finance</td><td>3300</td><td>3000</td><td>null</td></tr>\n",
       "<tr><td>Jen</td><td>Finance</td><td>3900</td><td>3300</td><td>null</td></tr>\n",
       "<tr><td>Kumar</td><td>Marketing</td><td>2000</td><td>null</td><td>null</td></tr>\n",
       "<tr><td>Jeff</td><td>Marketing</td><td>3000</td><td>2000</td><td>null</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+----------+------+-----+------+\n",
       "|employee|department|salary|lag_1|lead_2|\n",
       "+--------+----------+------+-----+------+\n",
       "|   James|     Sales|  3000| null|  3000|\n",
       "|     Ash|     Sales|  3000| 3000|  3500|\n",
       "|   Wayne|     Sales|  3000| 3000|  4100|\n",
       "|   Harry|     Sales|  3500| 3000|  4100|\n",
       "|  Robert|     Sales|  4100| 3500|  4600|\n",
       "|    Saif|     Sales|  4100| 4100|  null|\n",
       "| Michael|     Sales|  4600| 4100|  null|\n",
       "|   Maria|   Finance|  3000| null|  3900|\n",
       "|   Scott|   Finance|  3300| 3000|  null|\n",
       "|     Jen|   Finance|  3900| 3300|  null|\n",
       "|   Kumar| Marketing|  2000| null|  null|\n",
       "|    Jeff| Marketing|  3000| 2000|  null|\n",
       "+--------+----------+------+-----+------+"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfSalary\\\n",
    "    .withColumn('lag_1', F.lag('salary', 1).over(window))\\\n",
    "    .withColumn('lead_2', F.lead('salary', 2).over(window))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>employee</th><th>department</th><th>salary</th><th>cumsum</th></tr>\n",
       "<tr><td>James</td><td>Sales</td><td>3000</td><td>9000</td></tr>\n",
       "<tr><td>Ash</td><td>Sales</td><td>3000</td><td>9000</td></tr>\n",
       "<tr><td>Wayne</td><td>Sales</td><td>3000</td><td>9000</td></tr>\n",
       "<tr><td>Harry</td><td>Sales</td><td>3500</td><td>12500</td></tr>\n",
       "<tr><td>Robert</td><td>Sales</td><td>4100</td><td>20700</td></tr>\n",
       "<tr><td>Saif</td><td>Sales</td><td>4100</td><td>20700</td></tr>\n",
       "<tr><td>Michael</td><td>Sales</td><td>4600</td><td>25300</td></tr>\n",
       "<tr><td>Maria</td><td>Finance</td><td>3000</td><td>3000</td></tr>\n",
       "<tr><td>Scott</td><td>Finance</td><td>3300</td><td>6300</td></tr>\n",
       "<tr><td>Jen</td><td>Finance</td><td>3900</td><td>10200</td></tr>\n",
       "<tr><td>Kumar</td><td>Marketing</td><td>2000</td><td>2000</td></tr>\n",
       "<tr><td>Jeff</td><td>Marketing</td><td>3000</td><td>5000</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+----------+------+------+\n",
       "|employee|department|salary|cumsum|\n",
       "+--------+----------+------+------+\n",
       "|   James|     Sales|  3000|  9000|\n",
       "|     Ash|     Sales|  3000|  9000|\n",
       "|   Wayne|     Sales|  3000|  9000|\n",
       "|   Harry|     Sales|  3500| 12500|\n",
       "|  Robert|     Sales|  4100| 20700|\n",
       "|    Saif|     Sales|  4100| 20700|\n",
       "| Michael|     Sales|  4600| 25300|\n",
       "|   Maria|   Finance|  3000|  3000|\n",
       "|   Scott|   Finance|  3300|  6300|\n",
       "|     Jen|   Finance|  3900| 10200|\n",
       "|   Kumar| Marketing|  2000|  2000|\n",
       "|    Jeff| Marketing|  3000|  5000|\n",
       "+--------+----------+------+------+"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfSalary\\\n",
    "    .withColumn('cumsum', F.sum('salary').over(window))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Pivot table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Unpivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark; findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.enableHiveSupport().getOrCreate()\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>color</th><th>small</th><th>medium</th><th>large</th></tr>\n",
       "<tr><td>red</td><td>1000</td><td>1200</td><td>1500</td></tr>\n",
       "<tr><td>green</td><td>1500</td><td>1500</td><td>1575</td></tr>\n",
       "<tr><td>blue</td><td>2000</td><td>2200</td><td>2000</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----+-----+------+-----+\n",
       "|color|small|medium|large|\n",
       "+-----+-----+------+-----+\n",
       "|  red| 1000|  1200| 1500|\n",
       "|green| 1500|  1500| 1575|\n",
       "| blue| 2000|  2200| 2000|\n",
       "+-----+-----+------+-----+"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = (\n",
    "    ('red', 1000, 1200, 1500),\n",
    "    ('green', 1500, 1500, 1575),\n",
    "    ('blue', 2000, 2200, 2000)\n",
    ")\n",
    "\n",
    "schema = ['color', 'small', 'medium', 'large']\n",
    "\n",
    "dfProduct = spark.createDataFrame(data, schema)\n",
    "dfProduct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>color</th><th>size</th><th>price</th></tr>\n",
       "<tr><td>red</td><td>small</td><td>1000</td></tr>\n",
       "<tr><td>red</td><td>medium</td><td>1200</td></tr>\n",
       "<tr><td>red</td><td>large</td><td>1500</td></tr>\n",
       "<tr><td>green</td><td>small</td><td>1500</td></tr>\n",
       "<tr><td>green</td><td>medium</td><td>1500</td></tr>\n",
       "<tr><td>green</td><td>large</td><td>1575</td></tr>\n",
       "<tr><td>blue</td><td>small</td><td>2000</td></tr>\n",
       "<tr><td>blue</td><td>medium</td><td>2200</td></tr>\n",
       "<tr><td>blue</td><td>large</td><td>2000</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----+------+-----+\n",
       "|color|  size|price|\n",
       "+-----+------+-----+\n",
       "|  red| small| 1000|\n",
       "|  red|medium| 1200|\n",
       "|  red| large| 1500|\n",
       "|green| small| 1500|\n",
       "|green|medium| 1500|\n",
       "|green| large| 1575|\n",
       "| blue| small| 2000|\n",
       "| blue|medium| 2200|\n",
       "| blue| large| 2000|\n",
       "+-----+------+-----+"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfProduct.select('color', F.expr('stack(3, \"small\", small, \"medium\", medium, \"large\", large) as (size, price)'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Pivot table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark; findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.enableHiveSupport().getOrCreate()\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>invoice_id</th><th>brand</th><th>city</th><th>customer_type</th><th>gender</th><th>product_line</th><th>unit_price</th><th>quantity</th><th>tax</th><th>date</th><th>time</th><th>payment</th><th>cost</th><th>gross_margin_percentage</th><th>profit</th><th>rating</th></tr>\n",
       "<tr><td>750-67-8428</td><td>A</td><td>Yangon</td><td>Member</td><td>Female</td><td>Health and beauty</td><td>74.69</td><td>7</td><td>26.1415</td><td>01/05/2019</td><td>13:08</td><td>Ewallet</td><td>522.83</td><td>4.761904762</td><td>26.1415</td><td>9.1</td></tr>\n",
       "<tr><td>226-31-3081</td><td>C</td><td>Naypyitaw</td><td>Normal</td><td>Female</td><td>Electronic access...</td><td>15.28</td><td>5</td><td>3.82</td><td>03/08/2019</td><td>10:29</td><td>Cash</td><td>76.4</td><td>4.761904762</td><td>3.82</td><td>9.6</td></tr>\n",
       "<tr><td>631-41-3108</td><td>A</td><td>Yangon</td><td>Normal</td><td>Male</td><td>Home and lifestyle</td><td>46.33</td><td>7</td><td>16.2155</td><td>03/03/2019</td><td>13:23</td><td>Credit card</td><td>324.31</td><td>4.761904762</td><td>16.2155</td><td>7.4</td></tr>\n",
       "<tr><td>123-19-1176</td><td>A</td><td>Yangon</td><td>Member</td><td>Male</td><td>Health and beauty</td><td>58.22</td><td>8</td><td>23.288</td><td>1/27/2019</td><td>20:33</td><td>Ewallet</td><td>465.76</td><td>4.761904762</td><td>23.288</td><td>8.4</td></tr>\n",
       "<tr><td>373-73-7910</td><td>A</td><td>Yangon</td><td>Normal</td><td>Male</td><td>Sports and travel</td><td>86.31</td><td>7</td><td>30.2085</td><td>02/08/2019</td><td>10:37</td><td>Ewallet</td><td>604.17</td><td>4.761904762</td><td>30.2085</td><td>5.3</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------+-----+---------+-------------+------+--------------------+----------+--------+-------+----------+-----+-----------+------+-----------------------+-------+------+\n",
       "| invoice_id|brand|     city|customer_type|gender|        product_line|unit_price|quantity|    tax|      date| time|    payment|  cost|gross_margin_percentage| profit|rating|\n",
       "+-----------+-----+---------+-------------+------+--------------------+----------+--------+-------+----------+-----+-----------+------+-----------------------+-------+------+\n",
       "|750-67-8428|    A|   Yangon|       Member|Female|   Health and beauty|     74.69|       7|26.1415|01/05/2019|13:08|    Ewallet|522.83|            4.761904762|26.1415|   9.1|\n",
       "|226-31-3081|    C|Naypyitaw|       Normal|Female|Electronic access...|     15.28|       5|   3.82|03/08/2019|10:29|       Cash|  76.4|            4.761904762|   3.82|   9.6|\n",
       "|631-41-3108|    A|   Yangon|       Normal|  Male|  Home and lifestyle|     46.33|       7|16.2155|03/03/2019|13:23|Credit card|324.31|            4.761904762|16.2155|   7.4|\n",
       "|123-19-1176|    A|   Yangon|       Member|  Male|   Health and beauty|     58.22|       8| 23.288| 1/27/2019|20:33|    Ewallet|465.76|            4.761904762| 23.288|   8.4|\n",
       "|373-73-7910|    A|   Yangon|       Normal|  Male|   Sports and travel|     86.31|       7|30.2085|02/08/2019|10:37|    Ewallet|604.17|            4.761904762|30.2085|   5.3|\n",
       "+-----------+-----+---------+-------------+------+--------------------+----------+--------+-------+----------+-----+-----------+------+-----------------------+-------+------+"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfSupermarket = spark.read.csv('data/supermarket_sales.csv', header=True)\n",
    "dfSupermarket.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>product_line</th><th>Mandalay</th><th>Naypyitaw</th><th>Yangon</th></tr>\n",
       "<tr><td>Home and lifestyle</td><td>16.71</td><td>14.7</td><td>16.42</td></tr>\n",
       "<tr><td>Fashion accessories</td><td>12.61</td><td>15.79</td><td>15.25</td></tr>\n",
       "<tr><td>Health and beauty</td><td>17.95</td><td>15.22</td><td>12.76</td></tr>\n",
       "<tr><td>Electronic access...</td><td>14.76</td><td>16.42</td><td>14.54</td></tr>\n",
       "<tr><td>Food and beverages</td><td>14.49</td><td>17.15</td><td>14.09</td></tr>\n",
       "<tr><td>Sports and travel</td><td>15.35</td><td>16.68</td><td>15.64</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------+--------+---------+------+\n",
       "|        product_line|Mandalay|Naypyitaw|Yangon|\n",
       "+--------------------+--------+---------+------+\n",
       "|  Home and lifestyle|   16.71|     14.7| 16.42|\n",
       "| Fashion accessories|   12.61|    15.79| 15.25|\n",
       "|   Health and beauty|   17.95|    15.22| 12.76|\n",
       "|Electronic access...|   14.76|    16.42| 14.54|\n",
       "|  Food and beverages|   14.49|    17.15| 14.09|\n",
       "|   Sports and travel|   15.35|    16.68| 15.64|\n",
       "+--------------------+--------+---------+------+"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfSupermarket\\\n",
    "    .groupBy('product_line')\\\n",
    "    .pivot('city')\\\n",
    "    .agg(F.round(F.mean('profit'), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate pivoting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>product_line</th><th>Female, Member</th><th>Female, Normal</th><th>Male, Member</th><th>Male, Normal</th></tr>\n",
       "<tr><td>Home and lifestyle</td><td>17.46</td><td>19.05</td><td>14.21</td><td>13.84</td></tr>\n",
       "<tr><td>Fashion accessories</td><td>15.32</td><td>14.88</td><td>13.68</td><td>14.03</td></tr>\n",
       "<tr><td>Health and beauty</td><td>13.3</td><td>14.26</td><td>19.33</td><td>13.95</td></tr>\n",
       "<tr><td>Electronic access...</td><td>15.17</td><td>15.5</td><td>14.78</td><td>15.38</td></tr>\n",
       "<tr><td>Food and beverages</td><td>18.3</td><td>16.57</td><td>13.02</td><td>13.03</td></tr>\n",
       "<tr><td>Sports and travel</td><td>15.55</td><td>15.34</td><td>15.31</td><td>16.98</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------------------+--------------+--------------+------------+------------+\n",
       "|        product_line|Female, Member|Female, Normal|Male, Member|Male, Normal|\n",
       "+--------------------+--------------+--------------+------------+------------+\n",
       "|  Home and lifestyle|         17.46|         19.05|       14.21|       13.84|\n",
       "| Fashion accessories|         15.32|         14.88|       13.68|       14.03|\n",
       "|   Health and beauty|          13.3|         14.26|       19.33|       13.95|\n",
       "|Electronic access...|         15.17|          15.5|       14.78|       15.38|\n",
       "|  Food and beverages|          18.3|         16.57|       13.02|       13.03|\n",
       "|   Sports and travel|         15.55|         15.34|       15.31|       16.98|\n",
       "+--------------------+--------------+--------------+------------+------------+"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfSupermarket\\\n",
    "    .withColumn('info', F.concat(F.col('gender'), F.lit(', '), F.col('customer_type')))\\\n",
    "    .groupBy('product_line')\\\n",
    "    .pivot('info')\\\n",
    "    .agg(F.round(F.mean('profit'), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Crosstab\n",
    "Crosstab is a special case of pivot table, in which <code style='font-size:13px;'>count</code> is selected as the aggregate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark; findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.enableHiveSupport().getOrCreate()\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>invoice_id</th><th>brand</th><th>city</th><th>customer_type</th><th>gender</th><th>product_line</th><th>unit_price</th><th>quantity</th><th>tax</th><th>date</th><th>time</th><th>payment</th><th>cost</th><th>gross_margin_percentage</th><th>profit</th><th>rating</th></tr>\n",
       "<tr><td>750-67-8428</td><td>A</td><td>Yangon</td><td>Member</td><td>Female</td><td>Health and beauty</td><td>74.69</td><td>7</td><td>26.1415</td><td>01/05/2019</td><td>13:08</td><td>Ewallet</td><td>522.83</td><td>4.761904762</td><td>26.1415</td><td>9.1</td></tr>\n",
       "<tr><td>226-31-3081</td><td>C</td><td>Naypyitaw</td><td>Normal</td><td>Female</td><td>Electronic access...</td><td>15.28</td><td>5</td><td>3.82</td><td>03/08/2019</td><td>10:29</td><td>Cash</td><td>76.4</td><td>4.761904762</td><td>3.82</td><td>9.6</td></tr>\n",
       "<tr><td>631-41-3108</td><td>A</td><td>Yangon</td><td>Normal</td><td>Male</td><td>Home and lifestyle</td><td>46.33</td><td>7</td><td>16.2155</td><td>03/03/2019</td><td>13:23</td><td>Credit card</td><td>324.31</td><td>4.761904762</td><td>16.2155</td><td>7.4</td></tr>\n",
       "<tr><td>123-19-1176</td><td>A</td><td>Yangon</td><td>Member</td><td>Male</td><td>Health and beauty</td><td>58.22</td><td>8</td><td>23.288</td><td>1/27/2019</td><td>20:33</td><td>Ewallet</td><td>465.76</td><td>4.761904762</td><td>23.288</td><td>8.4</td></tr>\n",
       "<tr><td>373-73-7910</td><td>A</td><td>Yangon</td><td>Normal</td><td>Male</td><td>Sports and travel</td><td>86.31</td><td>7</td><td>30.2085</td><td>02/08/2019</td><td>10:37</td><td>Ewallet</td><td>604.17</td><td>4.761904762</td><td>30.2085</td><td>5.3</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----------+-----+---------+-------------+------+--------------------+----------+--------+-------+----------+-----+-----------+------+-----------------------+-------+------+\n",
       "| invoice_id|brand|     city|customer_type|gender|        product_line|unit_price|quantity|    tax|      date| time|    payment|  cost|gross_margin_percentage| profit|rating|\n",
       "+-----------+-----+---------+-------------+------+--------------------+----------+--------+-------+----------+-----+-----------+------+-----------------------+-------+------+\n",
       "|750-67-8428|    A|   Yangon|       Member|Female|   Health and beauty|     74.69|       7|26.1415|01/05/2019|13:08|    Ewallet|522.83|            4.761904762|26.1415|   9.1|\n",
       "|226-31-3081|    C|Naypyitaw|       Normal|Female|Electronic access...|     15.28|       5|   3.82|03/08/2019|10:29|       Cash|  76.4|            4.761904762|   3.82|   9.6|\n",
       "|631-41-3108|    A|   Yangon|       Normal|  Male|  Home and lifestyle|     46.33|       7|16.2155|03/03/2019|13:23|Credit card|324.31|            4.761904762|16.2155|   7.4|\n",
       "|123-19-1176|    A|   Yangon|       Member|  Male|   Health and beauty|     58.22|       8| 23.288| 1/27/2019|20:33|    Ewallet|465.76|            4.761904762| 23.288|   8.4|\n",
       "|373-73-7910|    A|   Yangon|       Normal|  Male|   Sports and travel|     86.31|       7|30.2085|02/08/2019|10:37|    Ewallet|604.17|            4.761904762|30.2085|   5.3|\n",
       "+-----------+-----+---------+-------------+------+--------------------+----------+--------+-------+----------+-----+-----------+------+-----------------------+-------+------+"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfSupermarket = spark.read.csv('data/supermarket_sales.csv', header=True)\n",
    "dfSupermarket.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>city_payment</th><th>Cash</th><th>Credit card</th><th>Ewallet</th></tr>\n",
       "<tr><td>Naypyitaw</td><td>124</td><td>98</td><td>106</td></tr>\n",
       "<tr><td>Mandalay</td><td>110</td><td>109</td><td>113</td></tr>\n",
       "<tr><td>Yangon</td><td>110</td><td>104</td><td>126</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------------+----+-----------+-------+\n",
       "|city_payment|Cash|Credit card|Ewallet|\n",
       "+------------+----+-----------+-------+\n",
       "|   Naypyitaw| 124|         98|    106|\n",
       "|    Mandalay| 110|        109|    113|\n",
       "|      Yangon| 110|        104|    126|\n",
       "+------------+----+-----------+-------+"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfSupermarket.crosstab('city', 'payment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Combining datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Union\n",
    "PySpark supports two union methods:\n",
    "- <code style='font-size:13px;'>DataFrame.union()</code>: union using the current order of columns\n",
    "- <code style='font-size:13px;'>DataFrame.unionByName()</code>: union using column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark; findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.enableHiveSupport().getOrCreate()\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>year</th><th>quarter</th><th>profit</th></tr>\n",
       "<tr><td>2019</td><td>1</td><td>2500</td></tr>\n",
       "<tr><td>2019</td><td>2</td><td>3500</td></tr>\n",
       "<tr><td>2019</td><td>3</td><td>4000</td></tr>\n",
       "<tr><td>2019</td><td>4</td><td>5000</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----+-------+------+\n",
       "|year|quarter|profit|\n",
       "+----+-------+------+\n",
       "|2019|      1|  2500|\n",
       "|2019|      2|  3500|\n",
       "|2019|      3|  4000|\n",
       "|2019|      4|  5000|\n",
       "+----+-------+------+"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = (\n",
    "    (2019, 1, 2500),\n",
    "    (2019, 2, 3500),\n",
    "    (2019, 3, 4000),\n",
    "    (2019, 4, 5000)\n",
    ")\n",
    "\n",
    "schema = ['year', 'quarter', 'profit']\n",
    "\n",
    "dfProfit2019 = spark.createDataFrame(data, schema)\n",
    "dfProfit2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>year</th><th>quarter</th><th>profit</th></tr>\n",
       "<tr><td>2020</td><td>1</td><td>2700</td></tr>\n",
       "<tr><td>2020</td><td>2</td><td>3900</td></tr>\n",
       "<tr><td>2020</td><td>3</td><td>5000</td></tr>\n",
       "<tr><td>2020</td><td>4</td><td>8000</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----+-------+------+\n",
       "|year|quarter|profit|\n",
       "+----+-------+------+\n",
       "|2020|      1|  2700|\n",
       "|2020|      2|  3900|\n",
       "|2020|      3|  5000|\n",
       "|2020|      4|  8000|\n",
       "+----+-------+------+"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = (\n",
    "    (2020, 1, 2700),\n",
    "    (2020, 2, 3900),\n",
    "    (2020, 3, 5000),\n",
    "    (2020, 4, 8000)\n",
    ")\n",
    "\n",
    "schema = ['year', 'quarter', 'profit']\n",
    "\n",
    "dfProfit2020 = spark.createDataFrame(data, schema)\n",
    "dfProfit2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>year</th><th>quarter</th><th>profit</th></tr>\n",
       "<tr><td>2019</td><td>1</td><td>2500</td></tr>\n",
       "<tr><td>2019</td><td>2</td><td>3500</td></tr>\n",
       "<tr><td>2019</td><td>3</td><td>4000</td></tr>\n",
       "<tr><td>2019</td><td>4</td><td>5000</td></tr>\n",
       "<tr><td>2020</td><td>1</td><td>2700</td></tr>\n",
       "<tr><td>2020</td><td>2</td><td>3900</td></tr>\n",
       "<tr><td>2020</td><td>3</td><td>5000</td></tr>\n",
       "<tr><td>2020</td><td>4</td><td>8000</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----+-------+------+\n",
       "|year|quarter|profit|\n",
       "+----+-------+------+\n",
       "|2019|      1|  2500|\n",
       "|2019|      2|  3500|\n",
       "|2019|      3|  4000|\n",
       "|2019|      4|  5000|\n",
       "|2020|      1|  2700|\n",
       "|2020|      2|  3900|\n",
       "|2020|      3|  5000|\n",
       "|2020|      4|  8000|\n",
       "+----+-------+------+"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfProfit2019.union(dfProfit2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>year</th><th>quarter</th><th>profit</th></tr>\n",
       "<tr><td>2019</td><td>1</td><td>2500</td></tr>\n",
       "<tr><td>2019</td><td>2</td><td>3500</td></tr>\n",
       "<tr><td>2019</td><td>3</td><td>4000</td></tr>\n",
       "<tr><td>2019</td><td>4</td><td>5000</td></tr>\n",
       "<tr><td>2020</td><td>1</td><td>2700</td></tr>\n",
       "<tr><td>2020</td><td>2</td><td>3900</td></tr>\n",
       "<tr><td>2020</td><td>3</td><td>5000</td></tr>\n",
       "<tr><td>2020</td><td>4</td><td>8000</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----+-------+------+\n",
       "|year|quarter|profit|\n",
       "+----+-------+------+\n",
       "|2019|      1|  2500|\n",
       "|2019|      2|  3500|\n",
       "|2019|      3|  4000|\n",
       "|2019|      4|  5000|\n",
       "|2020|      1|  2700|\n",
       "|2020|      2|  3900|\n",
       "|2020|      3|  5000|\n",
       "|2020|      4|  8000|\n",
       "+----+-------+------+"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfProfit2019.unionByName(dfProfit2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark; findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.enableHiveSupport().getOrCreate()\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional join\n",
    "Conditional join combines two dataframes but only keeps matching values in both tables. There are 4 types of conditional join: *inner*, *left*, *right* and *outer*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>name</th><th>income_before_tax</th><th>tax_band</th></tr>\n",
       "<tr><td>Hannah</td><td>1200</td><td>Allowance</td></tr>\n",
       "<tr><td>James</td><td>3000</td><td>Basic</td></tr>\n",
       "<tr><td>Gabriel</td><td>700</td><td>Allowance</td></tr>\n",
       "<tr><td>Smith</td><td>2000</td><td>Basic</td></tr>\n",
       "<tr><td>Alex</td><td>10000</td><td>Higher</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+-----------------+---------+\n",
       "|   name|income_before_tax| tax_band|\n",
       "+-------+-----------------+---------+\n",
       "| Hannah|             1200|Allowance|\n",
       "|  James|             3000|    Basic|\n",
       "|Gabriel|              700|Allowance|\n",
       "|  Smith|             2000|    Basic|\n",
       "|   Alex|            10000|   Higher|\n",
       "+-------+-----------------+---------+"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = (\n",
    "    ('Hannah', 1200, 'Allowance'),\n",
    "    ('James', 3000, 'Basic'),\n",
    "    ('Gabriel', 700, 'Allowance'),\n",
    "    ('Smith', 2000, 'Basic'),\n",
    "    ('Alex', 10000, 'Higher'),\n",
    ")\n",
    "\n",
    "schema = ['name', 'income_before_tax', 'tax_band']\n",
    "\n",
    "dfIncome = spark.createDataFrame(data, schema)\n",
    "dfIncome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>band</th><th>income_range</th><th>tax_rate</th></tr>\n",
       "<tr><td>Allowance</td><td>Up to 12,500</td><td>0.0</td></tr>\n",
       "<tr><td>Basic</td><td>12,501 to 50,000</td><td>0.2</td></tr>\n",
       "<tr><td>Higher</td><td>50,001 to 150,000</td><td>0.4</td></tr>\n",
       "<tr><td>Additional</td><td>Over 150,000</td><td>0.45</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------+-----------------+--------+\n",
       "|      band|     income_range|tax_rate|\n",
       "+----------+-----------------+--------+\n",
       "| Allowance|     Up to 12,500|     0.0|\n",
       "|     Basic| 12,501 to 50,000|     0.2|\n",
       "|    Higher|50,001 to 150,000|     0.4|\n",
       "|Additional|     Over 150,000|    0.45|\n",
       "+----------+-----------------+--------+"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = (\n",
    "    ('Allowance', 'Up to 12,500', 0.0),\n",
    "    ('Basic', '12,501 to 50,000', 0.2),\n",
    "    ('Higher', '50,001 to 150,000', 0.4),\n",
    "    ('Additional', 'Over 150,000', 0.45),\n",
    ")\n",
    "\n",
    "schema = ['band', 'income_range', 'tax_rate']\n",
    "\n",
    "dfTax = spark.createDataFrame(data, schema)\n",
    "dfTax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>name</th><th>income_before_tax</th><th>tax_band</th><th>band</th><th>income_range</th><th>tax_rate</th></tr>\n",
       "<tr><td>Hannah</td><td>1200</td><td>Allowance</td><td>Allowance</td><td>Up to 12,500</td><td>0.0</td></tr>\n",
       "<tr><td>Gabriel</td><td>700</td><td>Allowance</td><td>Allowance</td><td>Up to 12,500</td><td>0.0</td></tr>\n",
       "<tr><td>Alex</td><td>10000</td><td>Higher</td><td>Higher</td><td>50,001 to 150,000</td><td>0.4</td></tr>\n",
       "<tr><td>James</td><td>3000</td><td>Basic</td><td>Basic</td><td>12,501 to 50,000</td><td>0.2</td></tr>\n",
       "<tr><td>Smith</td><td>2000</td><td>Basic</td><td>Basic</td><td>12,501 to 50,000</td><td>0.2</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+-----------------+---------+---------+-----------------+--------+\n",
       "|   name|income_before_tax| tax_band|     band|     income_range|tax_rate|\n",
       "+-------+-----------------+---------+---------+-----------------+--------+\n",
       "| Hannah|             1200|Allowance|Allowance|     Up to 12,500|     0.0|\n",
       "|Gabriel|              700|Allowance|Allowance|     Up to 12,500|     0.0|\n",
       "|   Alex|            10000|   Higher|   Higher|50,001 to 150,000|     0.4|\n",
       "|  James|             3000|    Basic|    Basic| 12,501 to 50,000|     0.2|\n",
       "|  Smith|             2000|    Basic|    Basic| 12,501 to 50,000|     0.2|\n",
       "+-------+-----------------+---------+---------+-----------------+--------+"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfIncome.join(dfTax, dfIncome.tax_band==dfTax.band, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>band</th><th>name</th><th>income_before_tax</th><th>income_range</th><th>tax_rate</th></tr>\n",
       "<tr><td>Allowance</td><td>Hannah</td><td>1200</td><td>Up to 12,500</td><td>0.0</td></tr>\n",
       "<tr><td>Allowance</td><td>Gabriel</td><td>700</td><td>Up to 12,500</td><td>0.0</td></tr>\n",
       "<tr><td>Higher</td><td>Alex</td><td>10000</td><td>50,001 to 150,000</td><td>0.4</td></tr>\n",
       "<tr><td>Basic</td><td>James</td><td>3000</td><td>12,501 to 50,000</td><td>0.2</td></tr>\n",
       "<tr><td>Basic</td><td>Smith</td><td>2000</td><td>12,501 to 50,000</td><td>0.2</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------+-------+-----------------+-----------------+--------+\n",
       "|     band|   name|income_before_tax|     income_range|tax_rate|\n",
       "+---------+-------+-----------------+-----------------+--------+\n",
       "|Allowance| Hannah|             1200|     Up to 12,500|     0.0|\n",
       "|Allowance|Gabriel|              700|     Up to 12,500|     0.0|\n",
       "|   Higher|   Alex|            10000|50,001 to 150,000|     0.4|\n",
       "|    Basic|  James|             3000| 12,501 to 50,000|     0.2|\n",
       "|    Basic|  Smith|             2000| 12,501 to 50,000|     0.2|\n",
       "+---------+-------+-----------------+-----------------+--------+"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfIncome\\\n",
    "    .withColumnRenamed('tax_band', 'band')\\\n",
    "    .join(dfTax, on='band', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross join\n",
    "Cross join performs a Catersian product, returning all combinations of rows in the two dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>name</th><th>income_before_tax</th><th>tax_band</th><th>band</th><th>income_range</th><th>tax_rate</th></tr>\n",
       "<tr><td>Hannah</td><td>1200</td><td>Allowance</td><td>Allowance</td><td>Up to 12,500</td><td>0.0</td></tr>\n",
       "<tr><td>Hannah</td><td>1200</td><td>Allowance</td><td>Basic</td><td>12,501 to 50,000</td><td>0.2</td></tr>\n",
       "<tr><td>Hannah</td><td>1200</td><td>Allowance</td><td>Higher</td><td>50,001 to 150,000</td><td>0.4</td></tr>\n",
       "<tr><td>Hannah</td><td>1200</td><td>Allowance</td><td>Additional</td><td>Over 150,000</td><td>0.45</td></tr>\n",
       "<tr><td>James</td><td>3000</td><td>Basic</td><td>Allowance</td><td>Up to 12,500</td><td>0.0</td></tr>\n",
       "<tr><td>James</td><td>3000</td><td>Basic</td><td>Basic</td><td>12,501 to 50,000</td><td>0.2</td></tr>\n",
       "<tr><td>James</td><td>3000</td><td>Basic</td><td>Higher</td><td>50,001 to 150,000</td><td>0.4</td></tr>\n",
       "<tr><td>James</td><td>3000</td><td>Basic</td><td>Additional</td><td>Over 150,000</td><td>0.45</td></tr>\n",
       "<tr><td>Gabriel</td><td>700</td><td>Allowance</td><td>Allowance</td><td>Up to 12,500</td><td>0.0</td></tr>\n",
       "<tr><td>Gabriel</td><td>700</td><td>Allowance</td><td>Basic</td><td>12,501 to 50,000</td><td>0.2</td></tr>\n",
       "<tr><td>Gabriel</td><td>700</td><td>Allowance</td><td>Higher</td><td>50,001 to 150,000</td><td>0.4</td></tr>\n",
       "<tr><td>Gabriel</td><td>700</td><td>Allowance</td><td>Additional</td><td>Over 150,000</td><td>0.45</td></tr>\n",
       "<tr><td>Smith</td><td>2000</td><td>Basic</td><td>Allowance</td><td>Up to 12,500</td><td>0.0</td></tr>\n",
       "<tr><td>Smith</td><td>2000</td><td>Basic</td><td>Basic</td><td>12,501 to 50,000</td><td>0.2</td></tr>\n",
       "<tr><td>Smith</td><td>2000</td><td>Basic</td><td>Higher</td><td>50,001 to 150,000</td><td>0.4</td></tr>\n",
       "<tr><td>Smith</td><td>2000</td><td>Basic</td><td>Additional</td><td>Over 150,000</td><td>0.45</td></tr>\n",
       "<tr><td>Alex</td><td>10000</td><td>Higher</td><td>Allowance</td><td>Up to 12,500</td><td>0.0</td></tr>\n",
       "<tr><td>Alex</td><td>10000</td><td>Higher</td><td>Basic</td><td>12,501 to 50,000</td><td>0.2</td></tr>\n",
       "<tr><td>Alex</td><td>10000</td><td>Higher</td><td>Higher</td><td>50,001 to 150,000</td><td>0.4</td></tr>\n",
       "<tr><td>Alex</td><td>10000</td><td>Higher</td><td>Additional</td><td>Over 150,000</td><td>0.45</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+-----------------+---------+----------+-----------------+--------+\n",
       "|   name|income_before_tax| tax_band|      band|     income_range|tax_rate|\n",
       "+-------+-----------------+---------+----------+-----------------+--------+\n",
       "| Hannah|             1200|Allowance| Allowance|     Up to 12,500|     0.0|\n",
       "| Hannah|             1200|Allowance|     Basic| 12,501 to 50,000|     0.2|\n",
       "| Hannah|             1200|Allowance|    Higher|50,001 to 150,000|     0.4|\n",
       "| Hannah|             1200|Allowance|Additional|     Over 150,000|    0.45|\n",
       "|  James|             3000|    Basic| Allowance|     Up to 12,500|     0.0|\n",
       "|  James|             3000|    Basic|     Basic| 12,501 to 50,000|     0.2|\n",
       "|  James|             3000|    Basic|    Higher|50,001 to 150,000|     0.4|\n",
       "|  James|             3000|    Basic|Additional|     Over 150,000|    0.45|\n",
       "|Gabriel|              700|Allowance| Allowance|     Up to 12,500|     0.0|\n",
       "|Gabriel|              700|Allowance|     Basic| 12,501 to 50,000|     0.2|\n",
       "|Gabriel|              700|Allowance|    Higher|50,001 to 150,000|     0.4|\n",
       "|Gabriel|              700|Allowance|Additional|     Over 150,000|    0.45|\n",
       "|  Smith|             2000|    Basic| Allowance|     Up to 12,500|     0.0|\n",
       "|  Smith|             2000|    Basic|     Basic| 12,501 to 50,000|     0.2|\n",
       "|  Smith|             2000|    Basic|    Higher|50,001 to 150,000|     0.4|\n",
       "|  Smith|             2000|    Basic|Additional|     Over 150,000|    0.45|\n",
       "|   Alex|            10000|   Higher| Allowance|     Up to 12,500|     0.0|\n",
       "|   Alex|            10000|   Higher|     Basic| 12,501 to 50,000|     0.2|\n",
       "|   Alex|            10000|   Higher|    Higher|50,001 to 150,000|     0.4|\n",
       "|   Alex|            10000|   Higher|Additional|     Over 150,000|    0.45|\n",
       "+-------+-----------------+---------+----------+-----------------+--------+"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfIncome.crossJoin(dfTax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Working with data structures\n",
    "Arrays is a powerful data type provided in almost all data manipulation packages. It can save a lot of memory as it gathers multiple items into a single rows instead of having too many rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Creating array-type columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import findspark; findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.enableHiveSupport().getOrCreate()\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "spark.conf.set('spark.sql.repl.eagerEval.truncate', 1000)\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construcing arrays\n",
    "The `T.struct` function allows contructing complicated data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>customer</th><th>interest</th></tr>\n",
       "<tr><td>Hannah</td><td>[clothes, books, shoes, travelling]</td></tr>\n",
       "<tr><td>James</td><td>[sports, science, comics]</td></tr>\n",
       "<tr><td>Wayne</td><td>[travelling, music, shoes, technology]</td></tr>\n",
       "<tr><td>Jolie</td><td>[cooking, boardgames, coffee]</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+--------------------------------------+\n",
       "|customer|                              interest|\n",
       "+--------+--------------------------------------+\n",
       "|  Hannah|   [clothes, books, shoes, travelling]|\n",
       "|   James|             [sports, science, comics]|\n",
       "|   Wayne|[travelling, music, shoes, technology]|\n",
       "|   Jolie|         [cooking, boardgames, coffee]|\n",
       "+--------+--------------------------------------+"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    ('Hannah', ['clothes', 'books', 'shoes', 'travelling']),\n",
    "    ('James', ['sports', 'science', 'comics']),\n",
    "    ('Wayne', ['travelling', 'music', 'shoes', 'technology']),\n",
    "    ('Jolie', ['cooking', 'boardgames', 'coffee'])\n",
    "]\n",
    "\n",
    "schema = T.StructType([\n",
    "    T.StructField('customer', T.StringType()),\n",
    "    T.StructField('interest', T.ArrayType(T.StringType()))\n",
    "])\n",
    "\n",
    "dfTmp = spark.createDataFrame(data, schema)\n",
    "dfTmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer: string (nullable = true)\n",
      " |-- interest: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfTmp.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>customer</th><th>interest</th></tr>\n",
       "<tr><td>Hannah</td><td>[clothes, books, shoes]</td></tr>\n",
       "<tr><td>James</td><td>[science, sports, comics]</td></tr>\n",
       "<tr><td>Wayne</td><td>[music, shoes, technology]</td></tr>\n",
       "<tr><td>Jolie</td><td>[cooking, boardgames, coffee]</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+-----------------------------+\n",
       "|customer|                     interest|\n",
       "+--------+-----------------------------+\n",
       "|  Hannah|      [clothes, books, shoes]|\n",
       "|   James|    [science, sports, comics]|\n",
       "|   Wayne|   [music, shoes, technology]|\n",
       "|   Jolie|[cooking, boardgames, coffee]|\n",
       "+--------+-----------------------------+"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    ('Hannah', 'clothes', 'books', 'shoes'),\n",
    "    ('James', 'science', 'sports', 'comics'),\n",
    "    ('Wayne', 'music', 'shoes', 'technology'),\n",
    "    ('Jolie', 'cooking', 'boardgames', 'coffee')\n",
    "]\n",
    "\n",
    "schema = 'customer string, interest_1 string, interest_2 string, interest_3 string'\n",
    "\n",
    "dfTmp = spark.createDataFrame(data, schema)\n",
    "dfTmp.selectExpr('customer', 'ARRAY(interest_1, interest_2, interest_3) AS interest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>customer</th><th>interest</th><th>interest_split</th></tr>\n",
       "<tr><td>Hannah</td><td>clothes; books; shoes; travelling</td><td>[clothes, books, shoes, travelling]</td></tr>\n",
       "<tr><td>James</td><td>sports; science; comics</td><td>[sports, science, comics]</td></tr>\n",
       "<tr><td>Wayne</td><td>travelling; music; shoes; technology</td><td>[travelling, music, shoes, technology]</td></tr>\n",
       "<tr><td>Jolie</td><td>cooking; boardgames; coffee</td><td>[cooking, boardgames, coffee]</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+------------------------------------+--------------------------------------+\n",
       "|customer|                            interest|                        interest_split|\n",
       "+--------+------------------------------------+--------------------------------------+\n",
       "|  Hannah|   clothes; books; shoes; travelling|   [clothes, books, shoes, travelling]|\n",
       "|   James|             sports; science; comics|             [sports, science, comics]|\n",
       "|   Wayne|travelling; music; shoes; technology|[travelling, music, shoes, technology]|\n",
       "|   Jolie|         cooking; boardgames; coffee|         [cooking, boardgames, coffee]|\n",
       "+--------+------------------------------------+--------------------------------------+"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    ('Hannah', 'clothes; books; shoes; travelling'),\n",
    "    ('James', 'sports; science; comics'),\n",
    "    ('Wayne', 'travelling; music; shoes; technology'),\n",
    "    ('Jolie', 'cooking; boardgames; coffee')\n",
    "]\n",
    "\n",
    "schema = 'customer string, interest string'\n",
    "\n",
    "dfTmp = spark.createDataFrame(data, schema)\n",
    "dfTmp.withColumn('interest_split', F.split('interest', '; '))\n",
    "# dfTmp.withColumn('interest_split', F.expr('SPLIT(interest, \";\")'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering rows\n",
    "The `collect_list` functions gathers values in a column to form a vectors. However, this function sometime does not preserve the order of rows. The solution for this problem will be discussed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>customer</th><th>interest</th></tr>\n",
       "<tr><td>James</td><td>[sports, science, comics]</td></tr>\n",
       "<tr><td>Jolie</td><td>[cooking, boardgames, coffee]</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+-----------------------------+\n",
       "|customer|                     interest|\n",
       "+--------+-----------------------------+\n",
       "|   James|    [sports, science, comics]|\n",
       "|   Jolie|[cooking, boardgames, coffee]|\n",
       "+--------+-----------------------------+"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    ('James', 'sports'),\n",
    "    ('James', 'science'),\n",
    "    ('James', 'comics'),\n",
    "    ('Jolie', 'cooking'),\n",
    "    ('Jolie', 'boardgames'),\n",
    "    ('Jolie', 'coffee'),\n",
    "]\n",
    "\n",
    "schema = 'customer string, interest string'\n",
    "\n",
    "dfTmp = spark.createDataFrame(data, schema)\n",
    "dfTmp.groupby('customer').agg(F.collect_list('interest').alias('interest'))\n",
    "dfTmp.groupby('customer').agg(F.expr('COLLECT_LIST(interest) AS interest'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Basic array manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import findspark; findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.enableHiveSupport().getOrCreate()\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "spark.conf.set('spark.sql.repl.eagerEval.truncate', 1000)\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>customer</th><th>interest</th></tr>\n",
       "<tr><td>James</td><td>[sports, comics, null, travelling]</td></tr>\n",
       "<tr><td>Jolie</td><td>[boardgames, shopping, boardgames, null]</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+----------------------------------------+\n",
       "|customer|                                interest|\n",
       "+--------+----------------------------------------+\n",
       "|   James|      [sports, comics, null, travelling]|\n",
       "|   Jolie|[boardgames, shopping, boardgames, null]|\n",
       "+--------+----------------------------------------+"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    ('James', ['sports', 'comics', None, 'travelling']),\n",
    "    ('Jolie', ['boardgames', 'shopping', 'boardgames', None])\n",
    "]\n",
    "\n",
    "schema = T.StructType([\n",
    "    T.StructField('customer', T.StringType()),\n",
    "    T.StructField('interest', T.ArrayType(T.StringType()))\n",
    "])\n",
    "\n",
    "dfInterest = spark.createDataFrame(data, schema)\n",
    "dfInterest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploding\n",
    "Exploding is the technique that transform each array into a number of rows equal to the number of array elements. It can be thought as the inverse process of gathering rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>customer</th><th>interest</th></tr>\n",
       "<tr><td>James</td><td>sports</td></tr>\n",
       "<tr><td>James</td><td>comics</td></tr>\n",
       "<tr><td>James</td><td>null</td></tr>\n",
       "<tr><td>James</td><td>travelling</td></tr>\n",
       "<tr><td>Jolie</td><td>boardgames</td></tr>\n",
       "<tr><td>Jolie</td><td>shopping</td></tr>\n",
       "<tr><td>Jolie</td><td>boardgames</td></tr>\n",
       "<tr><td>Jolie</td><td>null</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+----------+\n",
       "|customer|  interest|\n",
       "+--------+----------+\n",
       "|   James|    sports|\n",
       "|   James|    comics|\n",
       "|   James|      null|\n",
       "|   James|travelling|\n",
       "|   Jolie|boardgames|\n",
       "|   Jolie|  shopping|\n",
       "|   Jolie|boardgames|\n",
       "|   Jolie|      null|\n",
       "+--------+----------+"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using SQL expressions (recommended)\n",
    "dfInterest.selectExpr('customer', 'EXPLODE(interest) AS interest')\n",
    "\n",
    "# using PySpark functions\n",
    "# dfInterest.select('customer', F.explode('interest').alias('interest'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>customer</th><th>rank</th><th>interest</th></tr>\n",
       "<tr><td>James</td><td>0</td><td>sports</td></tr>\n",
       "<tr><td>James</td><td>1</td><td>comics</td></tr>\n",
       "<tr><td>James</td><td>2</td><td>null</td></tr>\n",
       "<tr><td>James</td><td>3</td><td>travelling</td></tr>\n",
       "<tr><td>Jolie</td><td>0</td><td>boardgames</td></tr>\n",
       "<tr><td>Jolie</td><td>1</td><td>shopping</td></tr>\n",
       "<tr><td>Jolie</td><td>2</td><td>boardgames</td></tr>\n",
       "<tr><td>Jolie</td><td>3</td><td>null</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+----+----------+\n",
       "|customer|rank|  interest|\n",
       "+--------+----+----------+\n",
       "|   James|   0|    sports|\n",
       "|   James|   1|    comics|\n",
       "|   James|   2|      null|\n",
       "|   James|   3|travelling|\n",
       "|   Jolie|   0|boardgames|\n",
       "|   Jolie|   1|  shopping|\n",
       "|   Jolie|   2|boardgames|\n",
       "|   Jolie|   3|      null|\n",
       "+--------+----+----------+"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using SQL expressions (recommended)\n",
    "dfInterest.selectExpr('customer', 'POSEXPLODE(interest) AS (rank, interest)')\n",
    "\n",
    "# using PySpark functions\n",
    "# dfInterest.select('customer', F.posexplode('interest')).select('customer', F.col('pos').alias('rank'), F.col('col').alias('interest'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array join\n",
    "Array join is the inverse process of splitting a string column into an array column. It works exactly the same as the Python `str.join` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>customer</th><th>interest</th></tr>\n",
       "<tr><td>James</td><td>sports; comics; travelling</td></tr>\n",
       "<tr><td>Jolie</td><td>boardgames; shopping; boardgames</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+--------------------------------+\n",
       "|customer|                        interest|\n",
       "+--------+--------------------------------+\n",
       "|   James|      sports; comics; travelling|\n",
       "|   Jolie|boardgames; shopping; boardgames|\n",
       "+--------+--------------------------------+"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfInterest.selectExpr('customer', 'ARRAY_JOIN(interest, \"; \") AS interest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>customer</th><th>interest</th></tr>\n",
       "<tr><td>James</td><td>sports; comics; nan; travelling</td></tr>\n",
       "<tr><td>Jolie</td><td>boardgames; shopping; boardgames; nan</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+-------------------------------------+\n",
       "|customer|                             interest|\n",
       "+--------+-------------------------------------+\n",
       "|   James|      sports; comics; nan; travelling|\n",
       "|   Jolie|boardgames; shopping; boardgames; nan|\n",
       "+--------+-------------------------------------+"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfInterest.selectExpr('customer', 'ARRAY_JOIN(interest, \"; \", \"nan\") AS interest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrays concatenating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>customer</th><th>individual_interest</th><th>common_interest</th><th>concatenation</th><th>intersect</th><th>union</th></tr>\n",
       "<tr><td>James</td><td>[sports, comics, travelling]</td><td>[shopping, travelling]</td><td>[sports, comics, travelling, shopping, travelling]</td><td>[travelling]</td><td>[sports, comics, travelling, shopping]</td></tr>\n",
       "<tr><td>Jolie</td><td>[boardgames, shopping, boardgames]</td><td>[shopping, travelling]</td><td>[boardgames, shopping, boardgames, shopping, travelling]</td><td>[shopping]</td><td>[boardgames, shopping, travelling]</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+----------------------------------+----------------------+--------------------------------------------------------+------------+--------------------------------------+\n",
       "|customer|               individual_interest|       common_interest|                                           concatenation|   intersect|                                 union|\n",
       "+--------+----------------------------------+----------------------+--------------------------------------------------------+------------+--------------------------------------+\n",
       "|   James|      [sports, comics, travelling]|[shopping, travelling]|      [sports, comics, travelling, shopping, travelling]|[travelling]|[sports, comics, travelling, shopping]|\n",
       "|   Jolie|[boardgames, shopping, boardgames]|[shopping, travelling]|[boardgames, shopping, boardgames, shopping, travelling]|  [shopping]|    [boardgames, shopping, travelling]|\n",
       "+--------+----------------------------------+----------------------+--------------------------------------------------------+------------+--------------------------------------+"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfInterest\\\n",
    "    .withColumn('common_interest', F.expr('ARRAY(\"shopping\", \"travelling\")'))\\\n",
    "    .withColumn('individual_interest', F.expr('FILTER(interest, x -> x IS NOT NULL)'))\\\n",
    "    .selectExpr(\n",
    "        'customer',\n",
    "        'individual_interest',\n",
    "        'common_interest',\n",
    "        'CONCAT(individual_interest, common_interest) AS concatenation',\n",
    "        'ARRAY_INTERSECT(individual_interest, common_interest) AS intersect',\n",
    "        'ARRAY_UNION(individual_interest, common_interest) AS union',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscellaneous techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>original_array</th><th>shuffled_array</th><th>sorted_array</th><th>array_max</th><th>array_min</th></tr>\n",
       "<tr><td>[sports, comics, null, travelling]</td><td>[null, travelling, sports, comics]</td><td>[comics, sports, travelling, null]</td><td>travelling</td><td>comics</td></tr>\n",
       "<tr><td>[boardgames, shopping, boardgames, null]</td><td>[shopping, boardgames, boardgames, null]</td><td>[boardgames, boardgames, shopping, null]</td><td>shopping</td><td>boardgames</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------------------------------------+----------------------------------------+----------------------------------------+----------+----------+\n",
       "|                          original_array|                          shuffled_array|                            sorted_array| array_max| array_min|\n",
       "+----------------------------------------+----------------------------------------+----------------------------------------+----------+----------+\n",
       "|      [sports, comics, null, travelling]|      [null, travelling, sports, comics]|      [comics, sports, travelling, null]|travelling|    comics|\n",
       "|[boardgames, shopping, boardgames, null]|[shopping, boardgames, boardgames, null]|[boardgames, boardgames, shopping, null]|  shopping|boardgames|\n",
       "+----------------------------------------+----------------------------------------+----------------------------------------+----------+----------+"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfInterest.selectExpr(\n",
    "    'interest AS original_array',\n",
    "    'SHUFFLE(interest) AS shuffled_array',\n",
    "    'ARRAY_SORT(interest) AS sorted_array',\n",
    "    'ARRAY_MAX(interest) AS array_max',\n",
    "    'ARRAY_MIN(interest) AS array_min',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>original_array</th><th>sliced_array</th><th>notnull_array</th><th>distinct_array</th></tr>\n",
       "<tr><td>[sports, comics, null, travelling]</td><td>[sports, comics]</td><td>[sports, comics, travelling]</td><td>[sports, comics, null, travelling]</td></tr>\n",
       "<tr><td>[boardgames, shopping, boardgames, null]</td><td>[boardgames, shopping]</td><td>[boardgames, shopping, boardgames]</td><td>[boardgames, shopping, null]</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------------------------------------+----------------------+----------------------------------+----------------------------------+\n",
       "|                          original_array|          sliced_array|                     notnull_array|                    distinct_array|\n",
       "+----------------------------------------+----------------------+----------------------------------+----------------------------------+\n",
       "|      [sports, comics, null, travelling]|      [sports, comics]|      [sports, comics, travelling]|[sports, comics, null, travelling]|\n",
       "|[boardgames, shopping, boardgames, null]|[boardgames, shopping]|[boardgames, shopping, boardgames]|      [boardgames, shopping, null]|\n",
       "+----------------------------------------+----------------------+----------------------------------+----------------------------------+"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfInterest.selectExpr(\n",
    "    'interest AS original_array',\n",
    "    'SLICE(interest, 1, 2) AS sliced_array',\n",
    "    'FILTER(interest, x -> x IS NOT NULL) AS notnull_array',\n",
    "    'ARRAY_DISTINCT(interest) AS distinct_array'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Structure type manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import findspark; findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.enableHiveSupport().getOrCreate()\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "spark.conf.set('spark.sql.repl.eagerEval.truncate', 1000)\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing nested arrays\n",
    "A <code style='font-size:13px;'>struct</code> type column can be thought as a group of smaller columns. It should not be confused with <code style='font-size:13px;'>array</code> data type, whose array size may vary and elements cannot be accessed by name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>name</th><th>gender</th><th>salary</th></tr>\n",
       "<tr><td>{James, null, Smith}</td><td>M</td><td>3100</td></tr>\n",
       "<tr><td>{Michael, Rose, null}</td><td>M</td><td>4300</td></tr>\n",
       "<tr><td>{Robert, null, Williams}</td><td>M</td><td>1400</td></tr>\n",
       "<tr><td>{Maria, Anne, Jones}</td><td>F</td><td>5500</td></tr>\n",
       "<tr><td>{Jen, Mary, Brown}</td><td>F</td><td>2500</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+------------------------+------+------+\n",
       "|                    name|gender|salary|\n",
       "+------------------------+------+------+\n",
       "|    {James, null, Smith}|     M|  3100|\n",
       "|   {Michael, Rose, null}|     M|  4300|\n",
       "|{Robert, null, Williams}|     M|  1400|\n",
       "|    {Maria, Anne, Jones}|     F|  5500|\n",
       "|      {Jen, Mary, Brown}|     F|  2500|\n",
       "+------------------------+------+------+"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    (('James', None, 'Smith'), 'M', 3100),\n",
    "    (('Michael', 'Rose', None), 'M', 4300),\n",
    "    (('Robert', None, 'Williams'), 'M', 1400),\n",
    "    (('Maria', 'Anne', 'Jones'), 'F', 5500),\n",
    "    (('Jen', 'Mary', 'Brown'), 'F', 2500)\n",
    "]\n",
    "\n",
    "schema = T.StructType([\n",
    "    T.StructField('name', T.StructType([\n",
    "        T.StructField('firstname', T.StringType(), True),\n",
    "        T.StructField('middlename', T.StringType(), True),\n",
    "        T.StructField('lastname', T.StringType(), True)\n",
    "    ])),\n",
    "    T.StructField('gender', T.StringType(), True),\n",
    "    T.StructField('salary', T.IntegerType(), True)\n",
    "])\n",
    "\n",
    "dfCustomer = spark.createDataFrame(data, schema)\n",
    "dfCustomer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfCustomer.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>lastname</th><th>middlename</th><th>lastname</th></tr>\n",
       "<tr><td>Smith</td><td>null</td><td>Smith</td></tr>\n",
       "<tr><td>null</td><td>Rose</td><td>null</td></tr>\n",
       "<tr><td>Williams</td><td>null</td><td>Williams</td></tr>\n",
       "<tr><td>Jones</td><td>Anne</td><td>Jones</td></tr>\n",
       "<tr><td>Brown</td><td>Mary</td><td>Brown</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+----------+--------+\n",
       "|lastname|middlename|lastname|\n",
       "+--------+----------+--------+\n",
       "|   Smith|      null|   Smith|\n",
       "|    null|      Rose|    null|\n",
       "|Williams|      null|Williams|\n",
       "|   Jones|      Anne|   Jones|\n",
       "|   Brown|      Mary|   Brown|\n",
       "+--------+----------+--------+"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# child columns can be accessed easily\n",
    "dfCustomer.select('name.lastname', 'name.middlename', 'name.lastname')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>name</th><th>skills</th><th>score</th></tr>\n",
       "<tr><td>James</td><td>Python</td><td>9</td></tr>\n",
       "<tr><td>James</td><td>Tableau</td><td>7</td></tr>\n",
       "<tr><td>James</td><td>SQL</td><td>8</td></tr>\n",
       "<tr><td>Jolie</td><td>R</td><td>6</td></tr>\n",
       "<tr><td>Jolie</td><td>Python</td><td>9</td></tr>\n",
       "<tr><td>Jolie</td><td>Power BI</td><td>5</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----+--------+-----+\n",
       "| name|  skills|score|\n",
       "+-----+--------+-----+\n",
       "|James|  Python|    9|\n",
       "|James| Tableau|    7|\n",
       "|James|     SQL|    8|\n",
       "|Jolie|       R|    6|\n",
       "|Jolie|  Python|    9|\n",
       "|Jolie|Power BI|    5|\n",
       "+-----+--------+-----+"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    ('James', 'Python', 9),\n",
    "    ('James', 'Tableau', 7),\n",
    "    ('James', 'SQL', 8),\n",
    "    ('Jolie', 'R', 6),\n",
    "    ('Jolie', 'Python', 9),\n",
    "    ('Jolie', 'Power BI', 5)\n",
    "]\n",
    "\n",
    "schema = 'name string, skills string, score int'\n",
    "\n",
    "dfScore = spark.createDataFrame(data, schema)\n",
    "dfScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>name</th><th>strength</th></tr>\n",
       "<tr><td>James</td><td>{Python, 9}</td></tr>\n",
       "<tr><td>James</td><td>{Tableau, 7}</td></tr>\n",
       "<tr><td>James</td><td>{SQL, 8}</td></tr>\n",
       "<tr><td>Jolie</td><td>{R, 6}</td></tr>\n",
       "<tr><td>Jolie</td><td>{Python, 9}</td></tr>\n",
       "<tr><td>Jolie</td><td>{Power BI, 5}</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----+-------------+\n",
       "| name|     strength|\n",
       "+-----+-------------+\n",
       "|James|  {Python, 9}|\n",
       "|James| {Tableau, 7}|\n",
       "|James|     {SQL, 8}|\n",
       "|Jolie|       {R, 6}|\n",
       "|Jolie|  {Python, 9}|\n",
       "|Jolie|{Power BI, 5}|\n",
       "+-----+-------------+"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfScore.selectExpr('name', 'STRUCT(skills, score) AS strength')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>name</th><th>strength</th></tr>\n",
       "<tr><td>James</td><td>[Tableau, SQL, Python]</td></tr>\n",
       "<tr><td>Jolie</td><td>[Power BI, R, Python]</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----+----------------------+\n",
       "| name|              strength|\n",
       "+-----+----------------------+\n",
       "|James|[Tableau, SQL, Python]|\n",
       "|Jolie| [Power BI, R, Python]|\n",
       "+-----+----------------------+"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfScore\\\n",
    "    .groupby('name')\\\n",
    "    .agg(F.expr('COLLECT_LIST(STRUCT(score, skills)) AS strength'))\\\n",
    "    .selectExpr('name', 'ARRAY_SORT(strength).skills AS strength')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrays zipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>name</th><th>skills</th><th>score</th><th>strength</th></tr>\n",
       "<tr><td>James</td><td>[Python, Tableau, SQL]</td><td>[9, 7, 8]</td><td>[{Python, 9}, {Tableau, 7}, {SQL, 8}]</td></tr>\n",
       "<tr><td>Jolie</td><td>[R, Python, Power BI]</td><td>[6, 9, 5]</td><td>[{R, 6}, {Python, 9}, {Power BI, 5}]</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-----+----------------------+---------+-------------------------------------+\n",
       "| name|                skills|    score|                             strength|\n",
       "+-----+----------------------+---------+-------------------------------------+\n",
       "|James|[Python, Tableau, SQL]|[9, 7, 8]|[{Python, 9}, {Tableau, 7}, {SQL, 8}]|\n",
       "|Jolie| [R, Python, Power BI]|[6, 9, 5]| [{R, 6}, {Python, 9}, {Power BI, 5}]|\n",
       "+-----+----------------------+---------+-------------------------------------+"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    ('James', ['Python', 'Tableau', 'SQL'], [9, 7, 8]),\n",
    "    ('Jolie', ['R', 'Python', 'Power BI'], [6, 9, 5])\n",
    "]\n",
    "\n",
    "schema = T.StructType([\n",
    "    T.StructField('name', T.StringType()),\n",
    "    T.StructField('skills', T.ArrayType(T.StringType())),\n",
    "    T.StructField('score', T.ArrayType(T.IntegerType()))\n",
    "])\n",
    "\n",
    "dfTmp = spark.createDataFrame(data, schema)\n",
    "dfTmp.selectExpr('name', 'skills', 'score', 'ARRAYS_ZIP(skills, score) AS strength')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Dictionary type manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "*&#9829; By Quang Hung x Thuy Linh &#9829;*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

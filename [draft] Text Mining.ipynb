{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Syntactic processing\n",
    "\n",
    "Syntactic analysis or parsing is defined as the process of analyzing the strings of symbols in natural language conforming to the rules of formal grammar. The purpose of this process is to draw exact meaning, or perform dictionary meaning from the text. Syntax analysis checks the text for meaningfulness comparing to the rules of formal grammar. Example:\n",
    "1. Delhi is the capital of India.\n",
    "2. Is Delhi the of India capital.\n",
    "\n",
    "Two sentences have the same word but only sentence 1 was meaningful and syntactically correct. The purpose of sytactic processing is recover the right one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Part of speech (POS) tagging \n",
    "\n",
    "Part of speech is the process  which refers to categorizing words in a text (corpus) in correspondence with a particular part of speech, depending on the definition of the word and its context. To analyze the relationship and understanding meaning of text, pos tagging is very important process. POS tag are useful for building parse trees, which are used in building NERs and extracting relations between words. It is also use for building lemmatizers in 1.3 \n",
    "\n",
    "Some pos tagging techniques:\n",
    "- Lexical base method: Assigns the POS tag the most frequently occurring with a word in the training corpus\n",
    "- Rule based method: Assigns POS tags based on rules in dictionary.\n",
    "- Probabilistic method: This method assigns the POS tags based on the probability of a particular tag sequence occurring. Conditional Random Fields (CRFs) and Hidden Markov Models (HMMs) are probabilistic approaches to assign a POS Tag.\n",
    "- Deep learning method: Recurrent Neural Networks can also be used for POS tagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Random Fields (CRFs)\n",
    "\n",
    "Conditional Random Fields are a discriminative model, used for predicting sequences. They use contextual information from previous labels, thus increasing the amount of information the model has to make a good prediction.\n",
    "Discriminative classifier - they model the decision boundary between the different classes (just like logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Markov Models (HMMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Parsing\n",
    "One of the most important parts of syntactic processing is parsing. It means to break down a given sentence into its *grammatical components*. NLTK doesn't support pre-trained English grammar model, we have to manually specify grammar before parsing a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. NLP for Vietnamese language \n",
    "\n",
    "With the Vietnamese language, the packages haven't developed completely and they don't have specific documentation. Some common packages are pyvi, vncorenlp and underthesea. These packages provide some basic function such as word tokenize, pos tagging and removing accent. There is no quite difference between pyvi and underthesea despite of some pre-trained models that underthesea provides like NER, classify and sentiment analysis\n",
    "\n",
    "*Reference:* \n",
    "[Pyvi](https://pypi.org/project/pyvi/) and \n",
    "[Underthesea](https://underthesea.readthedocs.io/en/latest/readme.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvi import ViTokenizer, ViPosTagger, ViUtils,ViDiac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Há»“', 'gÆ°Æ¡m', 'lÃ ', 'danh_lam', 'tháº¯ng_cáº£nh', 'HÃ _Ná»™i']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ViTokenizer.tokenize(\"Há»“ gÆ°Æ¡m lÃ  danh lam tháº¯ng cáº£nh HÃ  Ná»™i\").split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TÃ´i', 'lÃ ', 'sinh_viÃªn', 'trÆ°á»ng', 'cao_Ä‘áº³ng', 'y_táº¿', 'hÃ _tÃ¢y']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_token, _ = ViTokenizer.spacy_tokenize(\"TÃ´i lÃ  sinh viÃªn trÆ°á»ng cao Ä‘áº³ng y táº¿ hÃ  tÃ¢y\")\n",
    "list_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Há»“', 'gÆ°Æ¡m', 'lÃ ', 'danh', 'lam', 'tháº¯ng', 'cáº£nh', 'HÃ ', 'Ná»™i'],\n",
       " ['N', 'N', 'V', 'N', 'N', 'V', 'N', 'Np', 'Np'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ViPosTagger.postagging(\"Há»“ gÆ°Æ¡m lÃ  danh lam tháº¯ng cáº£nh HÃ  Ná»™i\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Ho guom la danh lam thang canh Ha Noi'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ViUtils.remove_accents(u\"Há»“ gÆ°Æ¡m lÃ  danh lam tháº¯ng cáº£nh HÃ  Ná»™i\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Another way to remove accent is using unidecode packages*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ho guom la danh lam thang canh Ha Noi'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unidecode\n",
    "\n",
    "unidecode.unidecode('Há»“ gÆ°Æ¡m lÃ  danh lam tháº¯ng cáº£nh HÃ  Ná»™i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import underthesea\n",
    "from underthesea import sent_tokenize, word_tokenize, pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Vá»›i xá»­ lÃ­ tiáº¿ng viá»‡t, cÃ¡c thÆ° viá»‡n chÆ°a phÃ¡t triá»ƒn nhiá»u.',\n",
       " 'Má»™t sá»‘ thÆ° viá»‡n phá»• biáº¿n lÃ  pyvi vÃ  underthesea']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Vá»›i xá»­ lÃ­ tiáº¿ng viá»‡t, cÃ¡c thÆ° viá»‡n chÆ°a phÃ¡t triá»ƒn nhiá»u. Má»™t sá»‘ thÆ° viá»‡n phá»• biáº¿n lÃ  pyvi vÃ  underthesea\"\"\"\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Vá»›i', 'xá»­ lÃ­', 'tiáº¿ng', 'viá»‡t', ',', 'cÃ¡c', 'thÆ° viá»‡n', 'chÆ°a', 'phÃ¡t triá»ƒn', 'nhiá»u', '.', 'Má»™t sá»‘', 'thÆ° viá»‡n', 'phá»• biáº¿n', 'lÃ ', 'pyvi', 'vÃ ', 'underthesea']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Vá»›i', 'E'),\n",
       " ('xá»­ lÃ­', 'N'),\n",
       " ('tiáº¿ng', 'N'),\n",
       " ('viá»‡t', 'V'),\n",
       " (',', 'CH'),\n",
       " ('cÃ¡c', 'L'),\n",
       " ('thÆ° viá»‡n', 'N'),\n",
       " ('chÆ°a', 'R'),\n",
       " ('phÃ¡t triá»ƒn', 'V'),\n",
       " ('nhiá»u', 'A'),\n",
       " ('.', 'CH'),\n",
       " ('Má»™t sá»‘', 'L'),\n",
       " ('thÆ° viá»‡n', 'N'),\n",
       " ('phá»• biáº¿n', 'V'),\n",
       " ('lÃ ', 'V'),\n",
       " ('pyvi', 'N'),\n",
       " ('vÃ ', 'C'),\n",
       " ('underthesea', 'M')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Regex \n",
    "\n",
    "Some string processing techniques with regex was introduce in `2. [python] Classes`. Therefore, below will only introduce some common patterns in text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ğŸ”¥', 'ğ‘©', 'ğ‘¨', 'ğ‘ª', 'ğ‘²', 'ğ‘»', 'ğ‘¶', 'ğ‘º', 'ğ‘ª', 'ğ‘¯', 'ğ‘¶', 'ğ‘¶', 'ğ‘³', 'J', 'â„¢', 'ğŸ’ƒ', '\\U0001fab5', '\\U0001fab5', 'ã€', 'J', 'N', 'ã€‘', 'K', 'ğŸŒˆ', 'ğ—¡', 'ğ—˜', 'ğ—ª', 'ğ—”', 'ğ—¥', 'ğ—¥', 'ğ—œ', 'ğ—©', 'ğ—”', 'ğ—Ÿ', 'ğŸ’¢', 'ğŸ“½', 'ï¸', '9', '2', 'Q', 'ğŸ”µ', 'ğ…', 'ğ‘', 'ğ„', 'ğ„', 'ğ’', 'ğ‡', 'ğˆ', 'ğ', 'ğŸ”µ']\n"
     ]
    }
   ],
   "source": [
    "# find special characters\n",
    "pattern ='[^aÃ áº£Ã£Ã¡áº¡Äƒáº±áº³áºµáº¯áº·Ã¢áº§áº©áº«áº¥áº­bcdÄ‘eÃ¨áº»áº½Ã©áº¹Ãªá»á»ƒá»…áº¿á»‡fghiÃ¬á»‰Ä©Ã­á»‹jklmnoÃ²á»ÃµÃ³á»Ã´á»“á»•á»—á»‘á»™Æ¡á»á»Ÿá»¡á»›á»£pqrstuÃ¹á»§Å©Ãºá»¥Æ°á»«á»­á»¯á»©á»±vwxyá»³á»·á»¹Ã½á»µz\\s]'\n",
    "string = \"\"\"ğŸ”¥ğ‘©ğ‘¨ğ‘ªğ‘² ğ‘»ğ‘¶ ğ‘ºğ‘ªğ‘¯ğ‘¶ğ‘¶ğ‘³ balo Japan classicâ„¢   balo Ä‘i há»c  balo laptop   balo thá»i trang   balo chá»‘ng nÆ°á»›c\n",
    "ğŸ’ƒ Ä‘áº§m tráº¯ng ná»¯ cá»• vuÃ´ng eo chun vÃ¡y ná»¯ cá»™c tay cháº¥t Ä‘Å©i dÃ¡ng xÃ²e\n",
    "ğŸªµcÃ³ sáºµn set Ã¡o babydoll thÃ´ Ä‘Å©i viá»n ren kÃ¨m quáº§n Ä‘Ã¹i ğŸªµ\n",
    "ã€JNã€‘heybig spring and summer new Korean\n",
    "ğŸŒˆğ—¡ğ—˜ğ—ª ğ—”ğ—¥ğ—¥ğ—œğ—©ğ—”ğ—ŸğŸ’¢ Ã¡o khoÃ¡c kaki unisex ğŸ“½ï¸ videoáº£nh tháº­t a92\n",
    "Quáº§n jean nam trÆ¡n mÃ u xanh ğŸ”µ ğ…ğ‘ğ„ğ„ ğ’ğ‡ğˆğ ğŸ”µ quáº§n bÃ² nam co giÃ£n thá»i trang hpfashion\"\"\"\n",
    "\n",
    "print(re.findall(pattern,string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 'I ', ' '), (' ', 'a ', ' ')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find stop word\n",
    "pattern = '(^|\\s+)(\\S(\\s+|$))+'\n",
    "sen = 'I need a doctor'\n",
    "\n",
    "re.findall(pattern, sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://regex101.com/']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find url link\n",
    "pattern = 'http\\S+'\n",
    "sen = 'Reference: https://regex101.com/ (regex online checking)'\n",
    "\n",
    "re.findall(pattern, sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# find number of episol\n",
    "# pattern = '(?<=pháº§n\\s|táº­p\\s|t|t.)\\d+'\n",
    "# sen = \"\"\"thiÃªn tháº§n 1001 táº­p 19 \n",
    "# thiÃªn tháº§n 1001 táº­p 18 \n",
    "# phim trung quá»‘c: hÃ¡n sá»Ÿ tranh hÃ¹ng-t.85 \n",
    "# phim trung quá»‘c: hÃ¡n sá»Ÿ tranh hÃ¹ng-t86 \n",
    "# vá»¥ Ã¡n ngay bÃªn báº¡n: bá»™ hÃ i cá»‘t bÃ­ áº©n-pháº§n 7\"\"\"\n",
    "\n",
    "# re.findall(pattern, sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['www.google', 'id.zalo']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find sub domain of url\n",
    "pattern = '(?<=//)\\S+(?=\\.)'\n",
    "sen = \"\"\"https://www.google.ca/\n",
    "https://id.zalo.me/account/outapp\"\"\"\n",
    "\n",
    "re.findall(pattern, sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20211021121219895', '20211021185707803']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find item id in a url\n",
    "pattern = '\\d+(?=rf\\d+|\\.htm)'\n",
    "url1 = 'https://soha.vn/giam-doc-bv-bach-mai-nguyen-quang-tuan-bi-khoi-to-bo-y-te-noi-gi-20211021185707803.htm'\n",
    "url2 = 'https://soha.vn/phu-tho-ghi-nhan-them-17-ca-duong-tinh-voi-sars-cov-2-20211021121219895rf20211021185707803.htm'\n",
    "re.findall(pattern, url1)\n",
    "re.findall(pattern, url2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "*&#9829; By Quang Hung x Thuy Linh &#9829;*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

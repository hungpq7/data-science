{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Ensemble Learning overview\n",
    "Classical Machine Learning algorithms are usually shown to be poor when handling real-world datasets. Models fit from these algorithms often suffer from two problems: high bias and high variance; such a model is called a *weak learner*. In this topic, we are going through some elegant techniques that combine multiple algorithms to form a powerful model, which produces an improved overall result. This is referred to generally as [Ensemble Learning](https://en.wikipedia.org/wiki/Ensemble_learning). Enemble Learning methods in fact have proved their effectiveness in many Machine Learing competitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Voting\n",
    "[Voting](https://scikit-learn.org/stable/modules/ensemble.html#voting-classifier) (for classification) or [Averaging](https://scikit-learn.org/stable/modules/ensemble.html#voting-regressor) (for regression) is the simplest ensembling method. When doing voting for classification, there are two strategies can be applied: marjority voting on predicted results (hard voting) and taking argmax of the weighted average of predicted probabilities (soft voting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.set_printoptions(precision=4, floatmode='maxprec')\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import VotingClassifier, VotingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = KNeighborsClassifier()\n",
    "clf2 = DecisionTreeClassifier()\n",
    "clf3 = GaussianNB()\n",
    "\n",
    "models = [('knn', clf1), ('tree', clf2), ('gnb', clf3)]\n",
    "ensembler = VotingClassifier(models, voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.9379 (kNN Classifier)\n",
      "AUC = 1.0000 (Decision Tree)\n",
      "AUC = 0.9317 (Naive Bayes)\n",
      "AUC = 0.9783 (Voting Classifier)\n"
     ]
    }
   ],
   "source": [
    "models = [clf1, clf2, clf3, ensembler]\n",
    "names = ['kNN Classifier', 'Decision Tree', 'Naive Bayes', 'Voting Classifier']\n",
    "yTrue = y\n",
    "\n",
    "for name, model in zip(names, models):\n",
    "    model = model.fit(X, y)\n",
    "    yPred = model.predict(X)\n",
    "    auc = roc_auc_score(yTrue, yPred)\n",
    "    print(f'AUC = {auc:.4f} ({name})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Stacking\n",
    "[Stacking](https://scikit-learn.org/stable/modules/ensemble.html#stacked-generalization) involes in these steps:\n",
    "- Fit a number of weak learners to the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Bagging\n",
    "[Bagging](https://en.wikipedia.org/wiki/Bootstrap_aggregating) (Boostrap Aggregating) uses averaging method in order to reduce variance of the model. For specifically, bagging is divided into 2 parts: boostrapping and aggregating.\n",
    "- Boostrapping: We generate `bootstrap samples` of size n from an initial dataset of size N by `randomly with replacement` n observations. It's mean some obs can be repeat in each sample. Sampling with replacement ensures each bootstrap is independent from others.\n",
    "- Aggregating: After generating boostrap sample, we fit each of them into the weak learner. All the results of models will be combine by averaging (for regression) or voting the output (for classification). \n",
    "\n",
    "*Some assumptions come with bagging*:\n",
    "- The size for all bootstrap samples is fixed, it can be a number of observations or a ratio of the original size.\n",
    "- The distribution of each samples has to be the same and representativity for the initial dataset:\n",
    " + Firsly, the size N of the initial dataset should be large enough to capture most of the complexity of the underlying distribution so that sampling from the dataset is a good approximation of sampling from the real distribution\n",
    " + Secondly, the size N of the dataset should be large enough compared to the size B of the bootstrap samples so that samples are not too much correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Boosting \n",
    "The idea of boosting is to fit models iteratively such that the training of model at a given step depends on the models fitted at the previous steps in order to reduce bias.\\\n",
    "Unlike bagging, boosting use the result of previous model to improve the next model in the sequence. Observations in the dataset which were badly handled by the previous models will be weighted and fitting by next model. Intuitively, each new model focus its efforts on the most difficult observations to fit up to now, so at the end of the process, we have a strong learner with lower bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Random Forest\n",
    "*Reference: [Scikit Learn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Random Forest is a popular bagging method consists of a large number of Decision Trees, they operate as a committee outperforms any individual weak model. This wonderful effect - the wisdom of crowds - can be explained that trees protect each other from their individual errors. If trees share the same behaviors, they also make the same mistakes. So, the low correlation between trees is the key. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable randomness\n",
    "In a normal Decision Tree, every input variable is considered to find the best split at a node (so greedy). In contrast, each tree of a Random Forest only picks a random subset of variables, this forces even more variation amongst trees. Compare variable randomness to bootstrap aggregating, they both do sampling, but one selects rows, the other selects columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Notable hyperparameters:\n",
    "\n",
    "Hyperparameter|Meaning|Default value|Common values|\n",
    ":---|:---|:---|:---|\n",
    "`n_estimators`|Number of trees in the forest|`100`||\n",
    "`criterion`|Measure of impurity for each tree|`gini`|`entropy` `gini`|\n",
    "`max_features`|Number or ratio of variables used in each tree|`auto`|`0.8` `0.9`|\n",
    "`max_samples`|Number or ratio of instances used in each tree|`None`|`0.5`|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>bad_customer</th>\n",
       "      <th>credit_balance_percent</th>\n",
       "      <th>age</th>\n",
       "      <th>num_of_group1_pastdue</th>\n",
       "      <th>debt_ratio</th>\n",
       "      <th>income</th>\n",
       "      <th>num_of_loans</th>\n",
       "      <th>num_of_times_late_90days</th>\n",
       "      <th>num_of_estate_loans</th>\n",
       "      <th>num_of_group2_pastdue</th>\n",
       "      <th>num_of_dependents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.766127</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>0.802982</td>\n",
       "      <td>9120.0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.957151</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0.121876</td>\n",
       "      <td>2600.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.658180</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>0.085113</td>\n",
       "      <td>3042.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.233810</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036050</td>\n",
       "      <td>3300.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.907239</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>0.024926</td>\n",
       "      <td>63588.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  bad_customer  credit_balance_percent  age  num_of_group1_pastdue  \\\n",
       "0      0             1                0.766127   45                      2   \n",
       "1      1             0                0.957151   40                      0   \n",
       "2      2             0                0.658180   38                      1   \n",
       "3      3             0                0.233810   30                      0   \n",
       "4      4             0                0.907239   49                      1   \n",
       "\n",
       "   debt_ratio   income  num_of_loans  num_of_times_late_90days  \\\n",
       "0    0.802982   9120.0            13                         0   \n",
       "1    0.121876   2600.0             4                         0   \n",
       "2    0.085113   3042.0             2                         1   \n",
       "3    0.036050   3300.0             5                         0   \n",
       "4    0.024926  63588.0             7                         0   \n",
       "\n",
       "   num_of_estate_loans  num_of_group2_pastdue  num_of_dependents  \n",
       "0                    6                      0                2.0  \n",
       "1                    0                      0                1.0  \n",
       "2                    0                      0                0.0  \n",
       "3                    0                      0                0.0  \n",
       "4                    1                      0                0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit = pd.read_excel('data/credit_scoring.xlsx')\n",
    "credit = credit.dropna().reset_index()\n",
    "credit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = credit.Bad_customer.values\n",
    "x = credit.drop(columns='Bad_customer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 0.8, 'max_samples': 0.5}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'max_features': [0.8, 0.9],\n",
    "    'max_samples': [0.5]\n",
    "}\n",
    "\n",
    "forest = RandomForestClassifier()\n",
    "forest = GridSearchCV(forest, params, cv=5)\n",
    "forest = forest.fit(x_train, y_train)\n",
    "forest.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = forest.predict(x_train)\n",
    "y_test_pred = forest.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98     89543\n",
      "           1       0.99      0.58      0.73      6672\n",
      "\n",
      "    accuracy                           0.97     96215\n",
      "   macro avg       0.98      0.79      0.86     96215\n",
      "weighted avg       0.97      0.97      0.97     96215\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.96     22369\n",
      "           1       0.55      0.17      0.26      1685\n",
      "\n",
      "    accuracy                           0.93     24054\n",
      "   macro avg       0.75      0.58      0.61     24054\n",
      "weighted avg       0.91      0.93      0.92     24054\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. AdaBoost\n",
    "\n",
    "*Reference: [Scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)*\n",
    "\n",
    "AdaBoost is a specific boosting algorithm developed for binary classification. Adaboost usually come with decision tree has 1 node 2 leaves (called `stump`). Main idea of Adaboost are:\n",
    "- Combines several of stumps to make a strong learner\n",
    "- Stumps can be weighted so some stumps are important than others\n",
    "- Each stumps is made by taking mistake from previous stump, so the order of each stump in adaboost is important.\n",
    "\n",
    "**The workflow** \\\n",
    "Assume that we have $n$ samples (each sample has $a$ feature and has the same `sample weight` equal to $1/n$ - Total of sample weight is 1). To find the order of stump, we made stumps with each feature, calculate the gini index for each stump then the stump has the smallest gini will be the first.\n",
    "\n",
    "After that, we determine the weight of stump based on how well it classified the samples - or we call `Amount of say`. The total error of stump is `the sum of the weight assigned to the incorrect classified samples` (eg: if we have 2 incorrect samples then this stump will has $2/n$ total error). The formular of amount of say:\n",
    "\n",
    "$$\\mbox{Amount of say} = L*ln(\\frac{1-\\mbox{total error}}{\\mbox{total error}})$$\n",
    "\n",
    "With $L$ is learning rate and this equation mean the less total error the larger amount of say.\n",
    "\n",
    "The third idea of adaboost is the next stump is made by taking mistake from previous stump. For the next stump to relize the incorrect sample from first stump, we need to increase the sample weight of incorrect sample and decrease the others. Formular of new sample weight:\n",
    "- For incorrect sample:\n",
    "\n",
    "$$\\mbox{new sample weight} = \\mbox{sample weight} * e^{\\mbox{amount of say}}$$\n",
    "\n",
    "- For others:\n",
    "\n",
    "$$\\mbox{new sample weight} = \\mbox{sample weight} * e^{-\\mbox{amount of say}}$$\n",
    "\n",
    "Then we normalize all new sample weight to add up to 1. Using the new sample weigth for the next stump until we get all stump done.\n",
    "Lastly, to made prediction with adaboost, we have 2 group of stumps: classify on A or classify on B. If sum of amout of say of group A is bigger, the observation will be in class A and and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Notable hyperparameters:\n",
    "\n",
    "Hyperparameter|Meaning|Default value|Common values|\n",
    ":---|:---|:---|:---|\n",
    "`base_estimator`|Weak learner to build adaboost|`decisiontree`||\n",
    "`n_estimators`|Number of stumps in the forest|`50`|`100`|\n",
    "`learning_rate`||`1`|`1/2`|\n",
    "`algorithm`||`SAMME.R`||\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Bad_customer</th>\n",
       "      <th>credit_balance_percent</th>\n",
       "      <th>age</th>\n",
       "      <th>num_of_group1_pastdue</th>\n",
       "      <th>debt_ratio</th>\n",
       "      <th>income</th>\n",
       "      <th>num_of_loans</th>\n",
       "      <th>num_of_times_late_90days</th>\n",
       "      <th>num_of_estate_loans</th>\n",
       "      <th>num_of_group2_pastdue</th>\n",
       "      <th>num_of_dependents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.766127</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>0.802982</td>\n",
       "      <td>9120.0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.957151</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0.121876</td>\n",
       "      <td>2600.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.658180</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>0.085113</td>\n",
       "      <td>3042.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.233810</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036050</td>\n",
       "      <td>3300.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.907239</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>0.024926</td>\n",
       "      <td>63588.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  Bad_customer  credit_balance_percent  age  num_of_group1_pastdue  \\\n",
       "0      0             1                0.766127   45                      2   \n",
       "1      1             0                0.957151   40                      0   \n",
       "2      2             0                0.658180   38                      1   \n",
       "3      3             0                0.233810   30                      0   \n",
       "4      4             0                0.907239   49                      1   \n",
       "\n",
       "   debt_ratio   income  num_of_loans  num_of_times_late_90days  \\\n",
       "0    0.802982   9120.0            13                         0   \n",
       "1    0.121876   2600.0             4                         0   \n",
       "2    0.085113   3042.0             2                         1   \n",
       "3    0.036050   3300.0             5                         0   \n",
       "4    0.024926  63588.0             7                         0   \n",
       "\n",
       "   num_of_estate_loans  num_of_group2_pastdue  num_of_dependents  \n",
       "0                    6                      0                2.0  \n",
       "1                    0                      0                1.0  \n",
       "2                    0                      0                0.0  \n",
       "3                    0                      0                0.0  \n",
       "4                    1                      0                0.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit = pd.read_excel('data/credit_scoring.xlsx')\n",
    "credit = credit.dropna().reset_index()\n",
    "credit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = credit.Bad_customer.values\n",
    "x = credit.drop(columns='Bad_customer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'algorithm': 'SAMME.R', 'learning_rate': 0.5, 'n_estimators': 100}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'learning_rate': [0.5, 0.8, 1],\n",
    "    'n_estimators': [100],\n",
    "    'algorithm': ['SAMME','SAMME.R']\n",
    "}\n",
    "\n",
    "forest = AdaBoostClassifier()\n",
    "forest = GridSearchCV(forest, params, cv=5)\n",
    "forest = forest.fit(x_train, y_train)\n",
    "forest.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = forest.predict(x_train)\n",
    "y_test_pred = forest.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.97     89510\n",
      "           1       0.57      0.20      0.30      6705\n",
      "\n",
      "    accuracy                           0.93     96215\n",
      "   macro avg       0.76      0.60      0.63     96215\n",
      "weighted avg       0.92      0.93      0.92     96215\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.97     22402\n",
      "           1       0.55      0.19      0.29      1652\n",
      "\n",
      "    accuracy                           0.93     24054\n",
      "   macro avg       0.75      0.59      0.63     24054\n",
      "weighted avg       0.92      0.93      0.92     24054\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Gradient Boosting Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Gradient Boosting\n",
    "*Reference: [Scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier)*\\\n",
    "Gradient boosting is a powerful algorithm can work with both regression and classification problems. The purpose of GBM is minimizes the overall prediction error. The key idea is to set the target outcomes for the next model in the sequence in order to minimize the error.\\\n",
    "As same as adaboost, gradient boost has built the sequence of tree which the next tree is made by the error of the previous tree. There are some different between 2 algorithms:\n",
    "\n",
    "Feature|Adaboost|Gradient boost|\n",
    ":---|:---|:---|\n",
    "Weak learners| Stumps|Trees have the same sizes that larger than stumps (usually have 8-32 maximize leaves)|\n",
    "Weight of tree|Amount of say of each stump is different| All the trees have the same weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boosting for regression\n",
    "\n",
    "Assume that we have input data $(x_i, y_i)_n$ and loss function $L(y_i, F(x))$. For gradient boost, the common loss function is a differentiable function:\n",
    "$$L = \\frac{1}{2}(y-\\hat{y})^2$$\n",
    "The main purpose of algorithm is to find the argmin of loss function by using Gradient descent (for complicated function). \n",
    "\n",
    "Firstly, gradient boost will calculate the first argmin of loss function $F_0(x)$ - precisely the average of dependent variables $\\bar{y}$  and assign this value to the single node.\\\n",
    "Step 2 is a loop for $m$ trees run from 1 to $M$:\\\n",
    "**(A)** The algorithm will calculate the error of each prediction - called `pseudo residual`.\n",
    "$$r_{i,m} = y - F_{m-1}(x)$$  for $i = 1,2,\\dots,n$\n",
    "\n",
    "**(B)** The next tree in sequence will be fit with previous residuals. Each leaf in a tree is called `terminal region` $R_{j,m}$. The output value of each region is the average of residuals in that region. Formular of output of $R_{j,m}$ is:\n",
    "$$\\gamma_{j,m} = argmin\\sum_{x\\in R_{i,j}}{L(y_i, F_{m-1}(x_i)+\\gamma)}$$\n",
    "\n",
    "**(C)** Then we made the prediction for each sample by combining the previous trees and the residuals in new tree:\n",
    "$$F_m(x)=F_{m-1}(x)+\\nu\\sum_{j=1}^{J_m}{\\gamma_{j,m}}$$\n",
    "\n",
    "With $\\nu$ is the learning rate to prevent high variance. All tree are scaled by 1 learning rate. Each time adding a tree in prediction, the residual get smaller - meaning the predict get more accurate. The $F_m(x)$ is the output of gradient boosting.\n",
    "\n",
    "The process will stop when the residuals doesn't get smaller anymore or the model reach the maximum trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boosting for classification\n",
    "GBM for classification is as same as GBM for regression but instead of predict continuos values, algorithm will predict probabilities of the label to drop in class 0 or 1. \n",
    "\n",
    "Loss function of GBM now is the transformation of negative log(likelihood) - which predicted log(odds):\n",
    "$$L = - y * log(odds) + log(1+e^{log(odds)})$$\n",
    "The same as regression, the initial for prediction is $log(odds)$ - the argmin of loss function.\\\n",
    "Repeat $odds = \\frac{p}{1-p}$ with $p$ is probability of class 1.\n",
    "\n",
    "Use log(odd) to predict probability of a observation with formular:\n",
    "$$p = \\frac{e^{log(odds)}}{1+e^{log(odds)}}$$\n",
    "\n",
    "Step 2 is a loop for $m$ trees run from 1 to $M$:\\\n",
    "**(A)** The algorithm will calculate the `pseudo residual`.\n",
    "$$r = obs - p$$  \n",
    "\n",
    "**(B)** The next tree in sequence will be fit with previous residuals. . Exactly the equation of output of each leaf is:\n",
    "$$\\gamma_{j,m} = argmin\\sum{L(y_i, F_{m-1}(x_i)+\\gamma)}$$\n",
    "\n",
    "$$=> \\gamma_{j,m} = \\sum\\frac{residual}{p*(1-p)}$$\n",
    "\n",
    "\n",
    "**(C)** The new $log(odds)$ for each leaf will be result of combining between initial values, previous tree and residuals:\n",
    "$$F_m(x)=F_{m-1}(x)+\\nu\\sum{\\gamma_{j,m}}$$\n",
    "with $\\nu$ is learning rate\n",
    "\n",
    "Lastly, the prob of new tree:\n",
    "$$p = \\frac{e^{log(odds)}}{1+e^{log(odds)}}$$\n",
    "\n",
    "Continue until get the smallest residual or reach the maximum of trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Notable hyperparameters:\n",
    "\n",
    "Hyperparameter|Meaning|Default value|Common values|\n",
    ":---|:---|:---|:---|\n",
    "`loss`|The loss function|`deviance (logistic reg)`|`deviance`|\n",
    "`n_estimators`|Number of boosting stage|`100`||\n",
    "`learning_rate`||`0.1`||\n",
    "`subsample`|Fraction of sample used to fitting|`1.0`||\n",
    "`criterion`|Mearsure quality of split|`friedman_mse`||\n",
    "Tree parameters group|Some parameters prevent over fitting|`min_sample_split`,`min_sample_leaf`,`max_depth`,...||| \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Bad_customer</th>\n",
       "      <th>credit_balance_percent</th>\n",
       "      <th>age</th>\n",
       "      <th>num_of_group1_pastdue</th>\n",
       "      <th>debt_ratio</th>\n",
       "      <th>income</th>\n",
       "      <th>num_of_loans</th>\n",
       "      <th>num_of_times_late_90days</th>\n",
       "      <th>num_of_estate_loans</th>\n",
       "      <th>num_of_group2_pastdue</th>\n",
       "      <th>num_of_dependents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.766127</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>0.802982</td>\n",
       "      <td>9120.0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.957151</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0.121876</td>\n",
       "      <td>2600.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.658180</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>0.085113</td>\n",
       "      <td>3042.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.233810</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036050</td>\n",
       "      <td>3300.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.907239</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>0.024926</td>\n",
       "      <td>63588.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  Bad_customer  credit_balance_percent  age  num_of_group1_pastdue  \\\n",
       "0      0             1                0.766127   45                      2   \n",
       "1      1             0                0.957151   40                      0   \n",
       "2      2             0                0.658180   38                      1   \n",
       "3      3             0                0.233810   30                      0   \n",
       "4      4             0                0.907239   49                      1   \n",
       "\n",
       "   debt_ratio   income  num_of_loans  num_of_times_late_90days  \\\n",
       "0    0.802982   9120.0            13                         0   \n",
       "1    0.121876   2600.0             4                         0   \n",
       "2    0.085113   3042.0             2                         1   \n",
       "3    0.036050   3300.0             5                         0   \n",
       "4    0.024926  63588.0             7                         0   \n",
       "\n",
       "   num_of_estate_loans  num_of_group2_pastdue  num_of_dependents  \n",
       "0                    6                      0                2.0  \n",
       "1                    0                      0                1.0  \n",
       "2                    0                      0                0.0  \n",
       "3                    0                      0                0.0  \n",
       "4                    1                      0                0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit = pd.read_excel('data/credit_scoring.xlsx')\n",
    "credit = credit.dropna().reset_index()\n",
    "credit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = credit.Bad_customer.values\n",
    "x = credit.drop(columns='Bad_customer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'friedman_mse',\n",
       " 'learning_rate': 0.1,\n",
       " 'n_estimators': 100,\n",
       " 'subsample': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'learning_rate': [0.1,0.001],\n",
    "    'n_estimators': [100],\n",
    "    'subsample': [1],\n",
    "    'criterion':['friedman_mse']\n",
    "#     'min_sample_split':[10,5,2]  \n",
    "}\n",
    "\n",
    "gbm = GradientBoostingClassifier()\n",
    "gbm = GridSearchCV(gbm, params, cv=3)\n",
    "gbm = gbm.fit(x_train, y_train)\n",
    "gbm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = gbm.predict(x_train)\n",
    "y_test_pred = gbm.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.97     89550\n",
      "           1       0.62      0.20      0.30      6665\n",
      "\n",
      "    accuracy                           0.94     96215\n",
      "   macro avg       0.78      0.60      0.64     96215\n",
      "weighted avg       0.92      0.94      0.92     96215\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.97     22362\n",
      "           1       0.60      0.21      0.31      1692\n",
      "\n",
      "    accuracy                           0.93     24054\n",
      "   macro avg       0.77      0.60      0.64     24054\n",
      "weighted avg       0.92      0.93      0.92     24054\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. XGBoost\n",
    "\n",
    "**Signing**:\n",
    "- $O_{value}$: Output value\n",
    "- $\\lambda$ : Regulaziration parameter\n",
    "- $\\gamma$: Tree complexity parameter\n",
    "- $\\eta$: Learning rate\n",
    "- Gradient ($g$): First derivative of loss function\n",
    "- Hessian ($h$): Second derivative of loss function\n",
    "- Residual :$r$\n",
    "- Number of residual: $N_r$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The idea and mathematical behind xgboost\n",
    "\n",
    "The first step of fitting XGboost to the training data is making the initial prediction $p_0$ (default is `0.5` for both regression and classification). Like GBM, XGboost fit a tree to the residual of each observation but each tree in XGboost is unique. XGB build a tree which try to minimize the equation:\n",
    "$$\\sum_{i=1}^{n}L(y_i,p_{i}^0+O_{value}) + \\frac{1}{2}\\lambda*{O^2_{value}}  \\space \\mbox{(1)}$$\n",
    "with $L$ is *loss function* and ($\\frac{1}{2}\\lambda*{O^2_{value}}$) is *regularization term*.\\\n",
    "The goal of XGB is finding the optimal output value to minimize the equation $(1)$.\n",
    "\n",
    "To make an Xgboost tree, firstly we calculate the residuals of each obs different to initial prediction then making similariry score for the root:\n",
    "$$\\mbox{Similariry Score} = \\frac{(\\sum gradient)^2}{\\sum hessian + \\lambda}$$\n",
    "Similarity score tell us the similarity between residual of observations, if residuals is different, they cancel out the others so that the similarity score will be small. And $\\lambda$ is the regularization parameter, it's intend to reduce the prediction's sensitivity to individual observations and then prevent overfitting. When $\\lambda$ increase, the similarity score wil be decrease - the amount of decrease is inversely proportional to the number of residual in the node.\\\n",
    "After having the similarity for the root, we find an optimal split point for the tree. We'll choose one threshold to split the obs and then calculating similarity score for each node. To quantify how much better the leaves similar residual than the root, we calculate the gain:\n",
    "$$\\mbox{gain} = \\mbox{left}_{similarity} + \\mbox{right}_{similarity} - \\mbox{root}_{similarity}$$\n",
    "\n",
    "We can compare the gain of this thresold to the others and the threshold which made the largest gain will be chosen. Continue building tree with other feature until getting the maximum split.\n",
    "\n",
    "Now we have the full tree, to prune the tree, we use $\\gamma$ parameter. If $gain - \\gamma < 0$, we prune that leave.\n",
    "\n",
    "Next step is find the output for each leaf of the tree. To do that, we derivate the equation $(1)$ and we get the formular for optimal output:\n",
    "$$O_{value}=\\frac{-\\sum gradient}{\\sum hessian + \\lambda}$$\n",
    "\n",
    "The larger $\\lambda$, the closer output value to 0.\\\n",
    "Finally, we use the output value plus learning rate to make the new prediction:\n",
    "$$p_0 + \\sum_{m=1}^{m}\\eta * O_{value}$$\n",
    "\n",
    "Like GBM, the process go on with the next tree which is build based on residual of previous tree. And ending by reaching the smallest residual or maximum tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application in regression\n",
    "\n",
    "In regression, XGB use loss function just like GBM:\n",
    "$$L(y_i,p_i) = \\frac{1}{2}(y_i-p_i)^2$$\n",
    "\n",
    "Then gradient and hessian of loss function is :\n",
    "$g_i = -(y_i-p_i)$ and $h_i = 1$\n",
    "\n",
    "Replace to $O_{value}$ and similarity score we will have:\n",
    "$$O_{value} = \\frac{\\sum r}{N_r + \\lambda}$$\n",
    "\n",
    "$$\\mbox{similarity score} = \\frac{\\sum r^2}{N_{r} + \\lambda} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application in classification\n",
    "\n",
    "In classification, the loss function is the negative likelihood:\n",
    "$$L(y_i,p_i) = -[y_i*log(p_i) + (1-y_i)*log(1-p_i)]$$\n",
    "Then gradient and hessian of loss function is: \n",
    "$g_i = -(y_i-p_i)$ and $h_i = p_i*(1-p_i)$\n",
    "\n",
    "Replace we have:\n",
    "$$O_{value} = \\frac{\\sum r}{N_r + \\lambda}$$\n",
    "\n",
    "$$\\mbox{similarity score} = \\frac{\\sum r^2}{cover+ \\lambda} $$\n",
    "with $Cover = \\sum h_i= \\sum[\\mbox{previous prob}*(1-\\mbox{previous prob})]$ \n",
    "\n",
    "Despite making probability as a new prediction, we predict $log(odds)$ of prediction then convert to probability (because ouput value is the $log(odds)$ now):\n",
    "\n",
    "$$log(odds) prediction = log(\\frac{p_0}{1-p_0}) + \\eta*O_{value}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization in xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. LightGBM\n",
    "\n",
    "LightGBM is a gradient boosting framework which use tree based learning algorithm. \n",
    "\n",
    "The reason why lightgbm became powerful are:\n",
    "1. Optimization in accuracy: \n",
    "- LigthGBM uses *leaf_wise* stategy to grow the tree vertically. It will choose the leaf with max delta  loss to grow. \n",
    "- Optimal split for category feature. Other boosting can only use one-hot encoding for category feature but lightGBM can treat feature values equally although values in numeric (i.e: gender 0-unknow, 1-man, 2-woman)\n",
    "2.  Optimization in speed and memory usage: \n",
    "- LightGBM uses histogram-based algorithms, which bucket continuous feature values into discrete bins. Algo will find the best split between bins.\n",
    "- LightGBM uses Gradient-based one side sampling (GOSS) method. By usual, subsample process is done by taking random sample from dataset. But GOSS not only perform a faster way to do this but also keep distribution of the data. The instances with larger gradients (under-trained), contribute a lot more to the tree building process so we keep all instances with large gradient, but to prevent changing the distribution, we need to perform random sampling on instances with small gradients. Assume we have \n",
    "$n$ data instances, if we keep $a$ instances with large gradients and randomly samples $x$% $(n-a)$ instances with small gradients, we have the sampled data with size $a+ x$% $*(n-a)$. \n",
    "3. Sparse Optimization:\n",
    "- LightGBM handle sparse data by using Exclusive feature bundling (EFB). EFB is a technique that uses a greedy algorithm to combine (or bundle) these mutually exclusive (means never take non-zero in the same time) into a single feature and thus reduce the dimensionality.\n",
    "4. Optimization in Distributed Learning:\n",
    "- Distributed learning allows the use of multiple machines to produce a single model.\n",
    "- Feature parallel: Data had split vertically, each worker handle a subset of feature to find the best split on local feature set then they communicate to others to get the best one. \n",
    "- Data parallel: Data had split horizontally, each worker will construct the local histograms then merge them to make global histogram of the data and find the best split on global histogram. \n",
    "- Voting parallel: It is the special case of data parallel when communication cost is constant.\n",
    "\n",
    "||Data is small| Data is large\n",
    "|:--|:--|:--\n",
    "**Feature  is small**|Feature parallel|Data parallel|\n",
    "**Feature is large**|Feaure parallel|Voting parallel|\n",
    "\n",
    "<img src='data/parallel_lgbm.png' style='height:500px; margin: 0 auto 20px;'>\n",
    "\n",
    "LightGBM also supports GPU learning. The disadvantage of LGBM is it can easily overfit with small dataset and can't be use with one-hot encoding because of optimal category feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "*Reference*: [LightGBM docs](https://lightgbm.readthedocs.io/en/latest/Parameters.html)\\\n",
    "There are several parameters in LGBM which separate into groups:\n",
    "1. Core parameters\n",
    "- task: Type of task which perform on your data. Default is `train`, common is `train`/`predict`\n",
    "- object: Type of problem - regression, binary or multiclass. Default is `regression`.\n",
    "- boosting: Type of algorithms. Default is `GDBT`. Common value is `GOSS`.\n",
    "- num_iterations: Number of interation. Default is 100\n",
    "- learning_rate: Default is `0.1`. Common values are `0.001`, `0.003`\n",
    "- num_leaves: Max number of leaves in full tree. Default is `31`\n",
    "- tree_learner: type of distributed learning. Default is `serial`. Common values are: `feature`,`data`,`voting`.\n",
    "- device_type: `cpu` or `gpu`. GPU can perform better than CPU\n",
    "- data: path of training data\n",
    "- valid: path of validation data. Support multiple validation data\n",
    "2. Learning control parameter\n",
    "- max_depth: The maximum depth of tree. Default is `-1` means no limit.\n",
    "- min_data_in_leaf: The minimum observations a leaf. Default is `20`\n",
    "- feature_fraction: Subset of feature in each tree. Default is `1.0`\n",
    "- bagging_fraction: Subsample data. Default is `1.0`\n",
    "- early_stopping_round: Model will stop training if one metric of one validation data doesnt improve in last early_stopping_round rounds. Default is `0`.\n",
    "- lambda: lambda specifies regularization (L1/L2). Default is `0.0`\n",
    "- min_gain_to_split:the minimal gain to perform split. Default is `0.0`\n",
    "- max_cat_threshold:Limit number of split points considered for categorical features. Default is `32`\n",
    "3. IO parameter\n",
    "- max_bin: Max number of bins that feature values will be bucketed in. Default is `255`\n",
    "- categorical_feature: used to specify categorical features. Fill with index or name of columns.\n",
    "- is_enable_sparse: used to enable/disable sparse optimization. Default is `true`\n",
    "- enable_bundle: used EFB to bunlding feature. Default is `true`\n",
    "- weight_column: used to specify the weight column. Fill with index or name of column\n",
    "- save_binary: if true, LightGBM will save the dataset (including validation data) to a binary file. This speed ups the data loading for the next time. Default is `true`\n",
    "4. Metric parameter\n",
    "- metric: metric(s) to be evaluated on the evaluation set(s). Default is `\"\"`. Common values are `mae`,`mse`,`auc`,`binary_logloss`,..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgbm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Bad_customer</th>\n",
       "      <th>credit_balance_percent</th>\n",
       "      <th>age</th>\n",
       "      <th>num_of_group1_pastdue</th>\n",
       "      <th>debt_ratio</th>\n",
       "      <th>income</th>\n",
       "      <th>num_of_loans</th>\n",
       "      <th>num_of_times_late_90days</th>\n",
       "      <th>num_of_estate_loans</th>\n",
       "      <th>num_of_group2_pastdue</th>\n",
       "      <th>num_of_dependents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.766127</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>0.802982</td>\n",
       "      <td>9120.0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.957151</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0.121876</td>\n",
       "      <td>2600.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.658180</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>0.085113</td>\n",
       "      <td>3042.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.233810</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036050</td>\n",
       "      <td>3300.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.907239</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>0.024926</td>\n",
       "      <td>63588.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  Bad_customer  credit_balance_percent  age  num_of_group1_pastdue  \\\n",
       "0      0             1                0.766127   45                      2   \n",
       "1      1             0                0.957151   40                      0   \n",
       "2      2             0                0.658180   38                      1   \n",
       "3      3             0                0.233810   30                      0   \n",
       "4      4             0                0.907239   49                      1   \n",
       "\n",
       "   debt_ratio   income  num_of_loans  num_of_times_late_90days  \\\n",
       "0    0.802982   9120.0            13                         0   \n",
       "1    0.121876   2600.0             4                         0   \n",
       "2    0.085113   3042.0             2                         1   \n",
       "3    0.036050   3300.0             5                         0   \n",
       "4    0.024926  63588.0             7                         0   \n",
       "\n",
       "   num_of_estate_loans  num_of_group2_pastdue  num_of_dependents  \n",
       "0                    6                      0                2.0  \n",
       "1                    0                      0                1.0  \n",
       "2                    0                      0                0.0  \n",
       "3                    0                      0                0.0  \n",
       "4                    1                      0                0.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit = pd.read_excel('data/credit_scoring.xlsx')\n",
    "credit = credit.dropna().reset_index()\n",
    "credit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = credit.Bad_customer.values\n",
    "x = credit.drop(columns='Bad_customer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = lgbm.Dataset(x_train, label=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'learning_rate': [0.001],\n",
    "    'boosting_type': 'goss',\n",
    "    'objective': [1],\n",
    "    'criterion':['friedman_mse']\n",
    "#     'min_sample_split':[10,5,2]  \n",
    "}\n",
    "\n",
    "gbm = GradientBoostingClassifier()\n",
    "gbm = GridSearchCV(gbm, params, cv=3)\n",
    "gbm = gbm.fit(x_train, y_train)\n",
    "gbm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = gbm.predict(x_train)\n",
    "y_test_pred = gbm.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "*&#9829; By Quang Hung x Thuy Linh &#9829;*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

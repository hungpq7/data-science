{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Overview\n",
    "So far, we have been familiar in dealing with [cross-sectional] data and [time series] data, which are all structured data. However, most real-world data are [unstructured], such as text. So we would want to [mine text data] to identify meaningful patterns and insights. This goal is achieved in [Natural Language Processing] (NLP), a sub-field of [Artificial Intelligence] aiming to help computers understand human language.\n",
    "\n",
    "NLP is applied in a huge number of problems, some high-level applications appear in daily life are: autocomplete (like in Google Search), machine translation (like Google Translate), grammar error detection and correction (like in Microsoft Word) and virtual assistant (like Apple's Siri and Amazon's Alexa). In terms of data mining, common NLP tasks are: [keyword extraction], [sentiment analysis], [named-entity recognition], [document classification] and [email filtering].\n",
    "\n",
    "In order to *teach* machines to *understand* human language, there are usually two tasks need to be done: (1) text vectorization, including cleaning text data and (2) modeling. This topic covers the first task, which the ultimate goal is to represent text numerically. The featuring libraries in this topic are [NLTK] (Natural Language Toolkit), [Spacy] and [Gensim].\n",
    "\n",
    "[cross-sectional]: https://en.wikipedia.org/wiki/Cross-sectional_data\n",
    "[time series]: https://en.wikipedia.org/wiki/Time_series\n",
    "[unstructured]: https://en.wikipedia.org/wiki/Unstructured_data\n",
    "[mine text data]: https://en.wikipedia.org/wiki/Text_mining\n",
    "[Natural Language Processing]: https://en.wikipedia.org/wiki/Natural_language_processing\n",
    "[Artificial Intelligence]: https://en.wikipedia.org/wiki/Artificial_intelligence\n",
    "[keyword extraction]: https://en.wikipedia.org/wiki/Keyword_extraction\n",
    "[sentiment analysis]: https://en.wikipedia.org/wiki/Sentiment_analysis\n",
    "[named-entity recognition]: https://en.wikipedia.org/wiki/Named-entity_recognition\n",
    "[document classification]: https://en.wikipedia.org/wiki/Document_classification\n",
    "[email filtering]: https://en.wikipedia.org/wiki/Email_filtering\n",
    "[NLTK]: https://github.com/nltk/nltk\n",
    "[Spacy]: https://github.com/explosion/spaCy\n",
    "[Gensim]: https://github.com/RaRe-Technologies/gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Lexical analysis\n",
    "Lexical analysis is the process of converting a sequence of characters into a sequence of tokens (strings with an assigned and thus identified meaning). A program that performs lexical analysis may be termed a lexer or tokenizer. A lexer is the first step of NLP project that its output is the input of parsing process - helping the parsing more easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Text cleaning\n",
    "Typical text cleaning tasks should be done (if needed) before any more advance techniques. Text cleaning, if being done right, can reduce the dimensionality and improve language understanding. In this section I will make up a document and implement the following cleaning steps:\n",
    "- Lowercasing\n",
    "- Spell correction\n",
    "- Removal of emojis and emoticons\n",
    "- Removal of HTML tags\n",
    "- Removal of URLs\n",
    "- Removal of punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T15:44:34.700518Z",
     "iopub.status.busy": "2022-12-10T15:44:34.699261Z",
     "iopub.status.idle": "2022-12-10T15:44:36.464818Z",
     "shell.execute_reply": "2022-12-10T15:44:36.464162Z",
     "shell.execute_reply.started": "2022-12-10T15:44:34.700467Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T21:10:13.255029Z",
     "iopub.status.busy": "2022-12-10T21:10:13.254547Z",
     "iopub.status.idle": "2022-12-10T21:10:13.259457Z",
     "shell.execute_reply": "2022-12-10T21:10:13.258660Z",
     "shell.execute_reply.started": "2022-12-10T21:10:13.254997Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once a Spark session is created, its running jobs can be monitored at http://localhost:4040/jobs/.\n",
      "The default <code style='font-size:13px; color:firebrick'>learning_rate</code> in XGBoost is 0.3.\n",
      "By default, a notebook server runs locally at 127.0.0.1 and can be accessed from the browser using 127.0.0.1:8888.\n",
      "Read the <b>paper</b> of Word2Vec at https://arxiv.org/pdf/1301.3781.pdf and GloVe https://nlp.stanford.edu/pubs/glove.pdf.\n"
     ]
    }
   ],
   "source": [
    "doc = '''Once a Spark session is created, its running jobs can be monitored at http://localhost:4040/jobs/.\n",
    "The default <code style='font-size:13px; color:firebrick'>learning_rate</code> in XGBoost is 0.3.\n",
    "By default, a notebook server runs locally at 127.0.0.1 and can be accessed from the browser using 127.0.0.1:8888.\n",
    "Read the <b>paper</b> of Word2Vec at https://arxiv.org/pdf/1301.3781.pdf and GloVe https://nlp.stanford.edu/pubs/glove.pdf.'''\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T21:14:05.875807Z",
     "iopub.status.busy": "2022-12-10T21:14:05.875437Z",
     "iopub.status.idle": "2022-12-10T21:14:05.879235Z",
     "shell.execute_reply": "2022-12-10T21:14:05.878173Z",
     "shell.execute_reply.started": "2022-12-10T21:14:05.875781Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "patternURL = r'http[\\S]+[^.\\s]'\n",
    "patternIP = r'\\d+(?:\\.\\d+){3}(?::\\d+)?'\n",
    "patternHTML = r'<.*?>'\n",
    "patternPunc = r'[^\\w\\s]'\n",
    "patternNum = r'\\d'\n",
    "patternEnChar = r'[^a-z\\s+]+'\n",
    "patternViChar ='[^aàảãáạăằẳẵắặâầẩẫấậbcdđeèẻẽéẹêềểễếệfghiìỉĩíịjklmnoòỏõóọôồổỗốộơờởỡớợpqrstuùủũúụưừửữứựvwxyỳỷỹýỵz\\s]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T21:14:06.973577Z",
     "iopub.status.busy": "2022-12-10T21:14:06.973111Z",
     "iopub.status.idle": "2022-12-10T21:14:06.978553Z",
     "shell.execute_reply": "2022-12-10T21:14:06.977591Z",
     "shell.execute_reply.started": "2022-12-10T21:14:06.973542Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://localhost:4040/jobs/',\n",
       " 'https://arxiv.org/pdf/1301.3781.pdf',\n",
       " 'https://nlp.stanford.edu/pubs/glove.pdf']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(patternURL, doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T21:14:07.803545Z",
     "iopub.status.busy": "2022-12-10T21:14:07.803137Z",
     "iopub.status.idle": "2022-12-10T21:14:07.808059Z",
     "shell.execute_reply": "2022-12-10T21:14:07.807223Z",
     "shell.execute_reply.started": "2022-12-10T21:14:07.803514Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['127.0.0.1', '127.0.0.1:8888']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(patternIP, doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T21:14:12.676286Z",
     "iopub.status.busy": "2022-12-10T21:14:12.675858Z",
     "iopub.status.idle": "2022-12-10T21:14:12.680650Z",
     "shell.execute_reply": "2022-12-10T21:14:12.679872Z",
     "shell.execute_reply.started": "2022-12-10T21:14:12.676250Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<code style='font-size:13px; color:firebrick'>\", '</code>', '<b>', '</b>']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(patternHTML, doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Tokenization\n",
    "Tokenization is the process of separating a piece of text into smaller units called tokens, where tokens can be characters, words or sentences. For example, consider the sentence \"Never give up\". The most common way of tokenizing is using white spaces. Assuming space as a delimiter, the tokenization of the sentence results in 3 tokens: \"Never\", \"give\" and \"up\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T16:45:48.841693Z",
     "iopub.status.busy": "2022-11-17T16:45:48.841194Z",
     "iopub.status.idle": "2022-11-17T16:45:53.734159Z",
     "shell.execute_reply": "2022-11-17T16:45:53.733404Z",
     "shell.execute_reply.started": "2022-11-17T16:45:48.841609Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from sspipe import p, px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence tokenization\n",
    "NLTK's sentence tokenizer uses [Punkt], a pre-trained model for English to devide a text into a list of sentences based on recognizing starting words and sentence boundaries.\n",
    "\n",
    "[Punkt]: https://www.nltk.org/api/nltk.tokenize.punkt.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Punkt knows that the periods in Mr. Smith and Johann S. Bach do not mark sentence boundaries.',\n",
       " 'and sometimes sentences can start with non-capitalized words.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''Punkt knows that the periods in Mr. Smith and Johann S. Bach do not mark sentence boundaries.  \n",
    "and sometimes sentences can start with non-capitalized words.\n",
    "'''\n",
    "\n",
    "nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word tokenization\n",
    "Word tokenization is the most commonly used tokenization approach. It splits a piece of text into individual words based on certain delimiters (whitespaces, puntuations). Depending upon delimiters, different word-level tokens are formed. In NLTK, we can split a text piece into words using many strategies, most noticable:\n",
    "- <code style='font-size:13px'><a href='https://www.nltk.org/api/nltk.tokenize.treebank.html'>word_tokenize()</a></code>\n",
    "implements the Treebank pre-trained model.\n",
    "- <code style='font-size:13px'><a href='https://www.nltk.org/api/nltk.tokenize.regexp.html'>regexp_tokenize()</a></code>\n",
    "whichsplits text using a custom regex pattern. There are other convinent functions too, such as\n",
    "<code style='font-size:13px'>wordpunct_tokenize()</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-17T16:46:23.154884Z",
     "iopub.status.busy": "2022-11-17T16:46:23.154523Z",
     "iopub.status.idle": "2022-11-17T16:46:23.167061Z",
     "shell.execute_reply": "2022-11-17T16:46:23.166229Z",
     "shell.execute_reply.started": "2022-11-17T16:46:23.154857Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK', 'has', 'been', 'called', '``', 'a', 'wonderful', 'tool', 'for', 'computational', 'linguistics', '!', \"''\"]\n",
      "['NLTK', 'has', 'been', 'called', '\"', 'a', 'wonderful', 'tool', 'for', 'computational', 'linguistics', '!\"']\n",
      "['NLTK', 'has', 'been', 'called', 'a', 'wonderful', 'tool', 'for', 'computational', 'linguistics']\n"
     ]
    }
   ],
   "source": [
    "text = '''NLTK has been called \"a wonderful tool for computational linguistics!\"'''\n",
    "\n",
    "text | p(nltk.word_tokenize) | p(print)\n",
    "text | p(nltk.wordpunct_tokenize) | p(print)\n",
    "text | p(nltk.regexp_tokenize, pattern=r'\\w+') | p(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Word normalization\n",
    "English documents use different forms of a word, such as the [inflected word] \"historical\" is derived from the [root word] \"history\". Words in each family are usually related and have similar meanings, so in many situations, it is useful to use a word as a replacement for other words in the family. This technique is called *word normalization*; it has two approaches, *stemming* and *lemmatization*.\n",
    "\n",
    "[inflected word]: https://en.wikipedia.org/wiki/Inflection\n",
    "[root word]: https://en.wikipedia.org/wiki/Root_(linguistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "[Stemming] is a *logical* normalization technique that removes common prefixes and suffixes from an inflected word. The result is called a [stem]; a stem can be meaningless. For example, the word \"studying\" has its stem \"studi\" because the stemmer has a step that removes the \"-ing\" suffix and another step that substitues \"-y\" with \"-i\". NLTK supports two stemming algorithms, [Porter stemmer] (1980) and [Lancaster stemmer] (1990).\n",
    "\n",
    "[Stemming]: https://en.wikipedia.org/wiki/Stemming\n",
    "[stem]: https://en.wikipedia.org/wiki/Word_stem\n",
    "[Porter stemmer]: https://www.nltk.org/api/nltk.stem.porter.html\n",
    "[Lancaster stemmer]: https://www.nltk.org/api/nltk.stem.lancaster.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runner    -> runner\n",
      "running   -> run\n",
      "easy      -> easi\n",
      "easily    -> easili\n",
      "studied   -> studi\n",
      "studying  -> studi\n"
     ]
    }
   ],
   "source": [
    "listWord = ['runner', 'running', 'easy', 'easily', 'studied', 'studying']\n",
    "stemmer = nltk.PorterStemmer()\n",
    "for word in listWord:\n",
    "    stem = stemmer.stem(word)\n",
    "    print(f'{word:<9} -> {stem}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runner    -> run\n",
      "running   -> run\n",
      "easy      -> easy\n",
      "easily    -> easy\n",
      "studied   -> study\n",
      "studying  -> study\n"
     ]
    }
   ],
   "source": [
    "listWord = ['runner', 'running', 'easy', 'easily', 'studied', 'studying']\n",
    "stemmer = nltk.LancasterStemmer()\n",
    "for word in listWord:\n",
    "    stem = stemmer.stem(word)\n",
    "    print(f'{word:<9} -> {stem}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "[Lemmatization], unlike stemming, is a *physical* technique that looks up the corresponding [lemma] of a word from a prepared database. Due to this behavious, the technique is capable of handling irregular cases such as past tense of some verbs. Lemmatization, however, requires a large database to match the number of cases that stemming can cover, and thus it is more slowly. In NLTK, [Wordnet lemmatizer] works the best when being provided the appropriate *part-of-speech*.\n",
    "\n",
    "[Lemmatization]: https://en.wikipedia.org/wiki/Lemmatisation\n",
    "[lemma]: https://en.wikipedia.org/wiki/Lemma_(morphology)\n",
    "[Wordnet lemmatizer]: https://www.nltk.org/api/nltk.stem.wordnet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feet      -> foot\n",
      "minima    -> minimum\n",
      "dogs      -> dog\n",
      "leaves    -> leaf\n",
      "axes      -> ax\n",
      "mice      -> mouse\n"
     ]
    }
   ],
   "source": [
    "listWord = ['feet', 'minima', 'dogs', 'leaves', 'axes', 'mice']\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "for word in listWord:\n",
    "    lemma = lemmatizer.lemmatize(word, 'n')\n",
    "    print(f'{word:<9} -> {lemma}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ran       -> run\n",
      "read      -> read\n",
      "ate       -> eat\n",
      "fallen    -> fall\n",
      "sung      -> sing\n",
      "bought    -> buy\n"
     ]
    }
   ],
   "source": [
    "listWord = ['ran', 'read', 'ate', 'fallen', 'sung', 'bought']\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "for word in listWord:\n",
    "    lemma = lemmatizer.lemmatize(word, 'v')\n",
    "    print(f'{word:<9} -> {lemma}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "higher    -> high\n",
      "strongest -> strong\n",
      "better    -> good\n"
     ]
    }
   ],
   "source": [
    "listWord = ['higher', 'strongest', 'better']\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "for word in listWord:\n",
    "    lemma = lemmatizer.lemmatize(word, 'a')\n",
    "    print(f'{word:<9} -> {lemma}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Stop words removal \n",
    "Another interesting preprocessing technique for text data is removing [stop words]. They are common words but have a very little meaning (such as \"the\", \"a\" and \"in\"), and are usually filtered out by search engines while fetching results from the database. In NLP, specifically for mining tasks such as keyword extraction, sentiment analysis and document classification, it makes sense to remove stop words. But for AI applications like machine translation and question answering, stop words serve an important role and should not be discarded.\n",
    "\n",
    "[stop words]: https://en.wikipedia.org/wiki/Stop_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords, STOPWORDS\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we', 'been', 'everywhere', 'does', 'upon']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stopwordsGensim = list(STOPWORDS)\n",
    "stopwordsNltk = list(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "display(stopwordsGensim[:5])\n",
    "display(stopwordsNltk[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'songs know dark'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'my songs know what you did in the dark'\n",
    "remove_stopwords(text, stopwordsNltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Text representation\n",
    "Machine Learning algorithms require numeric input and do not process the string or raw text. The task of converting text data into numerical data is generally called text vectorization. Before moving to the technical details, we define some basic terminologies:\n",
    "- a *token* or a *term* is usually a word\n",
    "- a *document* is a collection of tokens, equivalent to an observation \n",
    "- a *corpus* is a collection of documents, equivalent to the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Text vectorization\n",
    "In this appoach, the final goal is to transform *each word into a scalar*. The corpus will then be transformed into a matrix, which resembles tabular data and let us apply tabular learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram\n",
    "[N-gram] is an extended way to define a token, that considers a sequence of $n$ consecutive words rather than a single word. In English, most of the time we would want to use $n=1$ (unigram), $n=2$ (bigram) or $n=3$ (trigram).\n",
    "\n",
    "[N-gram]: https://en.wikipedia.org/wiki/N-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag Of Words\n",
    "[Bag Of Words] (BOW) treats each token in the corpus a feature and counts how many times that word occurs in each document. This is a very simple technique with some notable downsides:\n",
    "- The number of returned features equals to the vocabulary size and is thus very large. The matrix of token counts is very sparse and is not memory efficient.\n",
    "- It does not take into account order of words in the documents, which is a very important properties of sequential data.\n",
    "\n",
    "[Bag Of Words]: https://en.wikipedia.org/wiki/Bag-of-words_model\n",
    "[n-gram]: https://en.wikipedia.org/wiki/N-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "[TF-IDF] is an improved version of BOW, calculated as the product of two components, $\\text{TF}$ (Term Frequency) and $\\text{IDF}$ (Inverse Document Frequency). Let's say for a token $t$, a document $d$ and the corpus $D$, we first define two basic concepts, *term frequency* and *document frequency*:\n",
    "- *Term frequency*, denoted $\\text{TF}(t,d)$, implies how frequent the token $t$ appears in the document $d$. There are several definitions for $\\text{TF}$, including *binary*, *count* and *ratio* (divided by document size).\n",
    "- *Document frequency*, denoted $\\text{DF}(t,d,D)$, is the ratio of the document $d$ that contains token $t$ in the entire corpus $D$. A high value of $\\text{DF}$ tells us that the token $t$ is popular word in the corpus and is likely to be a stop word.\n",
    "\n",
    "In the calculation, we take the inverse of document frequency (so that it penalizes stop words) then log transform it (to avoid exploding values): $\\text{IDF}=\\log(1\\div\\text{DF})$. The final calculation is $\\text{TF-IDF}=\\text{TF}(t,d)\\times\\text{IDF}(t,d,D) $.\n",
    "\n",
    "[TF-IDF]: https://en.wikipedia.org/wiki/Tf%E2%80%93idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Scikit-learn implements BoW via the class\n",
    "<code style='font-size:13px'><a href=https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html>CountVectorizer</a></code>\n",
    "and TF-IDF via the class\n",
    "<code style='font-size:13px'><a href=https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html>TfidfVectorizer</a></code>.\n",
    "Both classes are very powerful as they already integrate tokenization, stop words removal and other customization for steps like word normalization. The later class is highly recommended in practice because it can be easily transformed to BoW by removing both IDF re-weighting and normalization. It has the following hyperparameters:\n",
    "- <code style='font-size:13px; color:firebrick;'>prepocessor</code>: the function that processes raw text, suites for tasks like lowercasing and HTML tags removal.\n",
    "- <code style='font-size:13px; color:firebrick;'>tokenizer</code>: the custom tokenizer; the easiest way to think about it is NLTK's tokenization functions.\n",
    "- <code style='font-size:13px; color:firebrick;'>stop_words</code>: the list of stop words.\n",
    "- <code style='font-size:13px; color:firebrick;'>ngram_range</code>: the tuple containing lower and upper bounds of $n$ in N-gram. For example, *(2,3)* means using bigram and trigram.\n",
    "- <code style='font-size:13px; color:firebrick;'>max_df</code> and\n",
    "<code style='font-size:13px; color:firebrick;'>min_df</code>:\n",
    "the thresholds for filtering very frequent and very rare words.\n",
    "- <code style='font-size:13px; color:firebrick;'>max_features</code>: the maximum vocabulary size, default to *None*. Features are removed based on their rank of term frequency across the corpus.\n",
    "- <code style='font-size:13px; color:firebrick;'>binary</code>: whether to use binary TF, defaults to *False*.\n",
    "- <code style='font-size:13px; color:firebrick;'>sublinear_tf</code>: whether to apply log transformation to TF, defaults to *False*.\n",
    "- <code style='font-size:13px; color:firebrick;'>use_idf</code>: whether to divide the result by IDF, defaults to *True*.\n",
    "- <code style='font-size:13px; color:firebrick;'>smooth_idf</code>: whether to add smoothing term to IDF calculation, defaults to *True*.\n",
    "- <code style='font-size:13px; color:firebrick;'>norm</code>: whether to normalize the final result, defaults to *l2*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-27T07:04:55.746096Z",
     "iopub.status.busy": "2022-11-27T07:04:55.745589Z",
     "iopub.status.idle": "2022-11-27T07:04:59.518637Z",
     "shell.execute_reply": "2022-11-27T07:04:59.517980Z",
     "shell.execute_reply.started": "2022-11-27T07:04:55.746021Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import gensim\n",
    "from sspipe import p, px\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-27T07:05:00.462271Z",
     "iopub.status.busy": "2022-11-27T07:05:00.461868Z",
     "iopub.status.idle": "2022-11-27T07:05:00.465441Z",
     "shell.execute_reply": "2022-11-27T07:05:00.464536Z",
     "shell.execute_reply.started": "2022-11-27T07:05:00.462248Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"I want to buy a nice bike for my girl. She broke her old bike last year.\", \n",
    "    \"I had a great time watching that movie last night. We shouuld do the same next week\", \n",
    "    \"If you buy this now, you will get 3 different products for free in the next 10 days.\", \n",
    "    \"I am living in a small house in France, and my wish is to learn how to ski and snowboad\",\n",
    "    \"It is time to invest in some tech stock. The stock market is will become very hot in the next few months\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-27T07:05:02.427352Z",
     "iopub.status.busy": "2022-11-27T07:05:02.426757Z",
     "iopub.status.idle": "2022-11-27T07:05:02.435510Z",
     "shell.execute_reply": "2022-11-27T07:05:02.433185Z",
     "shell.execute_reply.started": "2022-11-27T07:05:02.427308Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LemmaTokenizer:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = nltk.WordNetLemmatizer()\n",
    "        self.tokenizer = nltk.RegexpTokenizer(pattern='\\w+')\n",
    "    \n",
    "    def __call__(self, doc):\n",
    "        # doc = doc.lower()\n",
    "        doc = self.tokenizer.tokenize(doc)\n",
    "        doc = [\n",
    "            token\n",
    "            | p(self.lemmatizer.lemmatize, 'n')\n",
    "            | p(self.lemmatizer.lemmatize, 'v')\n",
    "            | p(self.lemmatizer.lemmatize, 'a')\n",
    "            for token in doc\n",
    "        ]\n",
    "        return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>want buy</th>\n",
       "      <td>0.231553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>watch</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.281151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>watch movie</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.281151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>week</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.281151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wish</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.258199</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wish learn</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.258199</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>0.231553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0         1    2         3    4\n",
       "want buy     0.231553  0.000000  0.0  0.000000  0.0\n",
       "watch        0.000000  0.281151  0.0  0.000000  0.0\n",
       "watch movie  0.000000  0.281151  0.0  0.000000  0.0\n",
       "week         0.000000  0.281151  0.0  0.000000  0.0\n",
       "wish         0.000000  0.000000  0.0  0.258199  0.0\n",
       "wish learn   0.000000  0.000000  0.0  0.258199  0.0\n",
       "year         0.231553  0.000000  0.0  0.000000  0.0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = dict(\n",
    "    tokenizer=LemmaTokenizer(),\n",
    "    token_pattern=None,\n",
    "    stop_words=gensim.parsing.preprocessing.STOPWORDS,\n",
    "    ngram_range=(1,2),\n",
    ")\n",
    "\n",
    "vectorizer = TfidfVectorizer(params)\n",
    "vectorizer.fit(corpus)\n",
    "\n",
    "columns = vectorizer.get_feature_names_out()\n",
    "data = vectorizer.transform(corpus).toarray()\n",
    "pd.DataFrame(data=data, columns=columns).T.tail(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Word embedding\n",
    "Text vectorization techniques have proven to be too naive for complicated tasks. This leads to the idea of representing words by vectors, in order to increase the capability of capturing hidden information. There are existing methods to encode words as vectors such as one-hot encoding, but it wastes a lot of memory because of its sparsity. With such an expensive representation, we would want word vectors to be more dense and to have the ability to capture semantics.\n",
    "\n",
    "To efficiently perform word embedding, there need to be two components: a good learning algorithm and a lot of text to train a pre-trained model. In most popular word embedding methods, who you associated with tell you who your are. In this section, we are going to learn about three word embedding techniques, [Word2Vec] (by Google in 2013), [GloVe] (by Stanford in 2014) and [FastText] (by Facebook in 2015), and how to implement them using [Gensim].\n",
    "\n",
    "[Word2Vec]: https://en.wikipedia.org/wiki/Word2vec\n",
    "[FastText]: https://en.wikipedia.org/wiki/FastText\n",
    "[GloVe]: https://en.wikipedia.org/wiki/GloVe\n",
    "[Gensim]: https://radimrehurek.com/gensim/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Word2Vec\n",
    "[Word2Vec] is a pre-trained probabilistic word embedding model based on simple Neural Networks. It relies on conditional probabilities that predict words using the their surrounding neighbors. For example, in the sentence \"The quick brown fox jumps over the lazy dog\", consider the *center word* \"fox\" and a *context window* of size $2$, then the *context words* will be \"quick\", \"brown\", \"jumps\" and \"over\". They will be one-hot encoded so that we have a vector for the center word and another vector for the context words, with $1$ in the positions of words they are meant to present and $0$ where else. If the vocabulary has $V=10\\,000$ words, then both the *center vector* and the *context vector* also have the size of $10\\,000$.\n",
    "\n",
    "The Word2Vec model can be obtained via two self-supervised algorithms, Skip-Gram and CBOW (Continuous Bag Of Words). The Skip-Gram algorithm models the probability that *context words* occur given a *center word*, for example:\n",
    "$\\text{P}(\\text{quick, brown, jumps, over}\\mid\\text{fox})$.\n",
    "CBOW, in contrast, predicts the *center word* when seeing some *context words*: \n",
    "$\\text{P}(\\text{fox}\\mid\\text{quick, brown, jumps, over})$.\n",
    "Each pair of a center vector and a context vector will contribute an observation to our training data.\n",
    "\n",
    "<img src='image/skip_gram_architecture.png' style='height:270px; margin:20px auto;'>\n",
    "\n",
    "Both algorithms use the same Neural Network architecture with only a single hidden layer, but switch the input and output layers. We are going to describe only Skip-Gram, as the same logic also applies to CBOW.\n",
    "- The input layer is constructed by *center vectors*. Because they are one-hot vectors, multiplying the input layer with any matrix is like looking up the corresponding information from a table.\n",
    "- The hidden layer has no bias term as well as activation function, so that the architecture is simple enough to handle a large amount of data. The corresponding weight matrix $\\mathbf{W}$ will be then used as embedding vectors. If we set up the hidden layer with $N=300$ nodes, then this will be the size of embedding vectors.\n",
    "- The output layer uses cross entropy loss function and softmax activation function, making what we are doing is predicting probabilities of context words.\n",
    "\n",
    "Overall, CBOW is a faster architecture than Skip-Gram. Besides, Word2Vec is usually impelemented with some improvements: negative sampling, subsampling of frequent words and context window shrinking.\n",
    "\n",
    "[Word2Vec]: https://code.google.com/archive/p/word2vec/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText\n",
    "[FastText] use the same training algorithm as Word2Vec, but it treats each center word as a composed of n-gram characters. For example, for $n=3$, the word \"train\" will get you a $n$-gram sub-word for each character (\"tr\", \"tra\", \"rai\", \"ain\", \"in\"). In practice, $n$ is set to be in range $[3,6]$ so that the model can capture all types of prefixes and suffixes. This design gives two advantages over Word2Vec:\n",
    "- *Capturing morphology*. For words such as \"train\" and \"trained\", FastText can understand they are similar beacause of both internal structures and contexts. Word2Vec only considers contexts and is likely to be less accurate.\n",
    "- *Handling out-of-vocabulary words*. The embedding for a word in FastText is computed as the sum of all subword vectors.\n",
    "\n",
    "[FastText]: https://fasttext.cc/docs/en/support.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe\n",
    "[GloVe] (Global Vectors) is an embedding algorithm makes use of the [co-occurrance matrix] $\\mathbf{X}\\in\\mathbb{R}^{V\\times V}$ ($V$ is the vocabulary size). The algorithm takes into account global statistics and thus neutralize the effect of frequent words. To understand how GloVe works, let's denote:\n",
    "- $x_{ik}$ the number of times the pair of words $i$, $k$ occurs\n",
    "- $x_i=\\sum_k x_{ik}$ the number of times any word $k$ appears in the context of word $i$\n",
    "- $P_{ik}=P(k|i)=x_{ik}\\div x_i$ the probability that the word $k$ appears in the context of word $i$\n",
    "- $\\mathbf{w}_i$ the vector representation of word $i$, which is what we want to learn\n",
    "\n",
    "The underlying idea of GloVe is about the ratio $P_{ik}:P_{jk}$, where $i$, $j$ are indices of context words and $k$ is the index of out-of-context words. In the example below, we examine the relationship between two words $i=\\text{ice}$ and $j=\\text{steam}$, by studying the ratio of their co-occurrance probabilities with different *probe words*, $k$. We observe that this ratio is extremely large or small when word $k$ is related to one of $i,j$; and is very close to $1$ when the probe word is related to both $i,j$ or neither.\n",
    "\n",
    "<img src='image/co-occurance-probabilities.png' style='height:100px; margin:20px auto;'>\n",
    "\n",
    "Based on this insight, GloVe constructs its loss function as the squared error between the dot product of two embedding vectors $\\mathbf{x}_i^\\text{T}\\mathbf{x}_j $ and the probability of co-occurrance $\\log P(x_{ij})$.\n",
    "\n",
    "[GloVe]: https://nlp.stanford.edu/projects/glove/\n",
    "[co-occurrance matrix]: https://en.wikipedia.org/wiki/Co-occurrence_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained models\n",
    "Gensim integrates all pre-trained embedding vectors from Word2Vec, GloVe and FastText, physically. When you load a specific package, Gensim will download it and load to a\n",
    "<code style='font-size:13px'><a href=https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors>KeyedVectors</a></code>.\n",
    "Note that this object is a *physical* collection of word vectors, not a *logical* model. You can use this object to extract embedded vectors and compute word similarities in different ways.\n",
    "\n",
    "All methods relates to *similarity* of this object, by default, use cosine similarity, which enables an interesting feature, *word calculation*. For example, if we remove *man* from *king* and then add *woman*, we should get something very close to *queen*. This is completely achievable using cosine similarity and will be demonstrated later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim.downloader as api\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fasttext-wiki-news-subwords-300',\n",
       " 'conceptnet-numberbatch-17-06-300',\n",
       " 'word2vec-ruscorpora-300',\n",
       " 'word2vec-google-news-300',\n",
       " 'glove-wiki-gigaword-50',\n",
       " 'glove-wiki-gigaword-100',\n",
       " 'glove-wiki-gigaword-200',\n",
       " 'glove-wiki-gigaword-300',\n",
       " 'glove-twitter-25',\n",
       " 'glove-twitter-50',\n",
       " 'glove-twitter-100',\n",
       " 'glove-twitter-200',\n",
       " '__testing_word2vec-matrix-synopsis']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(api.info()['models'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 958.5/958.4MB downloaded\n"
     ]
    }
   ],
   "source": [
    "embedder = api.load('fasttext-wiki-news-subwords-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-27T07:14:50.336566Z",
     "iopub.status.busy": "2022-11-27T07:14:50.336184Z",
     "iopub.status.idle": "2022-11-27T07:14:50.345060Z",
     "shell.execute_reply": "2022-11-27T07:14:50.343826Z",
     "shell.execute_reply.started": "2022-11-27T07:14:50.336520Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.9642, -0.6098,  0.6745,  0.3511,  0.4132, -0.2124,  1.3796,\n",
       "         0.1285,  0.3157,  0.6633,  0.3391, -0.1893, -3.325 , -1.1491,\n",
       "        -0.4129,  0.2195,  0.8706, -0.5062, -0.1278, -0.067 ,  0.0658,\n",
       "         0.4393,  0.1758, -0.5606,  0.1353],\n",
       "       [-1.242 , -0.3598,  0.5728,  0.3668,  0.6002, -0.189 ,  1.2729,\n",
       "        -0.3692,  0.0891,  0.4034,  0.2513, -0.2555, -3.9209, -1.11  ,\n",
       "        -0.2131, -0.2385,  0.9532, -0.5275, -0.0008, -0.3577,  0.5558,\n",
       "         0.7787,  0.4687, -0.778 ,  0.7838]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder['cat', 'dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-27T07:59:05.801307Z",
     "iopub.status.busy": "2022-11-27T07:59:05.800693Z",
     "iopub.status.idle": "2022-11-27T07:59:05.809875Z",
     "shell.execute_reply": "2022-11-27T07:59:05.808987Z",
     "shell.execute_reply.started": "2022-11-27T07:59:05.801268Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9202422"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder.similarity('king', 'queen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dog', 0.9590820074081421),\n",
       " ('monkey', 0.920357882976532),\n",
       " ('bear', 0.9143136739730835),\n",
       " ('pet', 0.9108031392097473),\n",
       " ('girl', 0.8880629539489746)]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder.most_similar('cat', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('meets', 0.8841924071311951),\n",
       " ('prince', 0.832163393497467),\n",
       " ('queen', 0.8257461190223694),\n",
       " ('’s', 0.8174097537994385),\n",
       " ('crow', 0.813499391078949)]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder.most_similar(positive=['woman', 'king'], negative=['man'], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom training\n",
    "An important thing we should be sure to distinguish, is a pre-trained model and a learning algorithm. All the mathematics above are learning algorithms, they will be useful when we want to train our custom embedding model for unpopular languages such as Vietnamese. We can intialize a new model and train from scratch with our own dataset via two Gensim classes,\n",
    "<code style='font-size:13px'><a href=https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec>Word2Vec</a></code> and\n",
    "<code style='font-size:13px'><a href=https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.FastText>FastText</a></code>.\n",
    "They have the following hyperparameters:\n",
    "- <code style='font-size:13px; color:firebrick;'>vector_size</code>: the size of each embedding vector, defaults to *100*. Usually higher is better.\n",
    "- <code style='font-size:13px; color:firebrick;'>sg</code>: whether to use Skip-Gram, otherwise use CBOW, defaults to *True*. \n",
    "- <code style='font-size:13px; color:firebrick;'>window</code>: the context window size, defaults to *5*. Recommended values are *10* for Skip-Gram and *5* for CBOW.\n",
    "- <code style='font-size:13px; color:firebrick;'>negative</code>: the number of noisy words drawn for negative sampling, defaults to *5*. Should be in range $[5,20]$.\n",
    "- <code style='font-size:13px; color:firebrick;'>sample</code>: the sub-sampling rate of frequent words, defaults to *0.001*. Recommended values are in range $[10^{-5},10^{-3}]$.\n",
    "- <code style='font-size:13px; color:firebrick;'>shrink_windows</code>: whether to shrink context window, defaults to *True*.\n",
    "- <code style='font-size:13px; color:firebrick;'>min_n</code> and\n",
    "<code style='font-size:13px; color:firebrick;'>max_n</code>: the range of $n$ in FastText's N-gram, defaults to *3* and *6*.\n",
    "\n",
    "The appropriate format for these two classes is a list of lists (tokenized sentences). When our custom model has been intialized successfully on a corpus, you can keep teaching it using more and more data by calling the\n",
    "<code style='font-size:13px'>train()</code> method. You can also call the <code style='font-size:13px'>wv</code> (word vectors) method to access the embedding vectors via a <code style='font-size:13px'>KeyedVectors</code> object. A good practice which is highly recommended by Gensim is only using the embedded vectors (*physical*) instead of the full model (*logical*) because the physical option spends less memory and can extract vectors much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec, FastText, KeyedVectors\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['semeval-2016-2017-task3-subtaskBC',\n",
       " 'semeval-2016-2017-task3-subtaskA-unannotated',\n",
       " 'patent-2017',\n",
       " 'quora-duplicate-questions',\n",
       " 'wiki-english-20171001',\n",
       " 'text8',\n",
       " 'fake-news',\n",
       " '20-newsgroups',\n",
       " '__testing_matrix-synopsis',\n",
       " '__testing_multipart-matrix-synopsis']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(api.info()['corpora'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus1 = list(api.load('text8'))[:100]\n",
    "corpus2 = [\n",
    "    ['human', 'interface', 'computer'],\n",
    "    ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
    "    ['eps', 'user', 'interface', 'system'],\n",
    "    ['system', 'human', 'system', 'eps'],\n",
    "    ['user', 'response', 'time'],\n",
    "    ['graph', 'trees'],\n",
    "    ['graph', 'minors', 'trees'],\n",
    "    ['graph', 'minors', 'survey'],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FastText(corpus1, vector_size=7, window=5, min_count=1, workers=4, sg=True)\n",
    "model.train(corpus2, total_examples=1, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save('output/word2vec_sample.vectors')\n",
    "embedder = KeyedVectors.load('output/word2vec_sample.vectors', mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.6818,  1.1834, -2.3567, -1.0533, -0.7062, -1.593 , -0.3097],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder['system']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Contextual embedding\n",
    "Word embedding has done a decent job improving the text representation task, but they cannot handle [heteronyms]. This is because, the context is needed to determine the true meaning of such words. For example, the word \"bank\" in \"river bank\" and \"bank deposit\" has completely different meanings but will gain the same embedding by Word2Vec or GloVe.\n",
    "\n",
    "This problem can be solved using contextual embedding, a technique that pretrains a *logical* model rather than a *physical* vectors collection. The model has the capable of transforming a word into different embedding vectors based on specific context. Many contextual embedding methods have been proposed out there, where the most noticeable ones are ULMFiT, ELMo, BERT and [Transformer].\n",
    "\n",
    "[heteronyms]: https://en.wikipedia.org/wiki/Heteronym_(linguistics)\n",
    "[Transformer]: https://tfhub.dev/google/collections/transformer_encoders_text/1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT\n",
    "[BERT] (Bidirectional Encoder Representations from Transformers) is a state-of-the-art architecture for pretraining word representation proposed by Google in 2018. It is designed so that the embedding of a word takes into account both left and right context words. BERT models are pre-trained on a large corpus of text, can be then fine-tuned for specific tasks. There are a lot of BERT pre-train models can be found in TensorFlow Hub:\n",
    "- [BERT-base], models by original authors\n",
    "- [BERT-experts], fine-tuned BERT-base models for different domains\n",
    "- [ALBERT], a lite version of BERT with reduced number of parameters\n",
    "- [ELECTRA], a BERT-like architecture that pre-trains a discriminator\n",
    "\n",
    "When using BERT in TensorFlow, the corpus needs to go through a preprocessor first (read the documentation to know which processor should be used). Then, the BERT model takes the processed data as input to generate embedding vectors. This object has two important keys,\n",
    "<code style='font-size:13px'>sequence_output</code> which represents each token a vector and\n",
    "<code style='font-size:13px'>pooled_output</code> which represents the whole sequence a vector.\n",
    "\n",
    "[BERT]: https://en.wikipedia.org/wiki/BERT_(language_model)\n",
    "[BERT-base]: https://tfhub.dev/google/collections/bert/1\n",
    "[BERT-experts]: https://tfhub.dev/google/collections/experts/bert/1\n",
    "[ALBERT]: https://tfhub.dev/google/collections/albert/1\n",
    "[ELECTRA]: https://tfhub.dev/google/collections/electra/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertProcessor = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')\n",
    "bertEncoder = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = [\n",
    "    'this is such an amazing movie',\n",
    "    'this movie is terrible',\n",
    "]\n",
    "\n",
    "doc = bertProcessor(doc)\n",
    "embed = bertEncoder(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 512])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed['pooled_output'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 128, 512])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed['sequence_output'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Sequence labeling\n",
    "[Sequential labeling], as its name states, is the task of classifying members of a sequence into pre-defined categories. The main application of sequence labeling in NLP is syntactic analysis.\n",
    "\n",
    "[Sequential labeling]: https://en.wikipedia.org/wiki/Sequence_labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Syntactic analysis\n",
    "Syntactic analysis is the process of analyzing word roles and relationship. We are going to implement three most common analyses of this type using SpaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of speech\n",
    "[Part-of-speech] (POS, somtimes called POS tag) is the grammatical category of words. The problem of finding POS of words is call *POS tagging*. It is the simplest sequence labeling problem, as each word has only one POS tag. The complete list of POS tags can be found [here](https://github.com/explosion/spaCy/blob/master/spacy/glossary.py#L23).\n",
    "\n",
    "[Part-of-speech]: https://en.wikipedia.org/wiki/Part_of_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"I'm hungry, but there is nothing in the fridge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>postag</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>PRON</td>\n",
       "      <td>pronoun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'m</td>\n",
       "      <td>AUX</td>\n",
       "      <td>auxiliary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hungry</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>adjective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>punctuation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>but</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>coordinating conjunction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>there</td>\n",
       "      <td>PRON</td>\n",
       "      <td>pronoun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>is</td>\n",
       "      <td>VERB</td>\n",
       "      <td>verb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nothing</td>\n",
       "      <td>PRON</td>\n",
       "      <td>pronoun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "      <td>adposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>determiner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>fridge</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>noun</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      token postag               description\n",
       "0         I   PRON                   pronoun\n",
       "1        'm    AUX                 auxiliary\n",
       "2    hungry    ADJ                 adjective\n",
       "3         ,  PUNCT               punctuation\n",
       "4       but  CCONJ  coordinating conjunction\n",
       "5     there   PRON                   pronoun\n",
       "6        is   VERB                      verb\n",
       "7   nothing   PRON                   pronoun\n",
       "8        in    ADP                adposition\n",
       "9       the    DET                determiner\n",
       "10   fridge   NOUN                      noun"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listToken = [token.text for token in doc]\n",
    "listPos = [token.pos_ for token in doc]\n",
    "listDesc = [spacy.explain(token.pos_) for token in doc]\n",
    "pd.DataFrame({\n",
    "    'token': listToken,\n",
    "    'postag': listPos,\n",
    "    'description': listDesc,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency\n",
    "[Dependency] parsing is the process of analyzing the sentence structures and word relationships. It can be used to examine phrasal nouns and verbs. The full list of dependency labels can be found [here](https://github.com/explosion/spaCy/blob/master/spacy/glossary.py#L205).\n",
    "\n",
    "[Dependency]: https://en.wikipedia.org/wiki/Dependency_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"35676c0b8fbc4b19892f6ccc63891fee-0\" class=\"displacy\" width=\"1050\" height=\"287.0\" direction=\"ltr\" style=\"max-width: none; height: 287.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"150\">'m</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"150\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"250\">hungry,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"250\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">but</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"450\">there</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"450\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"550\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"550\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">nothing</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"850\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"850\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"950\">fridge</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"950\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-35676c0b8fbc4b19892f6ccc63891fee-0-0\" stroke-width=\"2px\" d=\"M70,152.0 C70,102.0 140.0,102.0 140.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-35676c0b8fbc4b19892f6ccc63891fee-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,154.0 L62,142.0 78,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-35676c0b8fbc4b19892f6ccc63891fee-0-1\" stroke-width=\"2px\" d=\"M170,152.0 C170,102.0 240.0,102.0 240.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-35676c0b8fbc4b19892f6ccc63891fee-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M240.0,154.0 L248.0,142.0 232.0,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-35676c0b8fbc4b19892f6ccc63891fee-0-2\" stroke-width=\"2px\" d=\"M170,152.0 C170,52.0 345.0,52.0 345.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-35676c0b8fbc4b19892f6ccc63891fee-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M345.0,154.0 L353.0,142.0 337.0,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-35676c0b8fbc4b19892f6ccc63891fee-0-3\" stroke-width=\"2px\" d=\"M470,152.0 C470,102.0 540.0,102.0 540.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-35676c0b8fbc4b19892f6ccc63891fee-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">expl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M470,154.0 L462,142.0 478,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-35676c0b8fbc4b19892f6ccc63891fee-0-4\" stroke-width=\"2px\" d=\"M170,152.0 C170,2.0 550.0,2.0 550.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-35676c0b8fbc4b19892f6ccc63891fee-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M550.0,154.0 L558.0,142.0 542.0,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-35676c0b8fbc4b19892f6ccc63891fee-0-5\" stroke-width=\"2px\" d=\"M570,152.0 C570,102.0 640.0,102.0 640.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-35676c0b8fbc4b19892f6ccc63891fee-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M640.0,154.0 L648.0,142.0 632.0,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-35676c0b8fbc4b19892f6ccc63891fee-0-6\" stroke-width=\"2px\" d=\"M670,152.0 C670,102.0 740.0,102.0 740.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-35676c0b8fbc4b19892f6ccc63891fee-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M740.0,154.0 L748.0,142.0 732.0,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-35676c0b8fbc4b19892f6ccc63891fee-0-7\" stroke-width=\"2px\" d=\"M870,152.0 C870,102.0 940.0,102.0 940.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-35676c0b8fbc4b19892f6ccc63891fee-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M870,154.0 L862,142.0 878,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-35676c0b8fbc4b19892f6ccc63891fee-0-8\" stroke-width=\"2px\" d=\"M770,152.0 C770,52.0 945.0,52.0 945.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-35676c0b8fbc4b19892f6ccc63891fee-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945.0,154.0 L953.0,142.0 937.0,142.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"I'm hungry, but there is nothing in the fridge\")\n",
    "displacy.render(doc, style='dep', options={'distance': 100})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named entities\n",
    "[Named entity recognition] is the problem of determining if a word has real-world meanings, such as name of people, organizations and locations.\n",
    "The list of Spacy's entity names can be found [here](https://github.com/explosion/spaCy/blob/master/spacy/glossary.py#L325).\n",
    "\n",
    "[Named entity recognition]: https://en.wikipedia.org/wiki/Named-entity_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is looking at buying \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    U.K.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " startup for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $1 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Apple', 383), ('U.K.', 384), ('$1 billion', 394)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(ent.text, ent.label) for ent in doc.ents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Hidden Markov Model\n",
    "[Hidden Markov Model]\n",
    "https://github.com/hmmlearn/hmmlearn\n",
    "\n",
    "[Hidden Markov model]: https://en.wikipedia.org/wiki/Hidden_Markov_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Conditional Random Field\n",
    "[Conditional Random Field]\n",
    "https://github.com/TeamHG-Memex/sklearn-crfsuite\n",
    "\n",
    "Conditional Random Fields are a discriminative model, used for predicting sequences. They use contextual information from previous labels, thus increasing the amount of information the model has to make a good prediction.\n",
    "Discriminative classifier - they model the decision boundary between the different classes (just like logistic regression)\n",
    "\n",
    "[Conditional Random Field]: https://en.wikipedia.org/wiki/Conditional_random_field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. NLP for Vietnamese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Character normalization\n",
    "There are two types of Vietnamese characters being widely used, [decomposed] and [composed]. They are visually identical, but are constructed from different Unicode code-points. We can see the the difference in the example below for the word \"Tiếng Việt\". Because of this, we use the\n",
    "<code style='font-size:13px'><a href=https://docs.python.org/3/library/unicodedata.html#unicodedata.normalize>unicodedata.normalize()</a></code>\n",
    "function to convert between the two forms, with the corresponding names NFD and NFC. As the composed (NFC) form takes less memory, it will be the go-to choice.\n",
    "\n",
    "[decomposed]: https://en.wikipedia.org/wiki/Combining_character\n",
    "[composed]: https://en.wikipedia.org/wiki/Precomposed_character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strNFC = 'Tiếng Việt'\n",
    "strNFD = 'Tiếng Việt'\n",
    "strNFC == strNFD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Ti\\xc3\\xaa\\xcc\\x81ng Vi\\xc3\\xaa\\xcc\\xa3t'\n",
      "b'Ti\\xe1\\xba\\xbfng Vi\\xe1\\xbb\\x87t'\n"
     ]
    }
   ],
   "source": [
    "print(strNFD.encode('utf-8'))\n",
    "print(strNFC.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Ti\\\\xea\\\\u0301ng Vi\\\\xea\\\\u0323t'\n",
      "b'Ti\\\\u1ebfng Vi\\\\u1ec7t'\n"
     ]
    }
   ],
   "source": [
    "print(strNFD.encode('unicode_escape'))\n",
    "print(strNFC.encode('unicode_escape'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "unicodedata.normalize('NFC', strNFD) == strNFC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also vectorize the normalization function so that we can transform a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import vectorize\n",
    "@vectorize\n",
    "def normalize_vn(doc):\n",
    "    return unicodedata.normalize('NFC', doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Tiếng Việt', 'Hà Nội', 'ngôn ngữ'], dtype='<U10')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\n",
    "    'Tiếng Việt',\n",
    "    'Hà Nội',\n",
    "    'ngôn ngữ'\n",
    "]\n",
    "\n",
    "normalize_vn(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Underthesea\n",
    "This section uses [Underthesea] for Vietnamese NLP. Other resources, [VNCoreNLP] and [PyVi], are available as well. Compared to English, processing Vietnames requires no stemming and lemmatization, but there will be additional tasks.\n",
    "\n",
    "[VNCoreNLP]: https://github.com/vncorenlp/VnCoreNLP\n",
    "[PyVi]: https://github.com/trungtv/pyvi\n",
    "[Underthesea]: https://github.com/undertheseanlp/underthesea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import underthesea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Đảm bảo chất lượng phòng thí nghiệm hóa học'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "underthesea.text_normalize('Ðảm baỏ chất lựơng phòng thí nghịêm hoá học')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hồ Gươm', 'là', 'danh lam', 'thắng cảnh', 'Hà Nội']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "underthesea.word_tokenize('Hồ Gươm là danh lam thắng cảnh Hà Nội')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hồ Gươm', 'Np'),\n",
       " ('là', 'V'),\n",
       " ('danh lam', 'N'),\n",
       " ('thắng cảnh', 'V'),\n",
       " ('Hà Nội', 'Np')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "underthesea.pos_tag('Hồ Gươm là danh lam thắng cảnh Hà Nội')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hồ Gươm', 'Np', 'B-NP'),\n",
       " ('là', 'V', 'B-VP'),\n",
       " ('danh lam', 'N', 'B-NP'),\n",
       " ('thắng cảnh', 'V', 'B-VP'),\n",
       " ('Hà Nội', 'Np', 'B-NP'),\n",
       " ('!', 'CH', 'O')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "underthesea.chunk('Hồ Gươm là danh lam thắng cảnh Hà Nội!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Hồ Gươm', 4, 'nsubj'),\n",
       " ('là', 4, 'cop'),\n",
       " ('danh lam', 4, 'nummod'),\n",
       " ('thắng cảnh', 0, 'root'),\n",
       " ('Hà Nội', 4, 'nmod'),\n",
       " ('!', 3, 'punct')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "underthesea.dependency_parse('Hồ Gươm là danh lam thắng cảnh Hà Nội!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hồ Gươm', 'Np', 'B-NP', 'B-PER'),\n",
       " ('là', 'V', 'B-VP', 'O'),\n",
       " ('danh lam', 'N', 'B-NP', 'O'),\n",
       " ('thắng cảnh', 'V', 'B-VP', 'O'),\n",
       " ('Hà Nội', 'Np', 'B-NP', 'B-LOC'),\n",
       " ('!', 'CH', 'O', 'O')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "underthesea.ner('Hồ Gươm là danh lam thắng cảnh Hà Nội!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# References\n",
    "- *tartarus.org - [An algorithm for suffix stripping](https://tartarus.org/martin/PorterStemmer/def.txt)*\n",
    "- *dl.acm.org - [Another stemmer](https://dl.acm.org/doi/pdf/10.1145/101306.101310)*\n",
    "- *arxiv.org - [Enriching word vectors with subword information](https://arxiv.org/pdf/1607.04606.pdf)*\n",
    "- *nlp.stanford.edu - [GloVe: Global Vectors for word representation](https://nlp.stanford.edu/pubs/glove.pdf)*\n",
    "- *lilianweng.github.io - [Learning word embedding](https://lilianweng.github.io/posts/2017-10-15-word-embedding/)*\n",
    "- *mccormickml.com - [Word2Vec tutorial - The Skip-Gram model](https://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)*\n",
    "- *arxiv.org - [BERT: Pre-training of Deep Bidirectional Transformers for language understanding](https://arxiv.org/pdf/1810.04805.pdf)*\n",
    "- *arxiv.org - [Universal Language Model Fine-tuning for text classification](https://arxiv.org/pdf/1801.06146.pdf)*\n",
    "- *arxiv.org - [Deep contextualized word representations](https://arxiv.org/pdf/1802.05365.pdf)*\n",
    "- *d2l.ai - [The Skip-Gram model](https://d2l.ai/chapter_natural-language-processing-pretraining/word2vec.html)*\n",
    "- *aegis4048.github.io - [Demystifying Neural Network in Skip-Gram language modeling](https://aegis4048.github.io/demystifying_neural_network_in_skip_gram_language_modeling)*\n",
    "- *jonathan-hui.medium.com - [NLP - Word embedding & GloVe](https://jonathan-hui.medium.com/nlp-word-embedding-glove-5e7f523999f6)*\n",
    "- *maelfabien.github.io - [Word embedding with Skip-Gram Word2Vec](https://maelfabien.github.io/machinelearning/NLP_3/)*\n",
    "- *amitness.com - [FastText embeddings](https://amitness.com/2020/06/fasttext-embeddings/)*\n",
    "- *medium.datadriveninvestor.com - [Word2Vec (Skip-Gram model) explained](https://medium.datadriveninvestor.com/word2vec-skip-gram-model-explained-383fa6ddc4ae)*\n",
    "- *towardsdatascience.com - [Intuitive guide to understanding GloVe embedding](https://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010)*\n",
    "---\n",
    "- https://www.kaggle.com/code/sudalairajkumar/getting-started-with-text-preprocessing/notebook#Conversion-of-Emoticon-to-Words\n",
    "- https://www.kaggle.com/learn-guide/natural-language-processing\n",
    "- https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset/code?resource=download\n",
    "- https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\n",
    "- https://medium.com/@samia.khalid/bert-explained-a-complete-guide-with-theory-and-tutorial-3ac9ebc8fa7c\n",
    "- https://www.kaggle.com/code/harshjain123/bert-for-everyone-tutorial-implementation\n",
    "- https://www.projectpro.io/article/bert-nlp-model-explained/558\n",
    "- https://towardsdatascience.com/bert-to-the-rescue-17671379687f\n",
    "- https://arxiv.org/pdf/2011.06727.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install gensim --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-08T17:10:35.545001Z",
     "iopub.status.busy": "2022-12-08T17:10:35.544511Z",
     "iopub.status.idle": "2022-12-08T17:10:43.862457Z",
     "shell.execute_reply": "2022-12-08T17:10:43.861149Z",
     "shell.execute_reply.started": "2022-12-08T17:10:35.544898Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q -U \"tensorflow-text==2.8.*\" --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install underthesea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install underthesea[deep] --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "*&#9829; By Quang Hung x Thuy Linh &#9829;*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
